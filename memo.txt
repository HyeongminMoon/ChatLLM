# 인스톨
- fastchat instruction
pip3 install -e ".[model_worker,webui]"
- Autogptq instruction
pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -> 2023dec (0.5.1)
pip install auto-gptq --extra-index-url https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.2/auto_gptq-0.4.2+cu118-cp310-cp310-linux_x86_64.whl -> fastchat
- xformers
pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu118 --force-reinstall
- vllm
vllm -> vllm-gptq_hf python setup.py develop
vllm-gptq_hf_2023dec -> 2023dec
-> vllm 0.2.4에 병합됨에 따라 다시 vllm
버그: async_llm_engine의 new_requests_event 부분들 다 지우기
버그: llamafied qwen -> llama.py의 bias 수정하기
- requirements.txt



# 컨트롤러
python fastchat/serve/controller.py --port 21001

# 인터페이스
python fastchat/serve/gradio_web_server.py --port 8860

# 멀티인터페이스
python3 fastchat/serve/gradio_web_server_multi.py --port 8860
python3 fastchat/serve/gradio_web_server_multi.py --port 8880 --host 0.0.0.0


# model worker
python fastchat/serve/model_worker.py --model-path /data/git_temp2/llm_weights/vicuna-13b-working-v1/ --controller http://localhost:21001 --port 31000 --worker http://localhost:31000

# gptq
python fastchat/serve/model_worker.py --model-path /data/git_temp2/llm_weights/wizard-vicuna-13B-GPTQ/ --gptq-wbit 4 --gptq-act-order --controller http://localhost:21001 --port 31004 --worker http://localhost:31004

# AutoGPTQ
python fastchat/serve/model_worker.py --model-path /data/git_temp2/llm_weights/ko_vicuna_7b-AutoGPTQ/ --gptq-wbit 4 --gptq-groupsize 128 --controller http://localhost:21001 --port 31004 --worker http://localhost:31004 --autogptq

python fastchat/serve/model_worker.py --model-path /data/git_temp2/llm_weights/FreeWilly2-GPTQ/ --controller http://localhost:21001 --port 31005 --worker http://localhost:31005 --autogptq --gptq-wbit 4



## StableBeluga2-70B-GPTQ
CUDA_VISIBLE_DEVICES=0 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/StableBeluga2-70B-GPTQ/ --controller http://localhost:21001 --port 31005 --worker http://localhost:31005 --autogptq --gptq-wbit 4 --model-name FreeWilly2-GPTQ

CUDA_VISIBLE_DEVICES=1 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/StableBeluga2-70B-GPTQ/ --controller http://localhost:21001 --port 31006 --worker http://localhost:31006 --autogptq --gptq-wbit 4 --model-name FreeWilly2-GPTQ

## vicuna-13B-v1.5-float16
CUDA_VISIBLE_DEVICES=2 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/vicuna-13b-v1.5/ --controller http://localhost:21001 --port 31007 --worker http://localhost:31007 --model-name vicuna-13b-v1.5-float16

## vicuna-13B-v1.5-int8
CUDA_VISIBLE_DEVICES=3 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/vicuna-13b-v1.5/ --controller http://localhost:21001 --port 31008 --worker http://localhost:31008 --model-name vicuna-13b-v1.5-int8 --load-8bit

## vicuna-13B-v1.5-int4
CUDA_VISIBLE_DEVICES=4 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/vicuna-13B-v1.5-GPTQ/ --controller http://localhost:21001 --port 31009 --worker http://localhost:31009 --autogptq --gptq-wbit 4 --model-name vicuna-13B-v1.5-int4

## vicuna-13B-v1.5-int4-extended_16K
CUDA_VISIBLE_DEVICES=5 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/vicuna-13B-v1.5-16K-GPTQ/ --controller http://localhost:21001 --port 31010 --worker http://localhost:31010 --autogptq --gptq-wbit 4 --model-name vicuna-13B-v1.5-int4-extended_16K

## stablecode-completion-alpha-3b-4k-GPTQ
CUDA_VISIBLE_DEVICES=0 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/StableBeluga2-70B-GPTQ/ --controller http://localhost:21001 --port 31005 --worker http://localhost:31005 --autogptq --gptq-wbit 4 --model-name FreeWilly2-GPTQ

## WizardCoder-Guanaco-15B-V1.1-GPTQ
CUDA_VISIBLE_DEVICES=0 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/StableBeluga2-70B-GPTQ/ --controller http://localhost:21001 --port 31005 --worker http://localhost:31005 --autogptq --gptq-wbit 4 --model-name FreeWilly2-GPTQ


## Platypus2-70B-Instruct-GPTQ
CUDA_VISIBLE_DEVICES=0 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/StableBeluga2-70B-GPTQ/ --controller http://localhost:21001 --port 31005 --worker http://localhost:31005 --autogptq --gptq-wbit 4 --model-name FreeWilly2-GPTQ


# vllm
NCCL_P2P_LEVEL=PIX CUDA_VISIBLE_DEVICES=0,1,2,3 python fastchat/serve/vllm_worker.py --model-path /disk1/data/llm_weights/vicuna-13b-v1.5/ --controller-address http://localhost:21001 --port 31032 --worker-address http://localhost:31032 --num-gpus 4 --max-num-batched-tokens 4000 --gpu-memory-utilization 0.5
NCCL_P2P_LEVEL=PIX CUDA_VISIBLE_DEVICES=4 python fastchat/serve/vllm_worker.py --model-path /data/llm_weights/vicuna-13b-v1.5/ --controller-address http://localhost:21001 --port 31015 --worker-address http://localhost:31015

# OLLM v1.0
NCCL_P2P_LEVEL=PIX CUDA_VISIBLE_DEVICES=7 python fastchat/serve/vllm_worker.py --model-path /data/llm_weights/gptq/MingAI-70B-chat-orca_v0.4_v0.2_retrained_checkpoint-2-GPTQ/ --controller-address http://localhost:21001 --port 31001 --worker-address http://localhost:31001 --num-gpus 1 --max-num-batched-tokens 10240 --host 0.0.0.0

# langchain

python fastchat/serve/model_worker.py --model-path /data/git_temp2/llm_weights/wizard-vicuna-13B-UC-GPTQ/ --gptq-wbit 4 --gptq-groupsize 128 --controller http://localhost:21001 --port 31004 --worker http://localhost:31004 --autogptq --model-names "gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002"

## AdosKoVicuna-7b
CUDA_VISIBLE_DEVICES=0 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/custom_trained/AdosKoVicuna-7b --controller http://localhost:21001 --port 31005 --worker http://localhost:31005

## AdosKoVicuna-13b-lora
CUDA_VISIBLE_DEVICES=1 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/custom_trained/AdosKoVicuna-13b-lora --controller http://localhost:21001 --port 31006 --worker http://localhost:31006

## Llama-2-7b
CUDA_VISIBLE_DEVICES=2 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/Llama-2-7b-hf --controller http://localhost:21001 --port 31007 --worker http://localhost:31007

## Llama-2-13b
CUDA_VISIBLE_DEVICES=3 python fastchat/serve/model_worker.py --model-path /disk1/data/llm_weights/Llama-2-13b-hf --controller http://localhost:21001 --port 31008 --worker http://localhost:31008

--tokenizer hf-internal-testing/llama-tokenizer


## Test Messages


5G 네트워크가 미래 기술과 어떻게 상호 작용할까요?
인공지능이 의료 분야에서 어떻게 활용될 수 있나요?
중세시대를 배경으로 하는 판타지 소설을 작성하세요
거대언어모델의 동향에 대해 레포트를 써줘
지구 온난화와 환경문제에 대한 해결책은 무엇일까요?
사이버 보안 위협과 대응 방안에 대해 설명해주세요.
로봇 공학의 혁신적인 응용 사례는 무엇인가요?
식품 산업에서 인공 지능의 활용 가능성은 무엇인가요?
우주 여행이 상업화되는 미래를 예측해주세요.
우주에서 물과 식량을 어떻게 공급할 수 있을까요?
양자 컴퓨팅이 현실에서 어떻게 활용될 수 있을까요?
인공지능이 교육 분야에서 어떻게 사용되고 있나요?
환경 보호에 대한 최신 기술과 솔루션은 무엇인가요?
우주 탐사 프로젝트의 현재 상황은 어떤가요?
고도로 자율주행 차량은 어떻게 작동하나요?
현대 예술의 트렌드는 어떻게 변하고 있나요?
미래의 우주 여행이 어떻게 계획되고 있나요?
인공 지능이 금융 분야에서 어떻게 사용되고 있나요?
사이버 보안의 최신 동향은 무엇인가요?
지구 온난화와 관련된 기술적 해결책은 무엇인가요?
로봇 수술이 어떻게 진행되고 있으며 혜택은 무엇인가요?
공공 건강에 관한 최신 연구는 무엇인가요?
인공지능이 법률 분야에서 어떻게 활용되고 있나요?
최근의 우주 탐사에서 어떤 발견이 있었나요?
로봇공학이 의료 분야에 어떻게 적용되고 있나요?
신경과학 연구의 최신 결과는 무엇인가요?


최근에 개봉한 인기 영화는 뭐야?
최근에 발표된 과학적 발견이 있었나요?
요즘 핫한 유행어는 뭐에요?
세계에서 가장 큰 도시는 어디에요?
주식 시장의 최신 동향을 알려줄 수 있나요?
가장 인기 있는 스포츠 이벤트는 무엇인가요?
최근에 나온 고급 전자제품 중 추천할 만한 것이 있나요?
최근에 발표된 건강 관련 뉴스가 뭐에요?
여름 휴가를 위한 여행지 추천해주세요.
세계 역사상 가장 중요한 사건은 무엇인가요?
가장 인기 있는 소셜 미디어 앱은 무엇인가요?
요즘 핫한 음악 가수나 밴드가 누구에요?
공부하기 좋은 공공 도서관 위치를 알려주세요.
실시간 뉴스 헤드라인을 보여줄 수 있나요?
요즘 어떤 패션 트렌드가 있는가요?
가장 큰 국제 축제는 어디에서 열리나요?
요즘 대세인 건강 식품은 무엇인가요?
세계에서 가장 높은 산은 어디인가요?
효과적인 스트레스 관리 방법을 알려주세요.
최근에 개최된 예술 전시회를 소개해주세요.
세계에서 가장 높은 빌딩은 무엇인가요?
요즘 핫한 앱 또는 소프트웨어가 뭐에요?
최근에 개발된 과학 기술 중 흥미로운 것이 뭐에요?
신규 출시된 자동차 모델을 소개해주세요.
가족과 함께 할 수 있는 놀이 공원을 추천해주세요.