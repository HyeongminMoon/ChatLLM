{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b13fc8-dd62-467d-8bb0-3bf5370d50cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_train_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_LEVEL\"] = \"PIX\"\n",
    "os.environ[\"MAX_JOBS\"] = \"16\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    LlamaTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from fastchat.modules.gptq_utils.llm_dataset_adapter import get_dataset_adapter\n",
    "\n",
    "# from utils.llm_dataset_adapter import BaseDatasetAdapter\n",
    "# from typing import List, Optional\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6515e345-b5c5-4228-9ef8-6fdeacfc7e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(model_config, tokenizer):\n",
    "    adapter = get_dataset_adapter(\n",
    "        model_config[\"data_path\"], format=model_config[\"data_format\"].lower()\n",
    "    )\n",
    "    dataset = adapter.load_data(\n",
    "        model_config[\"data_path\"],\n",
    "        tokenizer,\n",
    "        n_samples=model_config[\"n_samples\"],\n",
    "        max_length=model_config[\"max_length\"],\n",
    "    )  # , padding=False\n",
    "\n",
    "    return dataset\n",
    "\n",
    "import time\n",
    "def run_autogptq(model_config):\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=model_config[\"bits\"],  # quantize model to 4-bit\n",
    "        group_size=model_config[\n",
    "            \"group_size\"\n",
    "        ],  # it is recommended to set the value to 128\n",
    "        desc_act=model_config[\n",
    "            \"desc_act\"\n",
    "        ],  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    "        damp_percent=model_config[\"damp_percent\"],\n",
    "    )\n",
    "\n",
    "    # load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_config[\"model_path\"], local_files_only=True\n",
    "    )\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model_config[\"model_path\"],\n",
    "        quantize_config,\n",
    "        # use_safetensors=True,\n",
    "        local_files_only=True,\n",
    "        # device_map='auto',\n",
    "        # max_memory={i: \"80GIB\" for i in range(torch.cuda.device_count())},\n",
    "        \n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.unk_token  # train_lora.py\n",
    "    tokenizer.padding_side = model_config[\"padding_side\"]\n",
    "\n",
    "    dataset = get_dataset(model_config, tokenizer)\n",
    "\n",
    "    # quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "    examples = [\n",
    "        {\"input_ids\": example[\"input_ids\"], \"attention_mask\": example[\"attention_mask\"]}\n",
    "        for example in dataset\n",
    "    ]\n",
    "    start = time.time()\n",
    "    model.quantize(examples, batch_size=4)\n",
    "    print(\"quantization ellapse:\", time.time() - start)\n",
    "    # save quantized model using safetensors\n",
    "    model.save_quantized(model_config[\"save_path\"])  # , use_safetensors=True\n",
    "    print(f\"Successfully quantized at {model_config['save_path']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed9bdab-5064-49cc-aec3-b80514b088a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:19<00:00,  1.32s/it]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 535.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantization ellapse: 14438.823736190796\n",
      "Successfully quantized at /workspaces/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ.\n",
      "CPU times: user 5h 8min 46s, sys: 58min 21s, total: 6h 7min 7s\n",
      "Wall time: 4h 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_config = {\n",
    "    \"model_path\": \"/workspaces/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.42_2_dpo\",\n",
    "    \"data_path\": \"/workspaces/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/workspaces/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"padding_side\": \"right\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "    \"damp_percent\": 0.1,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80aee4d3-f034-4008-957c-4e1185e06f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### automatically moving necessities into output folder \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": model_config[\"bits\"],\n",
    "    \"group_size\": model_config[\"group_size\"],\n",
    "    \"damp_percent\": model_config[\"damp_percent\"],\n",
    "    \"desc_act\": model_config[\"desc_act\"],\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "# vllm auto-gptq 적용하려면 config에 quantize_config 들어가야함\n",
    "# 위 코드 오류 없으면 함수 내부에 넣기\n",
    "# tokernizer 옮기는 코드 추가\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_config[\"model_path\"], f_path)\n",
    "    dst = os.path.join(model_config[\"save_path\"], f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# pad_token 2->0 으로 수정 \"padding_side\": \"right\" -> MingAI 0.5a에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8667184-f7fa-43f1-b38a-ed0d2049107b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61/61 [01:03<00:00,  1.05s/it]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 612.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantization ellapse: 15148.160724639893\n",
      "Successfully quantized at /workspaces/data/llm_weights/gptq/COKAL-ko-v1-70B-GPTQ.\n",
      "CPU times: user 5h 17min 54s, sys: 1h 11min 36s, total: 6h 29min 30s\n",
      "Wall time: 4h 14min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_config = {\n",
    "    \"model_path\": \"/workspaces/disk0/data/llm_weights/COKAL-ko-v1-70B\",\n",
    "    \"data_path\": \"/workspaces/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/workspaces/data/llm_weights/gptq/COKAL-ko-v1-70B-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"padding_side\": \"right\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "    \"damp_percent\": 0.1,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227ab268-f976-4adf-b8f7-eb2a25483cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### automatically moving necessities into output folder \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": model_config[\"bits\"],\n",
    "    \"group_size\": model_config[\"group_size\"],\n",
    "    \"damp_percent\": model_config[\"damp_percent\"],\n",
    "    \"desc_act\": model_config[\"desc_act\"],\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "# vllm auto-gptq 적용하려면 config에 quantize_config 들어가야함\n",
    "# 위 코드 오류 없으면 함수 내부에 넣기\n",
    "# tokernizer 옮기는 코드 추가\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_config[\"model_path\"], f_path)\n",
    "    dst = os.path.join(model_config[\"save_path\"], f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# pad_token 2->0 으로 수정 \"padding_side\": \"right\" -> MingAI 0.5a에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7001e83-4bb1-446f-a1e5-7fa843da4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_config[\"model_path\"], local_files_only=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "# dataset = get_dataset(model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358078c-58b9-49c3-9722-bee0c6fd6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" 안녕\"\n",
    "tokenizer(\n",
    "    text,\n",
    "    padding=\"max_length\",\n",
    "    max_length=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc22aa-2284-4a1c-993a-02fc1a0071c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    model_config[\"model_path\"],\n",
    "    quantize_config,\n",
    "    # use_safetensors=True,\n",
    "    local_files_only=True,\n",
    "    # device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fbcb86-e7ae-496b-b988-6a4ca61bca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [02:08<00:00,  8.56s/it]\n",
      "Map: 100%|██████████| 128/128 [00:00<00:00, 686.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully quantized at /disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.5a-checkpoint-44362-GPTQ.\n",
      "CPU times: user 5h 31min 21s, sys: 1h 23min 44s, total: 6h 55min 6s\n",
      "Wall time: 4h 18min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_config = {\n",
    "    \"model_path\": \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.5a-checkpoint-44362\",\n",
    "    \"data_path\": \"/disk1/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.5a-checkpoint-44362-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "    \"damp_percent\": 0.1,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": model_config[\"bits\"],\n",
    "    \"group_size\": model_config[\"group_size\"],\n",
    "    \"damp_percent\": model_config[\"damp_percent\"],\n",
    "    \"desc_act\": model_config[\"desc_act\"],\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "# vllm auto-gptq 적용하려면 config에 quantize_config 들어가야함\n",
    "# 위 코드 오류 없으면 함수 내부에 넣기\n",
    "# tokernizer 옮기는 코드 추가\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_config[\"model_path\"], f_path)\n",
    "    dst = os.path.join(model_config[\"save_path\"], f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# llama 70b ko 기반모델 pad_token관련 파일 모두 수정.. 2->0 만약된다면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc1d5ec-ae5b-4a86-b99c-ae192ebf8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\n",
    "    os.path.join(\n",
    "        \"/disk1/data/llm_weights/gptq/Upstage-Llama-2-70B-instruct-v2-GPTQ\",\n",
    "        \"config.json\",\n",
    "    ),\n",
    "    \"r\",\n",
    ") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fc0612-e411-43b3-9efe-38be7e67e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"damp_percent\": 0.01,\n",
    "    \"desc_act\": True,\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06393714-9bc4-4042-8406-fb5ebdad78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586060e-f2b4-40bb-9e66-ae83124318e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_config = {\n",
    "    \"model_path\": \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.2_2/\",\n",
    "    \"data_path\": \"/disk1/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.2_2-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)\n",
    "\n",
    "# model_config = {\n",
    "#     'model_path': \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.2_2/\",\n",
    "#     'data_path': \"/disk1/data/llm_datasets/custom/merged_korean_datasets-vicuna-v1.json\",\n",
    "#     'save_path': \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.2_2-GPTQ\",\n",
    "#     'data_format': \"orca\",\n",
    "#     'max_length': 4096,\n",
    "#     'bits': 4,\n",
    "#     'group_size': 128,\n",
    "#     'desc_act': False,\n",
    "#     'n_samples': 128,\n",
    "# }\n",
    "\n",
    "# run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84618aa-b3c5-4528-b3c5-3fc3433dfa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_config = {\n",
    "    \"model_path\": \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.2_Llama2/\",\n",
    "    \"data_path\": \"/disk1/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.2_Llama2-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513c3b8-b401-413d-a0e4-387ea1aa0a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat_train_2023dec",
   "language": "python",
   "name": "fastchat_train_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
