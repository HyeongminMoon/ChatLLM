{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b13fc8-dd62-467d-8bb0-3bf5370d50cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_train_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-29 16:04:39,285] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_train_2023dec/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_LEVEL\"] = \"PIX\"\n",
    "os.environ[\"MAX_JOBS\"] = \"16\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    LlamaTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from fastchat.modules.gptq_utils.llm_dataset_adapter import get_dataset_adapter\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.train import LazySupervisedDataset\n",
    "# from utils.llm_dataset_adapter import BaseDatasetAdapter\n",
    "# from typing import List, Optional\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7eb99ab4-2f60-4c02-9394-9901ccccc970",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    # model_config[\"model_path\"],\n",
    "    \"/data/llm_weights/custom_trained/DIE-MoE-10.7Bx4_v8_truthy_ep1/\",\n",
    "    local_files_only=True,\n",
    "    model_max_length=model_config[\"max_length\"],\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token  # train_lora.py\n",
    "tokenizer.padding_side = model_config[\"padding_side\"]\n",
    "\n",
    "# raw_data = load_sft_dataset(model_config[\"data_path\"])\n",
    "# dataset = LazySupervisedDataset(raw_data, tokenizer, model_config[\"data_format\"])\n",
    "\n",
    "# choices = np.random.choice(range(len(dataset)), (model_config[\"n_samples\"],), replace=False,).tolist()\n",
    "# examples = [\n",
    "#     {\"input_ids\": dataset[idx][\"input_ids\"], \"attention_mask\": dataset[idx][\"attention_mask\"]}\n",
    "#     for idx in choices\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b405ec46-1de0-46d2-8328-d6bbaaea87c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Body found in woods behind farmhouse less than a mile from Kristin Westra's home\\nPolice say they have found a body in North Yarmouth near a farmhouse on Gray Road a mile from the home of the missing woman, Kristin Westra.\\nAuthor: Beth McEvoy (NEWS CENTER Maine), Shannon Moss (NEWS CENTER Maine)\\nPublished: 11:01 AM EDT October 5, 2018\\nUpdated: 8:25 AM EDT October 6, 2018\\nNORTH YARMOUTH (NEWS CENTER Maine) — Police say they have found\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 1, 16250, 1419, 297, 16748, 2910, 6354, 6186, 2108, 821, 264, 13677, 477, 16661, 262, 3797, 520, 28742, 28713, 1611, 13, 5096, 535, 1315, 590, 506, 1419, 264, 2187, 297, 3964, 627, 1785, 1881, 3065, 264, 6354, 6186, 356, 19420, 7356, 264, 13677, 477, 272, 1611, 302, 272, 6925, 2971, 28725, 16661, 262, 3797, 520, 28723, 13, 11318, 28747, 17822, 3283, 8459, 904, 325, 4390, 7965, 334, 21214, 23579, 557, 1295, 18182, 351, 2158, 325, 4390, 7965, 334, 21214, 23579, 28731, 13, 17878, 2926, 28747, 28705, 28740, 28740, 28747, 28734, 28740, 10401, 413, 13203, 4527, 28705, 28782, 28725, 28705, 28750, 28734, 28740, 28783, 13, 21833, 28747, 28705, 28783, 28747, 28750, 28782, 10401, 413, 13203, 4527, 28705, 28784, 28725, 28705, 28750, 28734, 28740, 28783, 13, 28759, 2934, 28769, 627, 1087, 4095, 1765, 28769, 325, 4390, 7965, 334, 21214, 23579, 28731, 1040, 12338, 1315, 590, 506, 1419,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00ead8d-e484-4edf-adc9-bd8a2d7411ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_train_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=4, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/workspaces/disk0/data/llm_weights/MoMo-70B-lora-1.8.4-DPO/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/data/llm_weights/merged/DIE-MoE-10.7Bx4_random\",\n",
    "    device_map='cpu',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c080e73-e1c3-40c4-a897-9ad772daff55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_ep3/\",\n",
    "    device_map='cpu',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b86461ea-a726-4291-ab3f-345743e469bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:13<00:00,  4.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(152064, 8192, padding_idx=151643)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "          (k_proj): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "          (v_proj): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/workspaces/disk0/data/llm_weights/MoMo-70B-lora-1.8.6-DPO/\",\n",
    "    device_map='cpu',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2aab18a-98f8-4c44-a458-fd70d6cf8573",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [03:24<00:00, 13.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
       "          (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/workspaces/disk0/data/llm_weights/Platypus2-70B-instruct/\",\n",
    "    device_map='cpu',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786be25-7f2b-4436-b75c-6741dd1328e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nhi<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa38734-3642-4249-8b52-c1b92daf790b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "# load quantized model to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(\"/workspaces/data/llm_weights/gptq/MoMo-70B-lora-1.8.4-DPO-GPTQ2\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073360f9-8bf5-4d33-af2d-f3ae8b603b5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "인공지능 분야의 LM이 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "인공지능 분야의 LM은 \"Language Model\"의 약어입니다. Language Model은 자연어 처리 분야에서 사용되는 기술 중 하나로, 주어진 텍스트 데이터를 기반으로 다음 단어 또는 문장의 확률을 예측하는 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "# inference with model.generate\n",
    "user_input = \"인공지능 분야의 LM이 무엇인가요?\"\n",
    "text = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# text = \"auto_gptq is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3bd5f4-e453-4106-8628-75cf32c8e68b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto_gptq is a Python package that provides a simple interface for generating'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c20b27-c35f-4227-9791-31c1721f42f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a216511-66fa-4a66-bb8c-0ef2e598f6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def run_autogptq(model_config):\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=model_config[\"bits\"],  # quantize model to 4-bit\n",
    "        group_size=model_config[\n",
    "            \"group_size\"\n",
    "        ],  # it is recommended to set the value to 128\n",
    "        desc_act=model_config[\n",
    "            \"desc_act\"\n",
    "        ],  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    "        damp_percent=model_config[\"damp_percent\"],\n",
    "    )\n",
    "\n",
    "    # load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_config[\"model_path\"], \n",
    "        local_files_only=True,\n",
    "        model_max_length=model_config[\"max_length\"],\n",
    "    )\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model_config[\"model_path\"],\n",
    "        quantize_config,\n",
    "        # use_safetensors=True,\n",
    "        local_files_only=True,\n",
    "        # device_map='auto',\n",
    "        # max_memory={i: \"80GIB\" for i in range(torch.cuda.device_count())},\n",
    "        \n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.unk_token  # train_lora.py\n",
    "    tokenizer.padding_side = model_config[\"padding_side\"]\n",
    "\n",
    "    raw_data = load_sft_dataset(model_config[\"data_path\"])\n",
    "    dataset = LazySupervisedDataset(raw_data, tokenizer, model_config[\"data_format\"])\n",
    "\n",
    "    choices = np.random.choice(range(len(dataset)), (model_config[\"n_samples\"],), replace=False,).tolist()\n",
    "    examples = [\n",
    "        {\"input_ids\": dataset[idx][\"input_ids\"], \"attention_mask\": dataset[idx][\"attention_mask\"]}\n",
    "        for idx in choices\n",
    "    ]\n",
    "    \n",
    "    start = time.time()\n",
    "    model.quantize(examples, batch_size=4)\n",
    "    print(\"quantization ellapse:\", time.time() - start)\n",
    "    # save quantized model using safetensors\n",
    "    model.save_quantized(model_config[\"save_path\"])  # , use_safetensors=True\n",
    "    print(f\"Successfully quantized at {model_config['save_path']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed9bdab-5064-49cc-aec3-b80514b088a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantization ellapse: 2864.339900493622\n",
      "Successfully quantized at /workspaces/data/llm_weights/gptq/M-DIE-M-10.7B_gpt4_ep3-GPTQ.\n",
      "CPU times: user 46min 48s, sys: 21min 27s, total: 1h 8min 15s\n",
      "Wall time: 48min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_config = {\n",
    "    \"model_path\": \"/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_ep3/\",\n",
    "    # \"data_path\": \"/workspaces/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"data_path\": \"/data/llm_datasets/custom/ados/sft/ados_msft_v4.json\",\n",
    "    # \"data_path\": \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"save_path\": \"/workspaces/data/llm_weights/gptq/M-DIE-M-10.7B_gpt4_ep3-GPTQ\",\n",
    "    \"data_format\": \"chat-orca\",\n",
    "    \"padding_side\": \"right\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": 32,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "    \"damp_percent\": 0.1,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc92e102-c85d-481a-a701-4f795de081ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### automatically moving necessities into output folder \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": model_config[\"bits\"],\n",
    "    \"group_size\": model_config[\"group_size\"],\n",
    "    \"damp_percent\": model_config[\"damp_percent\"],\n",
    "    \"desc_act\": model_config[\"desc_act\"],\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "# vllm auto-gptq 적용하려면 config에 quantize_config 들어가야함\n",
    "# 위 코드 오류 없으면 함수 내부에 넣기\n",
    "# tokernizer 옮기는 코드 추가\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_config[\"model_path\"], f_path)\n",
    "    dst = os.path.join(model_config[\"save_path\"], f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# pad_token 2->0 으로 수정 \"padding_side\": \"right\" -> MingAI 0.5a에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7513c3b8-b401-413d-a0e4-387ea1aa0a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "788ccc5f-f874-4e4e-828f-1f8419da02d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"/workspaces/disk0/data/llm_weights/MoMo-70B-lora-1.8.4-DPO/\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": true,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 8192,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 24576,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_seq_len\": 8192,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 64,\n",
       "  \"num_hidden_layers\": 80,\n",
       "  \"num_key_value_heads\": 64,\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"bits\": 4,\n",
       "    \"damp_percent\": 0.1,\n",
       "    \"desc_act\": true,\n",
       "    \"group_size\": -1,\n",
       "    \"model_file_base_name\": \"model\",\n",
       "    \"model_name_or_path\": null,\n",
       "    \"quant_method\": \"gptq\",\n",
       "    \"sym\": true,\n",
       "    \"true_sequential\": true\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"rotary_emb_base\": 1000000,\n",
       "  \"seq_length\": 32768,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.37.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaConfig.from_json_file(\"/data/llm_weights/gptq/MoMo-70B-lora-1.8.4-DPO-GPTQ2/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f05728d-3a1f-4bfc-82e5-7a7deffb5481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_attn_implementation',\n",
       " '_auto_class',\n",
       " '_create_repo',\n",
       " '_dict_from_json_file',\n",
       " '_get_config_dict',\n",
       " '_get_files_timestamps',\n",
       " '_rope_scaling_validation',\n",
       " '_set_token_in_kwargs',\n",
       " '_upload_modified_files',\n",
       " 'attribute_map',\n",
       " 'dict_torch_dtype_to_str',\n",
       " 'from_dict',\n",
       " 'from_json_file',\n",
       " 'from_pretrained',\n",
       " 'get_config_dict',\n",
       " 'is_composition',\n",
       " 'keys_to_ignore_at_inference',\n",
       " 'model_type',\n",
       " 'name_or_path',\n",
       " 'num_labels',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'save_pretrained',\n",
       " 'to_dict',\n",
       " 'to_diff_dict',\n",
       " 'to_json_file',\n",
       " 'to_json_string',\n",
       " 'update',\n",
       " 'update_from_string',\n",
       " 'use_return_dict']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(LlamaConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee4bf824-7fe8-4a2d-93d6-f53a8ec7fd88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PretrainedConfig.to_dict() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLlamaConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: PretrainedConfig.to_dict() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "LlamaConfig.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49b37e-795b-44cb-ae6e-2676aa5dba19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat_train_2023dec",
   "language": "python",
   "name": "fastchat_train_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
