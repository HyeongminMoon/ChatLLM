{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b13fc8-dd62-467d-8bb0-3bf5370d50cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_train_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 09:55:38,028] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_train_2023dec/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_LEVEL\"] = \"PIX\"\n",
    "os.environ[\"MAX_JOBS\"] = \"16\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    LlamaTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from fastchat.modules.gptq_utils.llm_dataset_adapter import get_dataset_adapter\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.train import LazySupervisedDataset\n",
    "# from utils.llm_dataset_adapter import BaseDatasetAdapter\n",
    "# from typing import List, Optional\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb99ab4-2f60-4c02-9394-9901ccccc970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_config[\"model_path\"], \n",
    "    local_files_only=True,\n",
    "    model_max_length=model_config[\"max_length\"],\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token  # train_lora.py\n",
    "tokenizer.padding_side = model_config[\"padding_side\"]\n",
    "\n",
    "raw_data = load_sft_dataset(model_config[\"data_path\"])\n",
    "dataset = LazySupervisedDataset(raw_data, tokenizer, model_config[\"data_format\"])\n",
    "\n",
    "choices = np.random.choice(range(len(dataset)), (model_config[\"n_samples\"],), replace=False,).tolist()\n",
    "examples = [\n",
    "    {\"input_ids\": dataset[idx][\"input_ids\"], \"attention_mask\": dataset[idx][\"attention_mask\"]}\n",
    "    for idx in choices\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a9759-f5bd-45af-8659-fd47e2f53255",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(examples[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00ead8d-e484-4edf-adc9-bd8a2d7411ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_train_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=4, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/workspaces/disk0/data/llm_weights/MoMo-70B-lora-1.8.4-DPO/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/data/llm_weights/merged/DIE-MoE-10.7Bx4_random\",\n",
    "    device_map='cpu',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c080e73-e1c3-40c4-a897-9ad772daff55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/data/llm_weights/custom_trained/DIE_F-10.7B_ep2/\",\n",
    "    device_map='cpu',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3675f23b-f8d6-46de-bfad-390d086ba1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [03:24<00:00, 13.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
       "          (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/workspaces/disk0/data/llm_weights/Platypus2-70B-instruct/\",\n",
    "    device_map='cpu',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786be25-7f2b-4436-b75c-6741dd1328e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nhi<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa38734-3642-4249-8b52-c1b92daf790b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "# load quantized model to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(\"/workspaces/data/llm_weights/gptq/MoMo-70B-lora-1.8.4-DPO-GPTQ2\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073360f9-8bf5-4d33-af2d-f3ae8b603b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "인공지능 분야의 LM이 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "인공지능 분야의 LM은 \"Language Model\"의 약어입니다. Language Model은 자연어 처리 분야에서 사용되는 기술 중 하나로, 주어진 텍스트 데이터를 기반으로 다음 단어 또는 문장의 확률을 예측하는 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "# inference with model.generate\n",
    "user_input = \"인공지능 분야의 LM이 무엇인가요?\"\n",
    "text = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# text = \"auto_gptq is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3bd5f4-e453-4106-8628-75cf32c8e68b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto_gptq is a Python package that provides a simple interface for generating'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c20b27-c35f-4227-9791-31c1721f42f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a216511-66fa-4a66-bb8c-0ef2e598f6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def run_autogptq(model_config):\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=model_config[\"bits\"],  # quantize model to 4-bit\n",
    "        group_size=model_config[\n",
    "            \"group_size\"\n",
    "        ],  # it is recommended to set the value to 128\n",
    "        desc_act=model_config[\n",
    "            \"desc_act\"\n",
    "        ],  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    "        damp_percent=model_config[\"damp_percent\"],\n",
    "    )\n",
    "\n",
    "    # load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_config[\"model_path\"], \n",
    "        local_files_only=True,\n",
    "        model_max_length=model_config[\"max_length\"],\n",
    "    )\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model_config[\"model_path\"],\n",
    "        quantize_config,\n",
    "        # use_safetensors=True,\n",
    "        local_files_only=True,\n",
    "        # device_map='auto',\n",
    "        # max_memory={i: \"80GIB\" for i in range(torch.cuda.device_count())},\n",
    "        \n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.unk_token  # train_lora.py\n",
    "    tokenizer.padding_side = model_config[\"padding_side\"]\n",
    "\n",
    "    raw_data = load_sft_dataset(model_config[\"data_path\"])\n",
    "    dataset = LazySupervisedDataset(raw_data, tokenizer, model_config[\"data_format\"])\n",
    "\n",
    "    choices = np.random.choice(range(len(dataset)), (model_config[\"n_samples\"],), replace=False,).tolist()\n",
    "    examples = [\n",
    "        {\"input_ids\": dataset[idx][\"input_ids\"], \"attention_mask\": dataset[idx][\"attention_mask\"]}\n",
    "        for idx in choices\n",
    "    ]\n",
    "    \n",
    "    start = time.time()\n",
    "    model.quantize(examples, batch_size=4)\n",
    "    print(\"quantization ellapse:\", time.time() - start)\n",
    "    # save quantized model using safetensors\n",
    "    model.save_quantized(model_config[\"save_path\"])  # , use_safetensors=True\n",
    "    print(f\"Successfully quantized at {model_config['save_path']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed9bdab-5064-49cc-aec3-b80514b088a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:43<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantization ellapse: 14300.248763084412\n",
      "Successfully quantized at /workspaces/data/llm_weights/gptq/MoMo-70B-lora-1.8.4-DPO-GPTQ2.\n",
      "CPU times: user 5h 18min 12s, sys: 1h 5min, total: 6h 23min 12s\n",
      "Wall time: 4h 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_config = {\n",
    "    \"model_path\": \"/workspaces/disk0/data/llm_weights/MoMo-70B-lora-1.8.4-DPO/\",\n",
    "    # \"data_path\": \"/workspaces/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    # \"data_path\": \"/data/llm_datasets/custom/ados/sft/ados_msft_v4.json\",\n",
    "    \"data_path\": \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"save_path\": \"/workspaces/data/llm_weights/gptq/MoMo-70B-lora-1.8.4-DPO-GPTQ2\",\n",
    "    \"data_format\": \"qwen\",\n",
    "    \"padding_side\": \"right\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "    \"damp_percent\": 0.1,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc92e102-c85d-481a-a701-4f795de081ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### automatically moving necessities into output folder \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": model_config[\"bits\"],\n",
    "    \"group_size\": model_config[\"group_size\"],\n",
    "    \"damp_percent\": model_config[\"damp_percent\"],\n",
    "    \"desc_act\": model_config[\"desc_act\"],\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "# vllm auto-gptq 적용하려면 config에 quantize_config 들어가야함\n",
    "# 위 코드 오류 없으면 함수 내부에 넣기\n",
    "# tokernizer 옮기는 코드 추가\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_config[\"model_path\"], f_path)\n",
    "    dst = os.path.join(model_config[\"save_path\"], f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# pad_token 2->0 으로 수정 \"padding_side\": \"right\" -> MingAI 0.5a에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8667184-f7fa-43f1-b38a-ed0d2049107b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_config = {\n",
    "    \"model_path\": \"/workspaces/disk0/data/llm_weights/COKAL-ko-v1-70B\",\n",
    "    \"data_path\": \"/workspaces/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/workspaces/data/llm_weights/gptq/COKAL-ko-v1-70B-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"padding_side\": \"right\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "    \"damp_percent\": 0.1,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ab268-f976-4adf-b8f7-eb2a25483cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### automatically moving necessities into output folder \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": model_config[\"bits\"],\n",
    "    \"group_size\": model_config[\"group_size\"],\n",
    "    \"damp_percent\": model_config[\"damp_percent\"],\n",
    "    \"desc_act\": model_config[\"desc_act\"],\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "# vllm auto-gptq 적용하려면 config에 quantize_config 들어가야함\n",
    "# 위 코드 오류 없으면 함수 내부에 넣기\n",
    "# tokernizer 옮기는 코드 추가\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_config[\"model_path\"], f_path)\n",
    "    dst = os.path.join(model_config[\"save_path\"], f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# pad_token 2->0 으로 수정 \"padding_side\": \"right\" -> MingAI 0.5a에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7001e83-4bb1-446f-a1e5-7fa843da4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_config[\"model_path\"], local_files_only=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "# dataset = get_dataset(model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358078c-58b9-49c3-9722-bee0c6fd6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" 안녕\"\n",
    "tokenizer(\n",
    "    text,\n",
    "    padding=\"max_length\",\n",
    "    max_length=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc22aa-2284-4a1c-993a-02fc1a0071c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    model_config[\"model_path\"],\n",
    "    quantize_config,\n",
    "    # use_safetensors=True,\n",
    "    local_files_only=True,\n",
    "    # device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbcb86-e7ae-496b-b988-6a4ca61bca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_config = {\n",
    "    \"model_path\": \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.5a-checkpoint-44362\",\n",
    "    \"data_path\": \"/disk1/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.5a-checkpoint-44362-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "    \"damp_percent\": 0.1,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": model_config[\"bits\"],\n",
    "    \"group_size\": model_config[\"group_size\"],\n",
    "    \"damp_percent\": model_config[\"damp_percent\"],\n",
    "    \"desc_act\": model_config[\"desc_act\"],\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(os.path.join(model_config[\"save_path\"], \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "# vllm auto-gptq 적용하려면 config에 quantize_config 들어가야함\n",
    "# 위 코드 오류 없으면 함수 내부에 넣기\n",
    "# tokernizer 옮기는 코드 추가\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_config[\"model_path\"], f_path)\n",
    "    dst = os.path.join(model_config[\"save_path\"], f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# llama 70b ko 기반모델 pad_token관련 파일 모두 수정.. 2->0 만약된다면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1d5ec-ae5b-4a86-b99c-ae192ebf8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\n",
    "    os.path.join(\n",
    "        \"/disk1/data/llm_weights/gptq/Upstage-Llama-2-70B-instruct-v2-GPTQ\",\n",
    "        \"config.json\",\n",
    "    ),\n",
    "    \"r\",\n",
    ") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc0612-e411-43b3-9efe-38be7e67e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"quantization_config\"] = {\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"damp_percent\": 0.01,\n",
    "    \"desc_act\": True,\n",
    "    \"sym\": True,\n",
    "    \"true_sequential\": True,\n",
    "    \"model_name_or_path\": None,\n",
    "    \"model_file_base_name\": \"model\",\n",
    "    \"quant_method\": \"gptq\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06393714-9bc4-4042-8406-fb5ebdad78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586060e-f2b4-40bb-9e66-ae83124318e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_config = {\n",
    "    \"model_path\": \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.2_2/\",\n",
    "    \"data_path\": \"/disk1/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.2_2-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)\n",
    "\n",
    "# model_config = {\n",
    "#     'model_path': \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.2_2/\",\n",
    "#     'data_path': \"/disk1/data/llm_datasets/custom/merged_korean_datasets-vicuna-v1.json\",\n",
    "#     'save_path': \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.2_2-GPTQ\",\n",
    "#     'data_format': \"orca\",\n",
    "#     'max_length': 4096,\n",
    "#     'bits': 4,\n",
    "#     'group_size': 128,\n",
    "#     'desc_act': False,\n",
    "#     'n_samples': 128,\n",
    "# }\n",
    "\n",
    "# run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84618aa-b3c5-4528-b3c5-3fc3433dfa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_config = {\n",
    "    \"model_path\": \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.2_Llama2/\",\n",
    "    \"data_path\": \"/disk1/data/llm_datasets/koalpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"save_path\": \"/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.2_Llama2-GPTQ\",\n",
    "    \"data_format\": \"orca\",\n",
    "    \"max_length\": 4096,\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": -1,\n",
    "    \"desc_act\": True,\n",
    "    \"n_samples\": 128,\n",
    "}\n",
    "\n",
    "run_autogptq(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513c3b8-b401-413d-a0e4-387ea1aa0a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat_train_2023dec",
   "language": "python",
   "name": "fastchat_train_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
