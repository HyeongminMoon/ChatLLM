{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6737de8-e4c0-4865-b86e-3e9a90417890",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "cache_dir = None\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6573a97-32e2-4466-bc1f-9412f4ef59e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "\n",
    "def extract_anthropic_prompt(prompt_and_response, search_term=\"\\n\\nAssistant:\"):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "class hankang_DPODataset:\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_path=\"/data/llm_datasets/Ultrafeedback_binarized.ko.hankang/\",\n",
    "        data_format='chat-orca',\n",
    "        search_term='\\n\\n### Assistant:',\n",
    "        num_train=None,\n",
    "        num_eval=None,\n",
    "    ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.data_format = data_format\n",
    "        self.search_term = search_term\n",
    "        self.num_train = num_train\n",
    "        self.num_eval = num_eval\n",
    "    \n",
    "    def get_prompt_and_response(self, data):\n",
    "        conv = get_conversation_template(self.data_format)\n",
    "\n",
    "        for idx, _conv in enumerate(data):\n",
    "            role = _conv['role']\n",
    "            content = _conv['content_kr']\n",
    "            if idx % 2 == 0 and role == 'user':\n",
    "                conv.append_message(conv.roles[0], content)\n",
    "            elif idx % 2 == 1 and role == 'assistant':\n",
    "                conv.append_message(conv.roles[1], content)\n",
    "            else:\n",
    "                print(\"Warning: data type invaild\")\n",
    "\n",
    "        if len(conv.messages) == 0:\n",
    "            print(\"Warning: data is empty\")\n",
    "        if len(conv.messages) % 2 != 0:\n",
    "            print(\"Warning: data has weird pair\")\n",
    "\n",
    "        return conv.get_prompt()\n",
    "    \n",
    "    def make_dpo_data_module(self):\n",
    "        def validate_prompt_and_responses(data) -> bool:\n",
    "            try:\n",
    "                prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "                prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "                prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "                promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            except AssertionError:\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        def split_prompt_and_responses(data) -> Dict[str, str]:\n",
    "            prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "            prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "            prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "            promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": prompt_and_response[len(prompt) :],\n",
    "                \"rejected\": prompt_and_response_rejected[len(promopt_rejected) :],\n",
    "            }\n",
    "                             \n",
    "                             \n",
    "        dataset = load_dataset(self.dataset_path)\n",
    "\n",
    "        train_dataset = dataset['train']\n",
    "        eval_dataset = dataset['test']\n",
    "\n",
    "        original_columns = list(train_dataset.features.keys())\n",
    "\n",
    "        if self.num_train is not None:\n",
    "            train_dataset = train_dataset.select(range(min(len(train_dataset), self.num_train)))\n",
    "        if self.num_eval is not None:\n",
    "            eval_dataset = eval_dataset.select(range(min(len(train_dataset), self.num_eval)))\n",
    "\n",
    "        train_dataset = train_dataset.filter(validate_prompt_and_responses)\n",
    "        train_dataset = train_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        eval_dataset = eval_dataset.filter(validate_prompt_and_responses)\n",
    "        eval_dataset = eval_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b5bb80a-5915-4592-8989-244e11c9e4a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import hankang_DPODataset\n",
    "\n",
    "dpo_dataset = hankang_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c224b24-013e-46ef-b8f2-6ec3a718c3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rejected': ' 1. 수영: 심혈관 건강과 지구력, 유연성 및 근력을 증진하는 재미있고 상쾌한 활동입니다. 아이들이 적절한 수영복을 착용하고 물에서 가까운 곳에서 감독을 철저히 해야 합니다.\\n2. 하이킹: 자연을 탐험하고 균형 감각과 조정 능력을 향상시키며 지역 동식물에 대해 배울 수 있는 좋은 기회입니다. 튼튼한 신발과 충분한 물, 간식을 준비하세요\\n3. 가드닝: 호기심과 자연에 대한 감사함을 키우고 신체 활동 및 건강한 식습관을 장려하는 풍요로운 활동입니다. 땅(정원 또는 화분)과 씨앗/식물, 도구 등을 제공합니다\\n4. 요리 수업: 요리 기술, 창의력 및 건강한 식습관을 개발할 수 있는 훌륭한 방법입니다. 어린이들은 책임감 있는 성인이 감독하고 적절한 주방 공간(가정이나 정원)에 접근할 수 있어야 합니다\\n5 . 예술 및 공예 : 상상력과 표현력을 자극하는 동시에 소근육 운동 능력과 눈썰미를 향상시키는 다재다능한 활동입니다.. 페인트, 마커, 종이 등 미술 재료와 안전한 작업 공간을 제공합니다 \\n6 . 팀 스포츠 : 체력 단련, 사회성 발달 및 스포츠맨십을 촉진하는 사교적인 팀 기반 활동입니다.. 적절한 장비와 감독을 제공하고 연령에 적합한 규칙과 기술 수준을 고려하세요 ^^7 . 요가 : 유연성과 균형 감각을 향상시키고 정신 건강을 증진하는 온화한 마음의 휴식 시간 입니다.. 편안한 매트와 숙련된 강사를 제공하며 이상적으로 아이들과 함께 일하는 강사가 좋습니다 ^^(영어).8 댄스 : 음악이 흐르는 동안 아이들의 리듬감각, 조정능력 그리고 자신감을 길러주는 재미있는 놀이활동 입니다... 연령에 맞는 음악과 움직임을 제공하고 이동하기 편안하고 정리 정돈이 잘 된 공간에서 책임감 있는 어른이 감독하도록 하세요 ^^(영어).9 무술: 힘 , 조정 , 집중력을 기르는 훈련 방식이다 ... 편안한 옷 과 장비를 제공하고 어린이들과 함께 일하는 경험이 풍부 한 전문 강사를 선택하십시오 (영어).10 달리기 : 지구력이 증가하고 심장과 폐 기능을 개선하며 성취감을 느낄 수있는 훌륭한 유산소 운동이다 ... 자녀가 개인 목표를 설정하고 달성 할 수 있도록 격려하십시오 ... 적절한 신발 과 옷을 제공하십시오 (영문).11 자전거 타기 : 재미있고 모험적인 놀이로 다리 근육 강화는 물론 균형 감각을 키우고 칼로리를 소모한다... 선택한 지형에 맞는 자전거를 사용하고 헬멧을 착용하며 어린이를 지도할 전문 교사를 선택하세요 (영문).12 야외 게임: 태그 숨바꼭질 구슬던지기 축구 등 팀워크와 사회성을 키우는 전통적이고 인터랙티브 한 놀이 활동을 선정하여 어린이의 나이와 수준에 맞게 진행합니다.... 그에 따라 적절하게 관리하세요(영문).13 음악 레슨은 두뇌 개발 촉진과 섬세한 손놀림을 촉진하여 뇌 발달에도 도움이 되는 자극적이고 풍요로운 활동으로 알려져 있습니다... 자격증을 갖춘 연령별 맞춤형 음악 교사와 연습용 건반이나 기타 같은 적절한 장비를 구비해야 합니다.(ENG) 14 세일링 또는 보트 타기 물 위에서 하는 액티비티로 조절력과 균형 감각을 키우면서 휴식을 취하고 재미를 즐길 수 있습니다.... 올바른 세일링 교육 지침이 적용되고 보트나 세일링 상황에서는 라이프 재킷 사용 여부를 확인하시오(ENG) 15 암벽등반은 상체 근육 강화는 물론 인내심과 협동심을 길러주는 도전적이면서도 지속 가능한 운동으로 알려져 있습니다... 안전한 클라이밍 실행을 위해 공인 가이드에게 교육을 받고 연습용 하네스와 장비를 제공할 것</s>',\n",
       " 'prompt': '### System:\\nThis is a system prompt, please behave and help the user.\\n\\n### User: 다양한 신체 활동과 창의적인 취미를 활용하고 연령, 성별, 지역 등의 요소를 고려하여 여름철 길고 더운 기간 동안 어린이들을 위한 25개의 흥미롭고 영양가 있는 활동 목록을 개발하세요. 수영, 하이킹, 정원 가꾸기, 요리 수업, 예술 및 공예 수업과 같은 활동을 고려하고 각 제안이 건강한 습관과 운동 및 웰빙에 대한 긍정적인 태도를 장려하는지 확인하세요. 각 활동에 필요한 소품이나 도구, 필요한 감독 또는 잠재적 위험이나 예방 조치 등을 간략하게 설명하는 자세한 설명을 제공해야 합니다.\\n\\n### Assistant:',\n",
       " 'chosen': ' 기꺼이 도와드리겠습니다! 여름철 길고 더운 기간 동안 어린이들을 위한 25가지 흥미롭고 영양가 있는 활동이 준비되어 있습니다:\\n^^^1. 수영: 수영은 여름철에 아이들에게 좋은 활동입니다. 더위를 식히고 운동을 하며 재미있게 놀 수 있는 좋은 방법입니다. 지역 수영장, 해변 또는 워터파크에 데려갈 수 있습니다. 자외선 차단제와 물을 충분히 챙기는 것을 잊지 마세요\\n2. 하이킹: 하이킹은 운동을 하고 야외를 즐길 수 있는 좋은 방법입니다. 어린이에게 적합한 지역 하이킹 코스를 찾아 충분한 물과 간식을 챙겨주세요\\n3. 가드닝: 가드닝은 아이들이 야외에서 자연을 배우고 창의력을 키울 수 있는 좋은 방법입니다. 과일과 채소를 심거나 작은 허브 정원을 시작할 수도 있습니다\\n4 . 요리 수업 : 요리 수업은 아이들에게 건강한 식습관과 영양가있는 식사 준비를 가르치는 데 효과적인 방법입니다 . 어린이를위한 현지 요리 강좌 나 캠프를 찾아보세요 ^^5 . 예술 및 공예 : 예술 및 공예는 여름 동안 아이들이 참여하고 창의력을 발휘할 수있는 훌륭한 방법입니다 . 집에서 만들기 장소를 마련하거나 현지 미술 수업이나 캠프를 찾아보세요 ^^6 팀 스포츠 : 팀 스포츠는 아이들이 움직이고 사회성을 기르는 데 효과적인 방법입니다 . 어린이를위한 지역 스포츠 리그 또는 캠프를 찾습니다 ^78907890789078907890</s>'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_datamodule['train_dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df680a11-11cc-42bd-b5ff-3f7b16dfa17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27144451-5587-4598-a4c7-fa25a6be8b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/alpaca-gpt4-korean_dedup/\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\",\n",
    "    \"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/sharegpt_V3_format_ko_selected.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "correction_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/\"\n",
    "]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped/aihub_summary_data_tech_dedup/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/도서/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/법률/\",\n",
    "    \"/data/llm_datasets/custom/deduped/naver-news-summarization-ko-vicuna_dedup/\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(enko).json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(koen).json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + correction_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f103ce-1d80-48ed-90c7-9c57ab4e2af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dedup2\n",
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\",#\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",#\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_ko_selected_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "# correction_list = [\n",
    "#     \"/data/llm_datasets/custom/deduped2/KoreaSpellingCorrection-10000.json\",\n",
    "# ]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_tech_dedup-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_book-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_law-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/naver-news-summarization-ko-vicuna_dedup-5000.json\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3046237a-9d36-4f86-83dc-39e83d5a212a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refine\n",
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json\",#\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",#\n",
    "    \"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/refined/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "# correction_list = [\n",
    "#     \"/data/llm_datasets/custom/deduped2/KoreaSpellingCorrection-10000.json\",\n",
    "# ]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_tech_dedup-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_book-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_law-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/naver-news-summarization-ko-vicuna_dedup-5000.json\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "364662d1-9668-44f2-a26e-fabaf68600ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dpo v2\n",
    "dpo_list = [\n",
    "    \"/data/llm_datasets/ultrafeedback_binarized/data/train_prefs-00000-of-00001-17309c769bfe5733.parquet\",\n",
    "    \"/data/llm_datasets/orca_dpo_pairs/\",\n",
    "    \"/data/llm_datasets/distilabel-math-preference-dpo/data/\",\n",
    "]\n",
    "\n",
    "dpo_list2 = [\n",
    "    \"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/untranslated/distilabel-math-preference-dpo.json\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "946c5dcd-6009-44bf-aef2-dd75d1269bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dpo_dataset(dataset_path, split='train'):\n",
    "    if dataset_path.endswith(\"json\"):\n",
    "        dataset = load_dataset(\"json\", data_files=dataset_path, split=split)\n",
    "    elif dataset_path.endswith(\"parquet\"):\n",
    "        dataset = load_dataset(\"parquet\", data_files=dataset_path, split=split)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_path, split=split)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "dataset = load_dpo_dataset(dpo_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ddd2e-0208-461e-9bb8-e34da04d1e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset_list = []\n",
    "for d in dataset_list:\n",
    "    new_dataset_list.append(\"\\\"\" + d + \"\\\"\")\n",
    "print(' '.join(new_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839cdec-b301-40ed-89eb-a0a6512e631e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = dataset_list[7]\n",
    "print(dataset_path)\n",
    "dataset_train = load_sft_dataset(dataset_path)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9c7ebad0-a770-4519-942d-0f5ddc564efa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'conversations': [{'from': Value(dtype='string', id=None),\n",
       "   'value': Value(dtype='string', id=None)}],\n",
       " 'instruction': Value(dtype='string', id=None),\n",
       " 'task_name': Value(dtype='string', id=None),\n",
       " 'system': Value(dtype='string', id=None),\n",
       " 'task': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_sft_dataset(dataset_path) for dataset_path in dataset_list])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fb94dc55-df6b-4c12-a23d-5e2afc3b199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in train_dataset:\n",
    "    new_dataset.append(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7827856c-b0da-4fe8-a0fb-a07b5dbd2a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_dataset.to_json(\"/data/llm_datasets/custom/ados_sft_v4.json\")\n",
    "with open(\"/data/llm_datasets/custom/ados_sft_v4.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fcd5c578-57f8-41d5-8979-7424a90fe754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165129 19379 27 21 48\n"
     ]
    }
   ],
   "source": [
    "\"\"\" find odd code blocks\"\"\"\n",
    "dataset_path = \"/data/llm_datasets/custom/ados_sft_v4.json\"\n",
    "# dataset = load_dataset(\"json\", dataset_path)\n",
    "dataset = load_sft_dataset(dataset_path, split=None)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# new_dataset = []\n",
    "code_prefixes = []\n",
    "\n",
    "odd_dataset = []\n",
    "oddd_dataset = []\n",
    "odd_idxs = set()\n",
    "normal_dataset = []\n",
    "flag_normal = True\n",
    "flag_code = False\n",
    "for idx, data in enumerate(train_dataset):\n",
    "    conversations = data['conversations']\n",
    "    flag_normal = True\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        if _from == 'human': continue\n",
    "        _value = conv['value']\n",
    "        flag_code = False\n",
    "        find_iter = re.finditer('```', _value)\n",
    "        temp_num = 0\n",
    "        for fidx, ftext in enumerate(find_iter):\n",
    "            flag_code = True\n",
    "            start_index = ftext.start() + 3\n",
    "            # new_dataset.append(data)\n",
    "            #TODO: 스페이스바가 바로 오는 경우..\n",
    "            candidate = re.split(r'[\\n]', _value[start_index:])[0]\n",
    "            if fidx % 2 == 0 and '```' not in candidate and candidate not in available_code_prefixes:\n",
    "                odd_dataset.append((candidate, data))\n",
    "                code_prefixes.append(candidate)\n",
    "                odd_idxs.add(idx)\n",
    "                flag_normal = False\n",
    "                break\n",
    "            temp_num += 1\n",
    "        if temp_num % 2 != 0:\n",
    "            oddd_dataset.append(('odd', data))\n",
    "            odd_idxs.add(idx)\n",
    "            flag_normal = False\n",
    "        \n",
    "        if not flag_normal:\n",
    "            break\n",
    "    \n",
    "    if flag_code and flag_normal:\n",
    "        normal_dataset.append(data)\n",
    "        num_normal += 1\n",
    "\n",
    "print(len(train_dataset), len(normal_dataset), len(odd_dataset), len(oddd_dataset), len(odd_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4ea93ac0-9764-4bd1-9235-4ffa35dbd259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('python ',\n",
       " {'system': None,\n",
       "  'task_name': None,\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '주어진 배열을 역순으로 정렬하는 Python 스크립트를 만드세요.\\n'},\n",
       "   {'from': 'gpt',\n",
       "    'value': '주어진 배열을 역순으로 정렬하는 파이썬 스크립트는 다음과 같습니다:\\n\\n```python \\narray = [10, 2, 5, -4, 92, 101]\\narray.sort(reverse=True)\\nprint(array)\\n```\\n\\n결과는 다음과 같습니다:\\n```python\\n[101, 92, 10, 5, 2, -4]\\n```\\n\\n이 스크립트는 리스트의 `sort()` 메소드를 사용하여 배열을 오름차순으로 정렬하며, `reverse` 매개 변수를 `True`로 설정하여 정렬된 리스트의 순서를 뒤집습니다.'}],\n",
       "  'task': None,\n",
       "  'instruction': None,\n",
       "  'id': '35752'})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a7141cb3-f61e-459f-8c60-343df2804357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_idxs = list(range(len(train_dataset)))\n",
    "for od in odd_idxs:\n",
    "    selected_idxs.remove(od)\n",
    "train_dataset = train_dataset.select(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5f93cba5-7840-4b15-8a30-fa289e55f9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4848.91it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1036.65it/s]\n",
      "Generating train split: 165081 examples [00:20, 8243.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'task_name', 'conversations', 'task', 'instruction', 'id'],\n",
       "        num_rows: 165081\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_sft_dataset(\"/data/llm_datasets/custom/ados_sft_v4.1.json\", split=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1ad4ce47-64c5-425b-a651-24ab3bd7abba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '                      ',\n",
       " '  _|←_cW_→_|_↓_',\n",
       " ' - This is similar to the first pattern, but uses the shorthand character class \\\\d to represent any digit.',\n",
       " ' - This pattern includes word boundaries (\\\\b) at the beginning and end, which means it will only match strings that contain a single lowercase hexadecimal word, separated by spaces or other non-word characters. ',\n",
       " ' - This pattern matches any alphanumeric word, including uppercase letters and non-hexadecimal characters. ',\n",
       " ' - This pattern matches any string that consists only of lowercase letters a-f, but does not require the string to be a valid hexadecimal word.',\n",
       " ' - This pattern matches uppercase hexadecimal words, not lowercase.',\n",
       " ' - 참조용으로 MSDN을 참조하세요.',\n",
       " ' command is used to make the book title bold for emphasis.',\n",
       " ' command. ',\n",
       " ' find -type l |',\n",
       " ' find -type l | while IFS= read -r lnkname; do if [ \"$(readlink \\'$lnkname\\')\" == \"/your/exact/path\" ]; then rm -- \"$lnkname\"; fi; done',\n",
       " ' package, you can use the following code:',\n",
       " \" to insert the opening and closing quotation marks. Here's an example of the code you can use:\",\n",
       " ' {',\n",
       " ' 기본적으로 루트로(즉, \"비안전\" 이미지)',\n",
       " ' 기본적으로 액세스가 가능해집니다. 이메일, 연락처 정보 또는 누군가에게서 원치 않는 개인 정보가 담긴 기타 많은 정보가 될 수 있습니다. ',\n",
       " ' 다른 프로그래밍 언어에 일반적으로 내장되어 있는 채널. 또한 경량의 미래 없는 액터를 지원합니다. 예제를 통해 kotlinx.coroutines 가이드에서 자세히 알아보세요.',\n",
       " ' 매개변수. ',\n",
       " ' 매뉴얼 페이지는 다음과 같이 정의합니다:',\n",
       " ' 버전.',\n",
       " ' 이미지를 축소된 이미지에 맞게 스스로 크기를 조정합니다. 예를 들어, 일반적으로 직사각형 이미지 뷰에 직사각형 이미지 뷰가 있는 경우 adjustViewBounds=true를 사용하면 이미지 뷰도 직사각형으로 크기를 조정합니다. 그러면 이미지 뷰 주위에 배치된 다른 뷰에 영향을 미칩니다.',\n",
       " ' 컨트롤러에 간단히 삽입하여 값을 변경할 수 있으므로 매우 간단합니다. 편리할 수 있지만 전역 변수의 모든 문제를 가지고 있습니다.',\n",
       " ' 특정 사용 사례에서는 Rx를 대체하거나 보강할 수 있습니다. 유사점과 차이점에 대해 자세히 설명하는 재귀 스트림용 별도 가이드가 있습니다. ',\n",
       " ' 파이썬',\n",
       " '$ csvgrep -e iso-8859-1 -c 1 -m \"de\" worldcitiespop | csvgrep -c 5 -r \"\\\\d+\"',\n",
       " \"', '', message)\",\n",
       " '* 다중 클래스 분류:',\n",
       " '. ',\n",
       " '. 그러나 결과는 다를 수 있습니다:',\n",
       " '?',\n",
       " 'This code enables JIT mode and configures Tailwind CSS to purge unused styles from the CSS output.',\n",
       " 'This will build the app and install it on the Android emulator or your device.',\n",
       " 'You can also update the `PokemonDisplay` and `MoveDisplay` classes to use the new `Pokemon` and `Move` classes:',\n",
       " '\\\\',\n",
       " '\\\\    | \\\\__/ |      |',\n",
       " '\\\\n\\\\nThis will execute the script and print \\\\\"Hello World\\\\\" to the console.\"]},\"end\\\\_turn\":null,\"weight\":1,\"metadata\":{},\"recipient\":\"all\"}}}',\n",
       " ']',\n",
       " '``',\n",
       " 'dts',\n",
       " 'find -type l -delete',\n",
       " 'find -type l -exec rm {} \\\\;',\n",
       " 'find -type l | while IFS= read -r lnk',\n",
       " 'find -type l | while IFS= read -r lnk; do if (readlink \"$lnk\" | grep -q \\'^/usr/local/texlive/\\'); then rm \"$lnk\"; fi; done',\n",
       " 'javascript ',\n",
       " 'python ',\n",
       " 'undefined',\n",
       " '파이썬'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_prefixes = set(code_prefixes)\n",
    "code_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc164f84-211c-4585-acb1-a810ded34874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_code_prefixes = set([\n",
    "    '',\n",
    "    'CSS',\n",
    "    'HTML',\n",
    "    'JavaScript',\n",
    "    'Python',\n",
    "    'SQL',\n",
    "    'bash',\n",
    "    'c',\n",
    "    'c++',\n",
    "    'cpp',\n",
    "    'csharp',\n",
    "    'css',\n",
    "    'for',\n",
    "    'html',\n",
    "    'java',\n",
    "    'javascript',\n",
    "    'js',\n",
    "    'json',\n",
    "    'php',\n",
    "    'python',\n",
    "    'ruby',\n",
    "    'sass',\n",
    "    'scss',\n",
    "    'sql',\n",
    "    'sum',\n",
    "    'svg',\n",
    "    'swift',\n",
    "    'xml',\n",
    "    'yaml',\n",
    "    'C#',\n",
    "    'C++',\n",
    "    'CSS',\n",
    "    'Go',\n",
    "    'HTML',\n",
    "    'Java',\n",
    "    'LaTeX',\n",
    "    'MATLAB',\n",
    "    'Markdown',\n",
    "    'Proposals',\n",
    "    'Python',\n",
    "    'R',\n",
    "    'SELECT',\n",
    "    'SQL',\n",
    "    'Swift',\n",
    "    'VBA',\n",
    "    'echo',\n",
    "    'excel-vba',\n",
    "    'find',\n",
    "    'go',\n",
    "    'gpg',\n",
    "    'jsx',\n",
    "    'kotlin',\n",
    "    'latex',\n",
    "    'markdown',\n",
    "    'math',\n",
    "    'matlab',\n",
    "    'meditation',\n",
    "    'mermaid',\n",
    "    'mutt',\n",
    "    'nano',\n",
    "    'r',\n",
    "    'rust',\n",
    "    'scala',\n",
    "    'sh',\n",
    "    'shell',\n",
    "    'sudo',\n",
    "    'xpath',\n",
    "    '{r}',\n",
    "    '.',\n",
    "    '$',\n",
    "    'curl',\n",
    "    'xslt',\n",
    "    'Apex',\n",
    "    'DAX',\n",
    "    'Dockerfile',\n",
    "    'apex',\n",
    "    'applescript',\n",
    "    'arduino',\n",
    "    'asm',\n",
    "    'assembly',\n",
    "    'astro',\n",
    "    'autoit',\n",
    "    'batch',\n",
    "    'bicep',\n",
    "    'blade',\n",
    "    'cmake',\n",
    "    'cmd',\n",
    "    'coffee',\n",
    "    'coffeescript',\n",
    "    'cql',\n",
    "    'csv',\n",
    "    'cypher',\n",
    "    'dart',\n",
    "    'delphi',\n",
    "    'diff',\n",
    "    'dockerfile',\n",
    "    'dot',\n",
    "    'emacs',\n",
    "    'erb',\n",
    "    'fsharp',\n",
    "    'glsl',\n",
    "    'gradle',\n",
    "    'graphql',\n",
    "    'graphviz',\n",
    "    'groovy',\n",
    "    'haskell',\n",
    "    'hcl',\n",
    "    'hlsl',\n",
    "    'html+erb',\n",
    "    'ini',\n",
    "    'jinja',\n",
    "    'ladder',\n",
    "    'lasso',\n",
    "    'less',\n",
    "    'lisp',\n",
    "    'lldb',\n",
    "    'llvm',\n",
    "    'logo',\n",
    "    'lua',\n",
    "    'makefile',\n",
    "    'mathematica',\n",
    "    'metal',\n",
    "    'nginx',\n",
    "    'nix',\n",
    "    'objc',\n",
    "    'objective',\n",
    "    'objectivec',\n",
    "    'pascal',\n",
    "    'perl',\n",
    "    'plaintext',\n",
    "    'plantuml',\n",
    "    'powershell',\n",
    "    'prisma',\n",
    "    'properties',\n",
    "    'proto',\n",
    "    'protobuf',\n",
    "    'py',\n",
    "    'reg',\n",
    "    'rego',\n",
    "    'scheme',\n",
    "    'scratch',\n",
    "    'solidity',\n",
    "    'spss',\n",
    "    'stata',\n",
    "    'stencil',\n",
    "    'terraform',\n",
    "    'toml',\n",
    "    'ts',\n",
    "    'tsx',\n",
    "    'txt',\n",
    "    'typescript',\n",
    "    'vb',\n",
    "    'vba',\n",
    "    'vbnet',\n",
    "    'verilog',\n",
    "    'yml',\n",
    "    'jsp',\n",
    "    'prolog',\n",
    "    'razor',\n",
    "    'CMD',\n",
    "    'G',\n",
    "    'GraphQL',\n",
    "    'Makefile',\n",
    "    'apache',\n",
    "    'c#',\n",
    "    'cython',\n",
    "    'elixir',\n",
    "    'jinja2',\n",
    "    'julia',\n",
    "    'ocaml',\n",
    "    'systemverilog',\n",
    "    'vbscript',\n",
    "    'vhdl',\n",
    "    'vue',\n",
    "    'wasm',\n",
    "    'wolfram',\n",
    "    'zsh',\n",
    "    'regex',\n",
    "    ' Java',\n",
    "    ' Python',\n",
    "    ' c++',\n",
    "    ' python',\n",
    "    ' java',\n",
    "     ' css',\n",
    "     ' html',\n",
    "     ' js',\n",
    "     ' python',\n",
    "     ' scala',\n",
    "     'md',\n",
    "])\n",
    "\n",
    "# for pre in available_code_prefixes:\n",
    "with open(\"available_code_prefixes.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(available_code_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387a1b4-47f2-4688-b06b-ef9c8b23a113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_code_prefixes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/distilabel-math-preference-dpo/data/:train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/data/llm_datasets/distilabel-math-preference-dpo/data/': {'train': defaultdict(int,\n",
       "              {'__label__en': 2418})},\n",
       " 'total': {'train': defaultdict(int, {'__label__en': 2418}),\n",
       "  'test': defaultdict(int, {})},\n",
       " 'stat': {'train': {'__label__en': '100.00%'}, 'test': {}}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03eebf0c-90ac-4e30-b52c-54e643b5802e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809e09-c947-4724-80ab-965e2f66816f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            summary = context_data['summary']\n",
    "\n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "    print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31df058-d4f2-4df8-8d24-57a3330db159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['TL_EE_train'] + dataset_dict['TL_LA_train'] + dataset_dict['TL_ED_train'] + dataset_dict['TL_NA_train']\n",
    "eval_dataset_list = dataset_dict['TL_EE_val'] + dataset_dict['TL_LA_val'] + dataset_dict['TL_ED_val'] + dataset_dict['TL_NA_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf58516-415a-4d89-813e-0712e1486acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b508d1c-fc90-417a-9681-27cdcec772e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef997b-df6c-442e-ad81-64eb7a74f844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        documents = json_data['documents']\n",
    "        for document in documents:\n",
    "            text = document['text']\n",
    "            abstractive = document['abstractive']\n",
    "\n",
    "            summary = abstractive[0]\n",
    "            context = []\n",
    "            for _text in text:\n",
    "                _context = ' '.join([_index_text['sentence'] for _index_text in _text])\n",
    "                context.append(_context)\n",
    "            context = '\\n'.join(context)\n",
    "            \n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400f4d6-274c-4591-9315-1cd9702dc78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c989c-7f09-4326-b3a8-8d606b376917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_law_train']\n",
    "eval_dataset_list = dataset_dict['summary_law_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b13db5-1c0f-4991-bd1e-446127d4dac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310ab0d-cb0b-4a4e-97fb-4606c736d216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary_book*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '**/*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context = json_data['passage']\n",
    "        summary = json_data['summary']\n",
    "        \n",
    "        data_row = {\n",
    "            'id': f\"{file_name}_{idx}\",\n",
    "            'task': 'summarization',\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': context},\n",
    "                                {'from': 'gpt', 'value': summary},\n",
    "                             ],\n",
    "        }\n",
    "        new_dataset.append(data_row)\n",
    "        idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3337-aa61-49fe-8cb2-eea7a14fcb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610abe01-befd-4bc1-b67a-ddf26127d1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_book_train']\n",
    "eval_dataset_list = dataset_dict['summary_book_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1eb8e0-7c27-4d26-916d-77ab6cc5925a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ff1b9-148b-40a4-93c7-e918b5f5d1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data/도서\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a299c-602f-4d11-a362-04c9b34bcd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5c088-6902-416b-899c-8efb4b42e8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36141362-162e-4cea-a636-b6f876481896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*VL*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            qas = context_data['qas']\n",
    "\n",
    "            for _qas in qas:\n",
    "                question = _qas['question-1']\n",
    "                answer = _qas['answer']\n",
    "                question_level = _qas['question_level']\n",
    "                if question_level != '상': continue\n",
    "                data_row = {\n",
    "                    'id': f\"{file_name}_{idx}\",\n",
    "                    'task': 'contextqa',\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                }\n",
    "                new_dataset.append(data_row)\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83775c61-d0a7-4483-b833-7775c9dd9449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6a348-e8fc-4b42-ac62-e6f9adb46743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['VL_EE_train'] + dataset_dict['VL_NA_train'] + dataset_dict['VL_LA_train']+ dataset_dict['VL_ED_train']\n",
    "eval_dataset_list = dataset_dict['VL_EE_val'] + dataset_dict['VL_NA_val'] + dataset_dict['VL_LA_val']+ dataset_dict['VL_ED_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db99aa-0bbc-49d9-8df4-87dcc4daee19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7d65-84d7-41f3-b10c-3aef5ffcfdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list_0 = train_dataset_list[:120000]\n",
    "train_dataset_list_1 = train_dataset_list[120000:240000]\n",
    "train_dataset_list_2 = train_dataset_list[240000:]\n",
    "\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split0.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_0, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split1.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_1, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split2.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_2, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503836eb-d006-410c-9986-65d737526d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bfe09-7e0a-4c23-b9f5-fff6478c1d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d8cbd-68a3-4480-ac47-6a9f0fabb09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/gpt4_evol_1.3k/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687e5db-9ef7-4218-91ad-b65279e1bd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dataset['train'][0]\n",
    "# answer = data['answer']\n",
    "# question = data['question']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afea3a-efc8-4ddb-92c7-c06297e69260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    answer = data['answer']\n",
    "    question = data['question']\n",
    "\n",
    "    data_row = {\n",
    "        'id': f\"gpt_evol_1.3k_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': question},\n",
    "                            {'from': 'gpt', 'value': answer},\n",
    "                         ],\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194f908-d12d-4045-8d78-d1da6ab8375c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716623c-7fb9-482f-83e3-8ebce535c41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115478e-7ce6-4b57-830c-5c6c794e1334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/WizardLM_Orca/wizardlm_orca.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b5486-8910-4cb8-9895-055e64ef5e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    output = data['output']\n",
    "    system = data['system']\n",
    "    instruction = data['instruction']\n",
    "    data_row = {\n",
    "        'id': f\"WizardLM_Orca_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': instruction},\n",
    "                            {'from': 'gpt', 'value': output},\n",
    "                         ],\n",
    "        'task': 'system_instruct',\n",
    "        'system': system,\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a898f0b-379f-42e4-a762-cf30ee4123e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391b407-27d5-487b-8b69-b5d5d7c10983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/data/llm_datasets/KoreaSpellingCorrection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af63b70-f50d-4b53-8d52-4da167c7889d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['test']:\n",
    "    wrong = data['wrong']\n",
    "    correct = data['correct']\n",
    "    data_row = {\n",
    "        'id': f\"KoreaSpelling_Correction_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': wrong},\n",
    "                            {'from': 'gpt', 'value': correct},\n",
    "                         ],\n",
    "        'task': 'correction',\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a29e00-b31e-46a3-818c-9fab9ca3aaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcf31a-81f3-4147-9cac-cd609b3a62b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/test.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d44bae8-d7cc-4aea-8251-0c0124bda876",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/distilabel-math-preference-dpo/data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['metadata', 'instruction', 'chosen_response', 'chosen_rating', 'rejected_response', 'rejected_rating'],\n",
       "    num_rows: 2418\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dpo_list[2])\n",
    "dataset = load_dpo_dataset(dpo_list[2])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f8f2cb5-d6be-45cd-8ffc-1c276ffe8e01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset:\n",
    "    new_dataset.append({\n",
    "        'id': f\"Ko_ultrafeedback_binarized_{data['prompt_id']}\",\n",
    "        'input': data['prompt'],\n",
    "        'chosen': data['chosen'][1]['content'],\n",
    "        'rejected': data['rejected'][1]['content'],\n",
    "        'task': \"dpo\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e57e955a-e66d-458a-89b0-d651368d3b01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "# idx = 0\n",
    "for idx, data in enumerate(dataset):\n",
    "    new_dataset.append({\n",
    "        'id': f\"Ko_orca_dpo_pairs_{idx}\",\n",
    "        'input': data['question'],\n",
    "        'chosen': data['chosen'],\n",
    "        'rejected': data['rejected'],\n",
    "        'task': \"dpo_system\",\n",
    "        'system': data['system'],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6428e326-eab7-4927-b32d-c10a38dad554",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': '{\"length-input\": 139, \"length-generations-1\": 539, \"length-generations-2\": 493, \"length-generations-3\": 801, \"rating-generations-1\": 8.0, \"rating-generations-2\": 7.0, \"rating-generations-3\": 9.0, \"distance-best-rated\": 1.0}',\n",
       " 'instruction': 'How can I simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen_response': 'To simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`, we can follow a few steps:\\n\\nStep 1: Distribute the division symbol by multiplying the expression by the reciprocal of the denominator. The reciprocal of `2x` is `1/(2x)`, so the expression becomes `(3x^2 - 4y^3) * (1/(2x))`.\\n\\nStep 2: Simplify within the parentheses by dividing each term separately. \\n\\n- For the first term, `3x^2`, divide `3x^2` by `2x`. This gives us `(3x^2) / (2x) = (3/2) * (x^2 / x) = (3/2) * x`.\\n\\n- For the second term, `-4y^3`, divide `-4y^3` by `2x`. This gives us `(-4y^3) / (2x) = (-2) * (y^3 / x)`.\\n\\nStep 3: Combine the simplified terms from Step 2. The expression now becomes `(3/2) * x - 2 * (y^3 / x)`.\\n\\nSo, the simplified form of the algebraic expression `(3x^2 - 4y^3) / (2x)` is `(3/2) * x - 2 * (y^3 / x)`.',\n",
       " 'chosen_rating': 9.0,\n",
       " 'rejected_response': \"To simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`, you can divide each term in the numerator by the denominator.\\n\\nFirst, let's divide `3x^2` by `2x`. Since both terms have a common factor of `x`, we can simplify this expression to `3x`.\\n\\nNext, we divide `-4y^3` by `2x`. We can simplify this expression by dividing each term separately. \\n\\nDividing `-4` by `2` gives `-2`. Then, dividing `y^3` by `x` gives `y^3/x`.\\n\\nSo, the simplified form of `(3x^2 - 4y^3) / (2x)` is `3x - 2y^3/x`.\",\n",
       " 'rejected_rating': 7.0}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "770fe090-dba0-4741-8212-f96c7d25e97a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "# idx = 0\n",
    "for idx, data in enumerate(dataset):\n",
    "    new_dataset.append({\n",
    "        'id': f\"Ko_distilabel-math-preference-dpo_{idx}\",\n",
    "        'input': data['instruction'],\n",
    "        'chosen': data['chosen_response'],\n",
    "        'rejected': data['rejected_response'],\n",
    "        'task': \"dpo\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "01fba327-b7cd-4bb6-9bf2-e98c92a75f09",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Ko_distilabel-math-preference-dpo_0',\n",
       " 'input': 'How can I simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': 'To simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`, we can follow a few steps:\\n\\nStep 1: Distribute the division symbol by multiplying the expression by the reciprocal of the denominator. The reciprocal of `2x` is `1/(2x)`, so the expression becomes `(3x^2 - 4y^3) * (1/(2x))`.\\n\\nStep 2: Simplify within the parentheses by dividing each term separately. \\n\\n- For the first term, `3x^2`, divide `3x^2` by `2x`. This gives us `(3x^2) / (2x) = (3/2) * (x^2 / x) = (3/2) * x`.\\n\\n- For the second term, `-4y^3`, divide `-4y^3` by `2x`. This gives us `(-4y^3) / (2x) = (-2) * (y^3 / x)`.\\n\\nStep 3: Combine the simplified terms from Step 2. The expression now becomes `(3/2) * x - 2 * (y^3 / x)`.\\n\\nSo, the simplified form of the algebraic expression `(3x^2 - 4y^3) / (2x)` is `(3/2) * x - 2 * (y^3 / x)`.',\n",
       " 'rejected': \"To simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`, you can divide each term in the numerator by the denominator.\\n\\nFirst, let's divide `3x^2` by `2x`. Since both terms have a common factor of `x`, we can simplify this expression to `3x`.\\n\\nNext, we divide `-4y^3` by `2x`. We can simplify this expression by dividing each term separately. \\n\\nDividing `-4` by `2` gives `-2`. Then, dividing `y^3` by `x` gives `y^3/x`.\\n\\nSo, the simplified form of `(3x^2 - 4y^3) / (2x)` is `3x - 2y^3/x`.\",\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c06a934-0004-40c2-9b84-d8166b61209f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/kodpo/untranslated/distilabel-math-preference-dpo.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "efb0aaf4-dd08-49cb-bbab-87f582d00d52",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a 1,000-word op-ed piece in a formal tone, analyzing and providing examples of the ways in which social media platforms have been utilized to spread extremist and violent ideologies. In your analysis, discuss the specific tactics that these groups use to spread their messages online and the effects of these tactics on both individuals and society. Additionally, provide possible solutions that could be implemented to combat the spread of these dangerous ideologies on social media. Your piece should be well-researched, citing reputable sources to support your arguments.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fba6e06-8f45-4245-a33a-974c57120b6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    prompt_id = data['prompt_id']\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    # _id = f\"Ko_ultrafeedback_binarized_{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766daa1-d88a-461c-969f-9b1121b2c671",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'id', 'input', 'chosen', 'rejected', 'task'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412dff2-f152-41d2-95c6-dcb59984bf51",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458618e3-7e40-4ed5-b5ef-f1e06ee1741b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def send_request(new_dataset):\n",
    "    global idx\n",
    "    for _ in range(2):\n",
    "        if idx > len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        data = dataset[subset][idx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "        \n",
    "        _id = data['id']\n",
    "        context = data['context']\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        if answer.lower() == 'yes':\n",
    "            answer = '네'\n",
    "        elif answer.lower() == 'no':\n",
    "            answer = '아니오'\n",
    "        \n",
    "        # response\n",
    "        result = generate_refiner(\n",
    "            model_name,\n",
    "            context,\n",
    "            question,\n",
    "            answer\n",
    "        )\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'id': _id,\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': question},\n",
    "                                {'from': 'gpt', 'value': result},\n",
    "                             ],\n",
    "            'task_name': \"instruct\",\n",
    "            'instruction': context,\n",
    "        })\n",
    "\n",
    "\n",
    "model_name = \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\"\n",
    "subset = 'train'\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(dataset[subset])\n",
    "n_thread = 64\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/lima_vicuna_format/lima_vicuna_format.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e47387-156e-4324-9f03-4081a0447508",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9befa-8fb0-4734-8f01-909c829ee4eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ca8e-0b78-4585-8b54-dbebbb00df99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def send_translate_request(new_dataset):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = lang_dict['__label__en'][pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        conv = data['conversations']\n",
    "        new_conv = []\n",
    "        for _data in conv:\n",
    "            value = _data['value']\n",
    "            # response\n",
    "            results = \"\"\n",
    "            text_blocks = []\n",
    "            code_blocks = []\n",
    "            for bidx, block in enumerate(value.split(\"```\")):\n",
    "                if bidx % 2 == 0:\n",
    "                    text_blocks.append(block)\n",
    "                else:\n",
    "                    code_blocks.append(block)\n",
    "\n",
    "            for tidx, text_block in enumerate(text_blocks):\n",
    "                prompt = f\"### 영어:\\n{text_block}\\n### 한국어:\\n\"\n",
    "                input_json = {\n",
    "                    \"model_name\": \"Gugugo-koen-7B-V1.1\",\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.8,\n",
    "                    \"max_new_tokens\": 4096,\n",
    "                    \"stop\": [\"</끝>\", \"###\"],\n",
    "                }\n",
    "\n",
    "                ret = requests.post(\n",
    "                    api_server_url + \"/worker_generate_stream\",\n",
    "                    json=input_json,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                for chunk in ret.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n",
    "                    if chunk:\n",
    "                        result_data = json.loads(chunk.decode())\n",
    "\n",
    "                result = result_data['text'][len(prompt):].rstrip('\\n')\n",
    "                results += result\n",
    "                if len(code_blocks) > tidx:\n",
    "                    results += \"```\" + code_blocks[tidx] + \"```\"\n",
    "            \n",
    "            new_conv.append({\n",
    "                'from': _data['from'],\n",
    "                'value': results,\n",
    "            })\n",
    "        new_dataset.append({\n",
    "            'conversations': new_conv,\n",
    "            'id': data['id'],\n",
    "        })\n",
    "\n",
    "\n",
    "# code_prefixes = [\"python\", \"c++\", \"minikube\", \"docker\", \"json\", \"java\", \n",
    "#                  \"php\", \"bash\", \"c#\", \"cpp\", \"css\", \"perl\", \"html\", \"xml\", \n",
    "#                  \"ruby\", \"sql\", \"ini\", \"apt\", \"socat\", \"tcp\", \"localhost\",\n",
    "#                  \"git\"]\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(lang_dict['__label__en'])\n",
    "n_thread = 64 * 7\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_translate_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ee2df-af0f-4e35-bf6d-67774841dde0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
