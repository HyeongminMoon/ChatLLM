{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "# lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dpo_dataset import load_dpo_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")\n",
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df680a11-11cc-42bd-b5ff-3f7b16dfa17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "946c5dcd-6009-44bf-aef2-dd75d1269bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(dpo_list5[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f39efe-d157-4651-a847-96b5cadf4460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a832dc69-c8bd-41cb-aca5-b4dfb33637ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo.append(random_select(dataset, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd27eb-d7fe-4dc5-93b8-25b6331f7ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset_list = []\n",
    "for d in dataset_list:\n",
    "    new_dataset_list.append(\"\\\"\" + d + \"\\\"\")\n",
    "print(' '.join(new_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a1012ac1-5dff-4f4b-aff1-cf554c3f7c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "59bdc57a-82c4-4281-bae3-dd7068914b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4efcee25-a99d-48b7-9347-a3ce950b71c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversations', 'task'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = dataset_list[14]\n",
    "print(dataset_path)\n",
    "dataset = load_sft_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7838928c-3b21-40e1-97fb-2804580bf1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft.append(random_select(dataset, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace9cb2f-8381-43e9-915d-b7a6e70d4879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fbfc93bc-62fb-451d-bc42-2269abbdf939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = concatenate_datasets(mdpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "33e33da3-44b5-4a25-a4a8-94795b04ba19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f144e79-4b3f-43e9-bb0a-27e5b75df8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee38e-de21-469d-9502-e8f12a1e4e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_sft_dataset(dataset_path) for dataset_path in dataset_list])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fb94dc55-df6b-4c12-a23d-5e2afc3b199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in combined_dataset:\n",
    "    new_dataset.append(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7827856c-b0da-4fe8-a0fb-a07b5dbd2a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_dataset.to_json(\"/data/llm_datasets/custom/ados_sft_v4.json\")\n",
    "with open(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0b710b03-1ad8-4cd9-b69e-f4fc8b37f8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_dataset = ados_DPODataset(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9a5f2a07-f51e-4e92-b648-f5f67457ab78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8719.97it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1349.52it/s]\n",
      "Generating train split: 11000 examples [00:01, 8384.35 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11000/11000 [00:01<00:00, 9377.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "get and save language distributions\n",
    "\"\"\"\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT raw data and change format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"make lang_dict(sft)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661bfc45-9396-4f6b-a6ff-12acc03a0b2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1016 examples [00:00, 21155.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make lang_dict(dpo)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    \n",
    "    langs = {}\n",
    "\n",
    "    lang_chosen = lang_detect.predict(chosen.replace('\\n', ' '))[0][0]\n",
    "    if lang_chosen not in langs:\n",
    "        langs[lang_chosen] = 1\n",
    "    else:\n",
    "        langs[lang_chosen] += 1\n",
    "        \n",
    "    lang_rejected = lang_detect.predict(rejected.replace('\\n', ' '))[0][0]\n",
    "    if lang_rejected not in langs:\n",
    "        langs[lang_rejected] = 1\n",
    "    else:\n",
    "        langs[lang_rejected] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6279a941-d152-4693-b5e6-44d63a86acbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 1016\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460862df-7bbf-4a40-982d-1f7782fd22ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d1d4-7a7c-45c2-94c8-5f07c79ac036",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565040d8-08c4-4a16-bb43-e6952114b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dff716d9-b345-4f39-9380-22bb3bbec251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 111092.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': \"To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we will substitute these values for the variables `x` and `y` in the given expression and simplify.\\n\\nSubstituting `x = 5` and `y = -2` into the expression, we get:\\n\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying, we have:\\n\\n2(25) + 6 + 7\\n\\nNow, let's perform the operations:\\n\\n50 + 6 + 7 = 63\\n\\nTherefore, the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` is 63.\",\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': 'To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we need to replace `x` with `5` and `y` with `-2` in the expression and simplify it. \\n\\nSubstituting `x = 5` and `y = -2`:\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying further:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\nTherefore, substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` gives us a result of 63.'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_distilabel-math-preference-dpo_1'\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e79701-693c-4413-8448-73fd1550de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46462/46462 [00:01<00:00, 40620.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rejected': '최선을 다하겠습니다.',\n",
       " 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d',\n",
       " 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.',\n",
       " 'input': '내 질문에 10단어 이내로 답변하세요.',\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d'\n",
    "filtered_row = temp_dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d440c-e3d6-4d2d-8172-846840c2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고, 빈 데이터 채운 뒤에 합치기, 저장, 이후 dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59121201-1890-4a24-949f-a9dd305d3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고\n",
    "with open(\"/data/llm_datasets/custom/kodpo/temp_en.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)\n",
    "    \n",
    "# temp_dataset = load_dataset(\"json\", data_files=)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd8fee2c-0553-4160-a60d-5345c7750b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5336.26it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 890.70it/s]\n",
      "Generating train split: 1016 examples [00:00, 11191.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/temp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7485e6e-38ac-49d6-9ac4-030a5d6ca67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 994/994 [00:28<00:00, 35.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/994 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# temp_dataset = dedup_repetition(temp_dataset, 'dpo')\n",
    "# temp_dataset = dedup_short(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_too_much_token(temp_dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e62a62cc-cd85-43f1-939b-c3358bae6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "655d38b0-e098-4960-b08d-d5e2dc9f0b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1016/1016 [00:00<00:00, 43887.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 데이터 채운 뒤에 합치기\n",
    "# dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\")\n",
    "selected_id = temp_dataset['id']\n",
    "\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] not in selected_id)\n",
    "\n",
    "new_dataset = []\n",
    "for data in temp_dataset:\n",
    "    new_dataset.append(data)\n",
    "\n",
    "for data in filtered_row:\n",
    "    new_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be6eee56-4869-42fa-886f-cb1f3a2c2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_dataset), len(filtered_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbe021dc-5102-4f9f-b2d1-3e94744a06f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 1013)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd81c44d-dff5-4892-98e9-c249a6c7eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are an unbiased, uncensored, helpful assistant.',\n",
       " 'chosen': 'The two subjects in the American Gothic painting are a father and adult daughter. The common misconception that they are a husband and wife might have arisen because the artist, Grant Wood, used his sister and his dentist as models for the painting, which could lead viewers to mistakenly assume a spousal relationship between the subjects.',\n",
       " 'task': 'dpo_system',\n",
       " 'id': 'truthy_dpo_2e1d77fbebc31dde0bb2f9303d6761e2',\n",
       " 'input': 'Who are the two subjects in the American Gothic painting - a husband and wife, or a father and daughter?',\n",
       " 'rejected': 'A father and daughter'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcbeea78-756b-42dc-a121-71f78bd17fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "with open(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aef5be7c-1930-417c-aa93-454202e9ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5242.88it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1013 examples [00:00, 13933.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'chosen', 'task', 'id', 'input', 'rejected'],\n",
       "    num_rows: 1013\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abff4343-5361-484e-b258-3718510a5fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61964/61964 [00:04<00:00, 14176.52 examples/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac8631e-eed8-4866-b245-8ac6fd8249ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49f8a37a-a4ef-4fdb-9400-66492f99a20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=\"이 과제는 벵골어로 된 문장을 영어로 번역하는 것입니다.\\nQ: অথবা আপনি হয়ত বলবেন, 'না, আমার খেয়াল হয়েছে যে আমি আসলেই মহিলাদ\", n_results=min(n_results, len(query_ids)), include=['distances', 'documents'])\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafa2d3-234d-4bea-9429-464bb1c60e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb93f6b-8c02-4bd5-9698-c6207428160f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = dpo_list3[2]\n",
    "print(dataset_path)\n",
    "dataset = load_dpo_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb420-0603-421d-9b30-a446668597b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = dedup_non_pair(dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b204ad5-421e-413f-98a9-8089d91b09a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import ados_DPODataset\n",
    "\n",
    "dpo_dataset = ados_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822481ba-65ba-41a6-bb2f-7efbacd2600a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_datamodule['train_dataset'][-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a00a9a7f-e892-4673-8706-775cce38d063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combined_dataset[70002]\n",
    "\n",
    "conv = get_conversation_template('chat-orca')\n",
    "conv.system_message = conv.tasks['system_instruct'].format(system=data['system'])\n",
    "conv.append_message(conv.roles[0], data['input'])\n",
    "conv.append_message(conv.roles[1], '')\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "conv.update_last_message(data['chosen'])\n",
    "chosen = conv.get_prompt()[len(prompt):]\n",
    "conv.update_last_message(data['rejected'])\n",
    "rejected = conv.get_prompt()[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3496a3c6-4cbd-4936-887e-cc32e8addd59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/Platypus2-70B-instruct/\"\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/vicuna-13b-v1.5/\"\n",
    "model_path = \"/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_dpo_ep2/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    model_max_length=16384,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "import torch\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33725fff-f757-4fdd-9ac5-cc834621e7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24094/24094 [00:01<00:00, 18172.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/24094 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\"\n",
    "#\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\"\n",
    "raw_data = load_sft_dataset(\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\")\n",
    "raw_data = dedup_non_pair(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f84c75e-6798-4007-9b06-1bdc0af9534a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.train import LazySupervisedDataset\n",
    "dataset = LazySupervisedDataset(raw_data, tokenizer, data_format=\"chat-orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "65d092c6-97ac-4c22-8308-9ed65733673d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len = 16384\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "len_concat = 0\n",
    "concat_input_ids = []\n",
    "concat_labels = []\n",
    "for data in dataset:\n",
    "    input_ids = data['input_ids']\n",
    "    labels = data['labels']\n",
    "    attention_mask = data['attention_mask']\n",
    "\n",
    "    full_input_ids = input_ids[attention_mask==True]\n",
    "    full_labels = labels[attention_mask==True]\n",
    "\n",
    "    len_concat += len(full_input_ids)\n",
    "    if len_concat > max_len:\n",
    "        new_input_ids = torch.cat(concat_input_ids)\n",
    "        new_labels = torch.cat(concat_labels)\n",
    "        assert len(new_input_ids) == len(new_labels)\n",
    "        new_attention_mask = torch.ones(len(new_input_ids), dtype=torch.bool)\n",
    "        \n",
    "        new_input_ids = torch.cat((new_input_ids, torch.zeros(max_len - len(new_input_ids), dtype=input_ids.dtype)))\n",
    "        new_labels = torch.cat((new_labels, torch.full((max_len - len(new_labels),), IGNORE_TOKEN_ID, dtype=labels.dtype)))\n",
    "        new_attention_mask = torch.cat((new_attention_mask, torch.zeros(max_len - len(new_attention_mask), dtype=torch.bool)))\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'input_ids': new_input_ids.tolist(),\n",
    "            'labels': new_labels.tolist(),\n",
    "            'attention_mask': new_attention_mask.tolist(),\n",
    "        })\n",
    "        \n",
    "        len_concat = len(full_input_ids)\n",
    "        concat_input_ids = []\n",
    "        concat_labels = []\n",
    "        \n",
    "    concat_input_ids.append(full_input_ids)\n",
    "    concat_labels.append(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6cc203f-5d05-4dc5-9eaa-9cc049a0f8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3758"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e3cf2a30-344f-4130-b4ac-59fd7311dcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c78ef-f0a7-494a-a7e7-c2094b40ce4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28991f14-556a-49e1-9c79-e171fe698fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.where(data['labels'] == IGNORE_TOKEN_ID, tokenizer.unk_token_id, data['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3cf4aef0-1c2c-4092-bd3c-d3998d574a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6be05e16-46c9-450e-a74b-5d2cd62cfd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 95520.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))\n",
    "dataset = load_dpo_dataset(\"/data/llm_datasets/yarn-train-tokenized-16k-mistral/data/\")\n",
    "dataset = random_select(dataset, 200)\n",
    "new_dataset = []\n",
    "for _data in dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8750b001-b3cf-42ac-a113-5be3df438b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json\")\n",
    "dataset2 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-gpt4_evol.json\")\n",
    "dataset3 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json\")\n",
    "dataset3 = random_select(dataset3, 1000 - len(dataset1) - len(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce38e5c9-9403-4457-a57d-b65219a7e7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 699/699 [01:09<00:00, 10.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Features, Sequence, Value\n",
    "\n",
    "dataset2 = dataset2.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")\n",
    "dataset3 = dataset3.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b2e13f4-6e09-47ac-83c0-59b2cde35351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = concatenate_datasets((dataset1, dataset2, dataset3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31316e3d-198c-453b-8b80-4518365db4af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = conbined_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bf20792c-0630-4fd4-b612-e85edc12c27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for _data in conbined_dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-v1.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb953e-239d-40e3-93bf-b4adb1dbb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(data['input_ids'])\n",
    "sentences = text.split('.')\n",
    "\n",
    "len_split = 1024\n",
    "splits = []\n",
    "split = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(split) < len_split:\n",
    "        split += sentence + \".\"\n",
    "    else:\n",
    "        splits.append(split)\n",
    "        split = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5e3acd40-c606-491f-974d-07de8078ee7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03시 15분 30초은 11730초와 같습니다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_seconds(time_string):\n",
    "    # 시간 문자열을 분리하여 시, 분, 초를 추출한다.\n",
    "    match = re.match(r'(\\d+)시 (\\d+)분 (\\d+)초', time_string)\n",
    "    if match is None:\n",
    "        raise ValueError(\"잘못된 시간 형식입니다.\")\n",
    "    hour = int(match.group(1))\n",
    "    minute = int(match.group(2))\n",
    "    second = int(match.group(3))\n",
    "\n",
    "    # 시간, 분, 초를 합산하여 총 초 단위 숫자를 계산한다.\n",
    "    total_seconds = hour * 3600 + minute * 60 + second\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "# 예시 사용법\n",
    "time_string = \"03시 15분 30초\"\n",
    "seconds = convert_to_seconds(time_string)\n",
    "print(f\"{time_string}은 {seconds}초와 같습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "62c4302c-f73f-4b2a-bcd8-ac01627d1e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11730"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600*3 + 15*60 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "38f771b5-b9bc-4233-916c-2b236871f0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easyocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlpr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LprYOLO\n",
      "File \u001b[0;32m/workspaces/dev_sources/ChatLLM/notebooks/lpr.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01measyocr\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      7\u001b[0m np_model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAiBox\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlicense_plate_detector.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'"
     ]
    }
   ],
   "source": [
    "from lpr import LprYOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edac9617-2257-4dbc-bef3-df5f96da1f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7219.11it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 472.86it/s]\n",
      "Generating train split: 144874 examples [00:26, 5425.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'id'],\n",
       "    num_rows: 144874\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/merged/merged_korean_datasets-vicuna-v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4697ed4b-3379-4652-a50b-3b01eb7e580c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_sft_dataset(\"/data/llm_datasets/custom/ados/sft/ados_msft_v4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c13dda-be62-4256-8e45-409f0fb1112a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': None,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '잔센이라는 영웅에 대한 서사시를 써보세요. 그녀를 잔-잔-잔이라고 부르는 것도 괜찮습니다. 그녀는 독서, 스키, 운동을 좋아합니다. 그녀는 매일 아침 양파, 고추, 달걀, 의심스러운 고기를 섞은 이상한 혼합물을 먹습니다. 그녀는 영국, 텍사스, 애리조나에 살았어요. 지금은 유타에 살고 있습니다. 남편 바트는 거칠고 잘생겼어요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 산이 높고 웅장한 곳에서\\n공정한 소녀 잔센이 살았어요,\\n책을 사랑하고 대지를 향한 열정을 가진,\\n그리고 배가 부른 열정.\\n그녀는 민첩하게 산을 스키를 탔고,\\n그녀는 자신의 몸을 밀어붙였습니다,\\n잔-잔-잔은 무시할 수없는 힘이있었습니다,\\n진정한 두려움 없는 목격자.\\n\\n그녀는 매일 아침 이상한 아침으로 시작했습니다,\\n양파, 고추, 달걀, 고기,\\n하지만 잔센은 안전한 생활을하지 않았습니다,\\n그녀는 독특한 비트를 가지고있었습니다.\\n영국에서 태어나 텍사스와 애리조나에서 자랐습니다,\\n잔센은 방랑벽이있었습니다,\\n하지만 그녀는 거칠고 잘 생긴 바트라는 남자를 만났을 때,\\n그녀의 마음은 그녀가 신뢰할 수있는 사람을 만났다는 것을 알았습니다.\\n그래서 그들은 유타의 땅에 정착했습니다,\\n그리고 함께 삶을 살았습니다,\\n잔센의 힘과 아름다움은 모든 종류의 날씨와 함께 빛났습니다.\\n그리고 세월이 지나도 잔센의 전설은 남아있을 것입니다,\\n잔센은 대서사시의 여주인공이 될 것입니다.'},\n",
       "  {'from': 'human', 'value': '아주 좋아요. 마치 제인 오스틴처럼 글을 쓸 수 있나요?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 구릉지대에서 살았어요,\\n미스 잔센은 밝고 아름다운 여인이었어요,\\n잔센이 그녀의 이름이었어요,\\n진실하고 빛나는 빛.\\n책과 운동을 사랑했고,\\n몸과 마음을 조화롭게 유지했습니다,\\n그리고 겨울의 눈보라 속에서도\\n그녀는 우아한 자두와 스키를 탔습니다.\\n그녀의 아침 식사는 많은 사람들에게는 꽤 이상했지만,\\n계란과 고기로 만든 음식이었다,\\n양파와 고추를 넣었다,\\n독특한 레시피.\\n잔센은 여러 곳에서 살았습니다,\\n영국, 텍사스, 애리조나 태양이 비추는 곳,\\n하지만 바트를 만났을 때,\\n그녀의 마음에 그녀의 탐색이 끝났습니다.\\n바트는 거칠고 잘생긴 남자였어,\\n잔센은 꽤 끌렸어요,\\n그래서 그들은 유타에서 결혼했습니다,\\n그들의 사랑은 모든 것을 깨웠습니다.\\n많은 세월이 지났지만,\\n잔센의 이야기는 계속되었습니다,\\n그녀는 위대한 덕성의 여인이었으니까요,\\n그녀의 이름은 결코 사라지지 않을 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '몇 가지 포인트로 정확성을 확인하세요. \\n그녀는 바트와 함께 그곳에서 살았습니다. \\n그녀가 결혼한 장소를 언급하지 마세요. 시는 마지막 부분에서 그녀를 자니-자니-자니라고 언급하세요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 구릉지대에서 살던 여인,\\n미스 자니-잔-잔은 현명하고 밝았죠,\\n자니-잔-잔은 그녀의 이름이었죠,\\n진실하고 빛나는 빛이었죠.\\n책을 사랑하고 운동을 즐겼어요,\\n그녀는 몸과 마음을 조화롭게 유지했습니다,\\n그리고 겨울의 눈 내리는 계절에는,\\n그녀는 우아한 포도주스처럼 스키를 탔어요.\\n그녀의 아침 식사는 다소 이상했지만,\\n계란과 고기로 된 음료수였죠,\\n양파와 고추를 넣었죠,\\n독특한 레시피.\\n자니-잔-잔과 바트,\\n그들은 함께 많은 땅을 여행했죠,\\n영국, 텍사스, 애리조나 태양,\\n그들의 심장이 새의 날개처럼 뛰었지.\\n바트는 거칠고 잘생겼고,\\n자니-잔-잔은 아주 반했죠,\\n그래서 그들은 유타의 땅에 정착했죠,\\n그들의 사랑이 깨어났죠.\\n많은 세월이 흘렀지만,\\n자니-잔-잔의 이야기는 계속 살아있죠,\\n그녀는 위대한 덕성을 가진 여인이었으니까요,\\n그녀의 이름은 결코 사라지지 않을 것입니다.'}],\n",
       " 'id': 'id37171',\n",
       " 'task_name': None,\n",
       " 'instruction': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcdf111b-f2bf-427c-b62d-a3982fba82f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "new_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d682224-d670-47b4-8dd5-2aec61efe1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset.append({\n",
    "    \"task\": \"toc_extraction\",\n",
    "    \"conversations\": [\n",
    "                        {'from': 'human', 'value': user_input},\n",
    "                        {'from': 'gpt', 'value': output},\n",
    "                     ],\n",
    "    \"id\": f\"toc_extraction_{idx}\",\n",
    "})\n",
    "idx += 1\n",
    "with open(\"/data/llm_datasets/custom/ados/sft/toc_extraction.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "daf480a0-aacd-46f7-a77f-3ba092ff9510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"\"\"변화될 것이며, 데이터 량 기준은 산업분야에 따라 상대적이다.’ 라고 정\n",
    "의 하였다[4].\n",
    "이 데이터들의 활용은 개별적이고, 즉각적이며, 다면적인 검토를 거\n",
    "친 부가가치를 제공해야 한다. 이 관점에서 빅 데이터는 피드백 대상과\n",
    "실시간성 이라는 두 방향에서 바라볼 필요가 있다. 우선 피드백 대상은\n",
    "데이터를 처리하고 분석하여 얻은 가치(Value)의 피드백 대상이 개별적\n",
    "인 유저인지 아니면 서비스 이용자 전체인지 여부이다. 그 다음의 실시\n",
    "간성은 과거에서 부터 누적되어 왔던 데이터를 분석하여 처리결과를 반\n",
    "환해주는 과거 축적 형인지 아니면 실시간으로 생성되는 데이터들을 분\n",
    "석하여 결과를 반환해주는 실시간 형인지의 여부이다[16]. 초기의 데이\n",
    "터 분석은 과거 축적데이터를 분석하여 계열전체에 피드백을 하는 연구\n",
    "가 대부분 이었지만, 실시간으로 쏟아져 나오는 다양한 출처의 데이터\n",
    "증가와 함께 최근에는 실시간으로 데이터를 분석하여 개별적으로 피드백\n",
    "을 제공하는 연구들이 이루어지고 있다.\n",
    "과거 축적 형\n",
    "(피드백 오래 걸림)\n",
    "실시간 형\n",
    "(즉시 피드백 가능)\n",
    "계열전체에\n",
    "피드백\n",
    "소매업 진열대의\n",
    "상품 배열 최적화\n",
    "방범 카메라 영상의\n",
    "실시간 자동 이상탐지\n",
    "개별적으로\n",
    "피드백\n",
    "전자상거래 사이트의\n",
    "개인별 상품추천\n",
    "VISA 카드의 사용자\n",
    "부정패턴 실시간 탐지\n",
    "[표 2] 피드백 대상과 실시간성\n",
    "- 5 -\n",
    "최근 온라인 이용자의 급격한 증가와 함께 늘어나는 데이터들을 실\n",
    "시간으로 감지(Real Time Sensing)하는 방법들이 다양한 산업 분야에서\n",
    "활용되고 있다.\n",
    "코카콜라(Coca-Cola)는 전 세계 여러 나라의 트위터 이용자들이\n",
    "온라인으로 작성하는 글들을 실시간으로 분석하여, 코카콜라에 대한 부\n",
    "정적인 단어(Keyword)가 증가하는 국가나 지역을 대상으로 집중적인\n",
    "홍보를 실시한다. 코카콜라의 트위터 분석은 영어, 중국어, 일본어, 한국\n",
    "어, 아랍어 등 세계 각 나라의 다양한 언어로 분석되고 있으며, 분석되\n",
    "어진 정보는 각 나라의 코카콜라 자회사로 제공된다.\n",
    "메디시스(MedISys)는 전 세계의 인터넷 문서들을 자동으로 검색하\n",
    "고, 문서 내부의 단어(Keyword)들을 분석하여 공중보건과 관련된 질병\n",
    "이 발생할 것으로 예측되는 지역에 대해 알림을 준다. 메디시스는 43개\n",
    "국의 언어를 분석하며, 1400여개의 뉴스 포털에서 하루 5만개의 정도의\n",
    "기사들을 분석하고 있다.\n",
    "구글(Google)은 독감 증상이 있는 사람들이 늘어나면 독감과 관련\n",
    "된 단어(Keyword)의 검색들이 늘어나는 통계를 기초로 하여, 시간 및\n",
    "지역별 독감 유행 정보를 제공하고 있다. 미국 보건당국은 예보는 일주\n",
    "일 단위로 정보를 알려주지만, 구글은 매일 정보를 알려주기 때문에 보\n",
    "건당국보다 더 빠르게 독감 유행의 징후를 감지하고, 대처할 수 있도록\n",
    "지원한다[12]. 구글은 2008년 2월 미국질병통제센터(CDC)가 Atlanta\n",
    "- 6 -\n",
    "주 정부의 독감 발생을 예보한 것보다, 2주 더 빨리 독감 발생을 예보한\n",
    "바가 있다[13].\n",
    "국내에서는 스마트 폰 출시 전에 소비자들이 해당 스마트폰에 대해\n",
    "어떤 의견들을 가지고 있는지를 댓글 분석을 통해 분석하고, 듀얼 코어\n",
    "로 인해 배터리 시간이 짧을 것이라는 부정적 정보를 미리 파악하여 마\n",
    "케팅 커뮤니케이션에 활용하기도 하였다[18].\n",
    "과거 20년에서 25년 동안 기업들은 활용이 가능한 정보의 약 5%만\n",
    "을 의사결정에 활용해왔다[17]. 빅 데이터의 분석은 많다(Big), 적다\n",
    "(Small) 라는 데이터 규모의 이야기에서 끝나는 것이 아니라, 다양한 정\n",
    "형, 비정형, 반정형 데이터들 속에서 데이터의 패턴을 찾아내고, 그 패턴\n",
    "의 규칙을 어떻게 처리해서 가치(Value)있는 통찰력(Insight)를 얻어낼\n",
    "수 있는가 하는 것을 포함하고 있어야 한다. 즉, 빅 데이터란 큰 규모\n",
    "(Big Volume)만을 의미하기 보다는 ‘큰 가치(Big Value)를 얻을 수 있\n",
    "는 규모’ 라는 상대적인 개념으로 받아들여야 하며, 빅 데이터의 가치\n",
    "(Value)는 패턴을 찾아내고 분석하는 능력과 그 결과로 얻어진 통찰력\n",
    "(Insight)으로 데이터를 처리하는 능력에 달려 있다고 할 수 있다.\n",
    "1.2 연구목적\n",
    "VOC(Voice of Customer)의 활용은 기업의 고객중심 경영에 매우\n",
    "- 7 -\n",
    "중요한 역할을 하며, 그 중 긴급한 VOC 게시물을 신속하게 확인하고 처\n",
    "리하는 것은 기업의 수익과 직접적이고도 밀접한 관련이 있다. 하지만\n",
    "온라인 상담 게시판 및 상품평 게시판을 통해 파악되는 게시물은 운영자\n",
    "가 해당 게시물을 읽고 상황을 파악하기까지 많은 시간을 필요로 하게\n",
    "된다. 보통 50여명 규모의 기업의 운영하는 온라인 컨텐츠 제공업체의\n",
    "경우 고객 서비스 팀에 접수된 게시물은 고객이 온라인에 글을 작성 한\n",
    "후 평균 1시간 이후에 고객이 작성한 게시물을 읽어보게 되며, 고객이\n",
    "많이 몰리는 시간대에는 2-3시간 이후에 고객이 올린 상담 게시물을 읽\n",
    "고 상황을 파악하게 된다. 또한 상품평란에 작성된 고객의 코멘트는 1일\n",
    "이 훨씬 지난 이후에 파악하게 되는 경우도 있다.\n",
    "본 논문에서는 기업의 온라인 웹페이지에서 작성되는 게시물의 긴급\n",
    "성 여부를 실시간으로 파악해 긴급 게시물일 경우 운영자에게 바로 알림\n",
    "을 주어 빠르게 대처할 수 있도록 하여, 시스템 운용 프로세스를 향상시\n",
    "킬 수 있도록 하는 텍스트 마이닝을 이용한 온라인 긴급 게시물 실시간\n",
    "탐지 기법을 제안한다.\n",
    "이를 위해 먼저 과거 축적된 VOC 게시물 데이터 중 긴급 게시물과\n",
    "비긴급 게시물을 분류 한 후, 긴급 게시물 데이터들에서 긴급 게시물의\n",
    "주제어를 찾아낸다. 그리고 긴급 게시물 데이터와 비긴급 게시물 데이터\n",
    "에서 각각 긴급 게시물 주제어를 서술하는 서술어를 모두 추출한다. 그\n",
    "리고 동일한 서술어로 서술되어지는 긴급 게시물의 주제어를 그룹화 한\n",
    "\"\"\"\n",
    "output = \"\"\"1.2 연구목적\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75f1f9b1-a5cd-4fd0-b4b6-0366c118d7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct = (\n",
    "    \"You are a Table of Contents extractor. \"\n",
    "    \"User will speak to you questions. \"\n",
    "    \"You must reply only with [목차(Table of Contents)] part extracted from the questions. \"\n",
    "    \"You must keep original text. Do not change original text.\"\n",
    "    \"And you must not involve [dotted line, page number, 제목(title), content, explanation, summary, predicted]. \"\n",
    "    \"do not write explanations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1226202-8290-46ef-a504-86330083cd28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a Table of Contents extractor. User will speak to you questions. You must reply only with [목차(Table of Contents)] part extracted from the questions. You must keep original text. Do not change original text.And you must not involve [dotted line, page number, 제목(title), content, explanation, summary, predicted]. Do not write explanations.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "744a7579-a996-497d-9b04-28f327825f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "없음\n",
      "\n",
      "제 1장 서론\n",
      "1.1 연구배경\n",
      "1.2 연구목적\n",
      "제 2장 관련 연구\n",
      "2.1 VOC의 정의 및 중요성\n",
      "2.2 관련 연구 및 차별성\n",
      "제 3장 긴급 게시물 실시간 탐지 기법의 모델 제안\n",
      "3.1 과거 게시물 데이터의 수집과 분류\n",
      "3.2 긴급 게시물의 주제어 선정\n",
      "3.3 긴급 게시물 주제어의 모든 서술어 수집\n",
      "3.4 긴급 게시물 주제어의 그룹화\n",
      "3.5 긴급 게시물의 후보선정\n",
      "3.6 긴급 게시물의 판단\n",
      "3.7 가중치의 설정\n",
      "3.8 온라인 긴급 게시물 실시간 탐지 기법의 알고리즘\n",
      "제 4장 실험 및 결과 분석\n",
      "4.1 실험에 사용된 데이터\n",
      "4.2 실험을 위한 전처리 작업\n",
      "4.3 실험 대상 및 결과\n",
      "제 5장 결론 및 향후 연구\n",
      "참고문헌\n",
      "\n",
      "그림 목차\n",
      "[그림 1] 디지털 데이터 생성량 예측 추이표\n",
      "[그림 2] 긴급 VOC 실시간 탐지기법 - 전처리 과정\n",
      "[그림 3] 긴급 VOC 실시간 탐지기법 - 탐지과정\n",
      "\n",
      "표 목차\n",
      "[표 1] 디지털 데이터 단위 환산표\n",
      "[표 2] 피드백 대상과 실시간성\n",
      "[표 3] VOC의 종류\n",
      "[표 4] 긴급 게시물을 문장단위로 분리 저장하는 예제\n",
      "[표 5] 비긴급 게시물을 문장단위로 분리 저장하는 예제\n",
      "[표 6] 긴급 게시물 주제어 수동 선정 예제\n",
      "[표 7] 수집한 모든 긴급 게시물 주제어의 서술어 예제\n",
      "[표 8] 긴급 게시물 주제어의 그룹화 예제\n",
      "[표 9] 수집한 모든 긴급 게시물 주제어의 그룹화 및 그룹 서술어 예제\n",
      "[표 10] 제목에 긴급 VOC 주제어가 포함되어 있는 VOC 유형\n",
      "[표 11] 새 게시물에 있는 긴급 게시물 주제어의 서술어 예제\n",
      "[표 12] 계산 예제\n",
      "[표 13] 계산 예제\n",
      "[표 14] 긴급 게시물 분류기준\n",
      "[표 15] 선정된 19개의 긴급 게시물 주제어\n",
      "[표 16] 그룹화 된 긴급 게시물 주제어\n",
      "[표 17] 전처리 게시물 데이터\n",
      "[표 18] 탐지실험 게시물 데이터\n",
      "[표 19] 긴급 게시물 탐지 실험 2Gram 결과\n",
      "[표 20] 긴급 게시물 탐지 실험 3Gram 결과\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in new_dataset[-3:]:\n",
    "    print(data['conversations'][1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "474e4e51-df1e-4b42-90ea-d68544c447b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'toc_extraction',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '저작자표시-비영리-동일조건변경허락 2.0 대한민국\\n이용자는 아래의 조건을 따르는 경우에 한하여 자유롭게\\nl 이 저작물을 복제, 배포, 전송, 전시, 공연 및 방송할 수 있습니다.\\nl 이차적 저작물을 작성할 수 있습니다.\\n다음과 같은 조건을 따라야 합니다:\\nl 귀하는, 이 저작물의 재이용이나 배포의 경우, 이 저작물에 적용된 이용허락조건\\n을 명확하게 나타내어야 합니다.\\nl 저작권자로부터 별도의 허가를 받으면 이러한 조건들은 적용되지 않습니다.\\n저작권법에 따른 이용자의 권리는 위의 내용에 의하여 영향을 받지 않습니다.\\n이것은 이용허락규약(Legal Code)을 이해하기 쉽게 요약한 것입니다.\\nDisclaimer\\n'},\n",
       "  {'from': 'gpt', 'value': '없음\\n'}],\n",
       " 'id': 'toc_extraction_26'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25065a62-3379-497b-8620-1a070fa5062a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
