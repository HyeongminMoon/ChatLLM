{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6737de8-e4c0-4865-b86e-3e9a90417890",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "cache_dir = None\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6573a97-32e2-4466-bc1f-9412f4ef59e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "\n",
    "def extract_anthropic_prompt(prompt_and_response, search_term=\"\\n\\nAssistant:\"):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "class hankang_DPODataset:\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_path=\"/data/llm_datasets/Ultrafeedback_binarized.ko.hankang/\",\n",
    "        data_format='chat-orca',\n",
    "        search_term='\\n\\n### Assistant:',\n",
    "        num_train=None,\n",
    "        num_eval=None,\n",
    "    ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.data_format = data_format\n",
    "        self.search_term = search_term\n",
    "        self.num_train = num_train\n",
    "        self.num_eval = num_eval\n",
    "    \n",
    "    def get_prompt_and_response(self, data):\n",
    "        conv = get_conversation_template(self.data_format)\n",
    "\n",
    "        for idx, _conv in enumerate(data):\n",
    "            role = _conv['role']\n",
    "            content = _conv['content_kr']\n",
    "            if idx % 2 == 0 and role == 'user':\n",
    "                conv.append_message(conv.roles[0], content)\n",
    "            elif idx % 2 == 1 and role == 'assistant':\n",
    "                conv.append_message(conv.roles[1], content)\n",
    "            else:\n",
    "                print(\"Warning: data type invaild\")\n",
    "\n",
    "        if len(conv.messages) == 0:\n",
    "            print(\"Warning: data is empty\")\n",
    "        if len(conv.messages) % 2 != 0:\n",
    "            print(\"Warning: data has weird pair\")\n",
    "\n",
    "        return conv.get_prompt()\n",
    "    \n",
    "    def make_dpo_data_module(self):\n",
    "        def validate_prompt_and_responses(data) -> bool:\n",
    "            try:\n",
    "                prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "                prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "                prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "                promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            except AssertionError:\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        def split_prompt_and_responses(data) -> Dict[str, str]:\n",
    "            prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "            prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "            prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "            promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": prompt_and_response[len(prompt) :],\n",
    "                \"rejected\": prompt_and_response_rejected[len(promopt_rejected) :],\n",
    "            }\n",
    "                             \n",
    "                             \n",
    "        dataset = load_dataset(self.dataset_path)\n",
    "\n",
    "        train_dataset = dataset['train']\n",
    "        eval_dataset = dataset['test']\n",
    "\n",
    "        original_columns = list(train_dataset.features.keys())\n",
    "\n",
    "        if self.num_train is not None:\n",
    "            train_dataset = train_dataset.select(range(min(len(train_dataset), self.num_train)))\n",
    "        if self.num_eval is not None:\n",
    "            eval_dataset = eval_dataset.select(range(min(len(train_dataset), self.num_eval)))\n",
    "\n",
    "        train_dataset = train_dataset.filter(validate_prompt_and_responses)\n",
    "        train_dataset = train_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        eval_dataset = eval_dataset.filter(validate_prompt_and_responses)\n",
    "        eval_dataset = eval_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773658d4-5976-453e-b91b-d281a76e7393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import hankang_DPODataset\n",
    "\n",
    "dpo_dataset = hankang_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf507f-f317-4c6f-a79d-6f23d0025508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27144451-5587-4598-a4c7-fa25a6be8b0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/alpaca-gpt4-korean_dedup/\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\",\n",
    "    \"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/sharegpt_V3_format_ko_selected.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "correction_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/\"\n",
    "]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped/aihub_summary_data_tech_dedup/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/도서/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/법률/\",\n",
    "    \"/data/llm_datasets/custom/deduped/naver-news-summarization-ko-vicuna_dedup/\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(enko).json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(koen).json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + correction_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f103ce-1d80-48ed-90c7-9c57ab4e2af0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dedup2\n",
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\",#\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",#\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_ko_selected_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "# correction_list = [\n",
    "#     \"/data/llm_datasets/custom/deduped2/KoreaSpellingCorrection-10000.json\",\n",
    "# ]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_tech_dedup-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_book-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_law-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/naver-news-summarization-ko-vicuna_dedup-5000.json\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3046237a-9d36-4f86-83dc-39e83d5a212a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refine\n",
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json\",#\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",#\n",
    "    \"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/refined/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "# correction_list = [\n",
    "#     \"/data/llm_datasets/custom/deduped2/KoreaSpellingCorrection-10000.json\",\n",
    "# ]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_tech_dedup-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_book-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_law-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/naver-news-summarization-ko-vicuna_dedup-5000.json\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea3b7b-b2dc-429e-8123-6ffd1c7c6d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset_list = []\n",
    "for d in dataset_list:\n",
    "    new_dataset_list.append(\"\\\"\" + d + \"\\\"\")\n",
    "print(' '.join(new_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839cdec-b301-40ed-89eb-a0a6512e631e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = dataset_list[7]\n",
    "print(dataset_path)\n",
    "dataset_train = load_sft_dataset(dataset_path)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7ebad0-a770-4519-942d-0f5ddc564efa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6154/6154 [00:00<00:00, 18837.87 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3548/3548 [00:00<00:00, 30495.15 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24094/24094 [00:01<00:00, 22535.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_sft_dataset(dataset_path) for dataset_path in dataset_list])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0858c7-a34f-48aa-bbb3-6051ba445d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" find odd code blocks\"\"\"\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "new_dataset = []\n",
    "code_prefixes = []\n",
    "\n",
    "odd_dataset = []\n",
    "oddd_dataset = []\n",
    "odd_idxs = set()\n",
    "for idx, data in enumerate(train_dataset):\n",
    "    conversations = data['conversations']\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        if _from == 'human': continue\n",
    "        _value = conv['value']\n",
    "        find_iter = re.finditer('```', _value)\n",
    "        temp_num = 0\n",
    "        for ftext in find_iter:\n",
    "            start_index = ftext.start() + 3\n",
    "            new_dataset.append(data)\n",
    "            candidate = re.split(r'[ \\n]', _value[start_index:])[0]\n",
    "            if '```' not in candidate and candidate not in available_code_prefixes:\n",
    "                odd_dataset.append((candidate, data))\n",
    "                code_prefixes.append(candidate)\n",
    "                odd_idxs.add(idx)\n",
    "            temp_num += 1\n",
    "        if temp_num % 2 != 0:\n",
    "            oddd_dataset.append(('odd', data))\n",
    "            odd_idxs.add(idx)\n",
    "\n",
    "print(len(new_dataset), len(odd_dataset), len(oddd_dataset), len(odd_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bcda4-9560-436e-ab5d-d7d291ef1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_idxs = list(range(len(train_dataset)))\n",
    "for od in odd_idxs:\n",
    "    selected_idxs.remove(od)\n",
    "train_dataset = train_dataset.select(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe020b-e9c1-4c8a-853a-196dd01be98c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_prefixes = set(code_prefixes)\n",
    "code_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc164f84-211c-4585-acb1-a810ded34874",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_code_prefixes = set([\n",
    "    '',\n",
    "    'CSS',\n",
    "    'HTML',\n",
    "    'JavaScript',\n",
    "    'Python',\n",
    "    'SQL',\n",
    "    'bash',\n",
    "    'c',\n",
    "    'c++',\n",
    "    'cpp',\n",
    "    'csharp',\n",
    "    'css',\n",
    "    'for',\n",
    "    'html',\n",
    "    'java',\n",
    "    'javascript',\n",
    "    'js',\n",
    "    'json',\n",
    "    'php',\n",
    "    'python',\n",
    "    'ruby',\n",
    "    'sass',\n",
    "    'scss',\n",
    "    'sql',\n",
    "    'sum',\n",
    "    'svg',\n",
    "    'swift',\n",
    "    'xml',\n",
    "    'yaml',\n",
    "    'C#',\n",
    "    'C++',\n",
    "    'CSS',\n",
    "    'Go',\n",
    "    'HTML',\n",
    "    'Java',\n",
    "    'LaTeX',\n",
    "    'MATLAB',\n",
    "    'Markdown',\n",
    "    'Proposals',\n",
    "    'Python',\n",
    "    'R',\n",
    "    'SELECT',\n",
    "    'SQL',\n",
    "    'Swift',\n",
    "    'VBA',\n",
    "    'echo',\n",
    "    'excel-vba',\n",
    "    'find',\n",
    "    'go',\n",
    "    'gpg',\n",
    "    'jsx',\n",
    "    'kotlin',\n",
    "    'latex',\n",
    "    'markdown',\n",
    "    'math',\n",
    "    'matlab',\n",
    "    'meditation',\n",
    "    'mermaid',\n",
    "    'mutt',\n",
    "    'nano',\n",
    "    'r',\n",
    "    'rust',\n",
    "    'scala',\n",
    "    'sh',\n",
    "    'shell',\n",
    "    'sudo',\n",
    "    'xpath',\n",
    "    '{r}',\n",
    "    '.',\n",
    "    '$',\n",
    "    'curl',\n",
    "    'xslt',\n",
    "    'Apex',\n",
    "    'DAX',\n",
    "    'Dockerfile',\n",
    "    'apex',\n",
    "    'applescript',\n",
    "    'arduino',\n",
    "    'asm',\n",
    "    'assembly',\n",
    "    'astro',\n",
    "    'autoit',\n",
    "    'batch',\n",
    "    'bicep',\n",
    "    'blade',\n",
    "    'cmake',\n",
    "    'cmd',\n",
    "    'coffee',\n",
    "    'coffeescript',\n",
    "    'cql',\n",
    "    'csv',\n",
    "    'cypher',\n",
    "    'dart',\n",
    "    'delphi',\n",
    "    'diff',\n",
    "    'dockerfile',\n",
    "    'dot',\n",
    "    'emacs',\n",
    "    'erb',\n",
    "    'fsharp',\n",
    "    'glsl',\n",
    "    'gradle',\n",
    "    'graphql',\n",
    "    'graphviz',\n",
    "    'groovy',\n",
    "    'haskell',\n",
    "    'hcl',\n",
    "    'hlsl',\n",
    "    'html+erb',\n",
    "    'ini',\n",
    "    'jinja',\n",
    "    'ladder',\n",
    "    'lasso',\n",
    "    'less',\n",
    "    'lisp',\n",
    "    'lldb',\n",
    "    'llvm',\n",
    "    'logo',\n",
    "    'lua',\n",
    "    'makefile',\n",
    "    'mathematica',\n",
    "    'metal',\n",
    "    'nginx',\n",
    "    'nix',\n",
    "    'objc',\n",
    "    'objective',\n",
    "    'objectivec',\n",
    "    'pascal',\n",
    "    'perl',\n",
    "    'plaintext',\n",
    "    'plantuml',\n",
    "    'powershell',\n",
    "    'prisma',\n",
    "    'properties',\n",
    "    'proto',\n",
    "    'protobuf',\n",
    "    'py',\n",
    "    'reg',\n",
    "    'rego',\n",
    "    'scheme',\n",
    "    'scratch',\n",
    "    'solidity',\n",
    "    'spss',\n",
    "    'stata',\n",
    "    'stencil',\n",
    "    'terraform',\n",
    "    'toml',\n",
    "    'ts',\n",
    "    'tsx',\n",
    "    'txt',\n",
    "    'typescript',\n",
    "    'vb',\n",
    "    'vba',\n",
    "    'vbnet',\n",
    "    'verilog',\n",
    "    'yml',\n",
    "    'jsp',\n",
    "    'prolog',\n",
    "    'razor',\n",
    "    'CMD',\n",
    "    'G',\n",
    "    'GraphQL',\n",
    "    'Makefile',\n",
    "    'apache',\n",
    "    'c#',\n",
    "    'cython',\n",
    "    'elixir',\n",
    "    'jinja2',\n",
    "    'julia',\n",
    "    'ocaml',\n",
    "    'systemverilog',\n",
    "    'vbscript',\n",
    "    'vhdl',\n",
    "    'vue',\n",
    "    'wasm',\n",
    "    'wolfram',\n",
    "    'zsh',\n",
    "])\n",
    "\n",
    "# for pre in available_code_prefixes:\n",
    "with open(\"available_code_prefixes.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(available_code_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387a1b4-47f2-4688-b06b-ef9c8b23a113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_code_prefixes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        dataset = load_sft_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                conversations = data['conversations']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "                for conv in conversations:\n",
    "                    _from = conv['from']\n",
    "                    _value = conv['value']\n",
    "\n",
    "                    len_conv += len(_value)\n",
    "                    lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                    lang = lang[0]\n",
    "                    if lang not in langs:\n",
    "                        langs[lang] = 1\n",
    "                    else:\n",
    "                        langs[lang] += 1\n",
    "\n",
    "                if '__label__en' in langs:\n",
    "                    langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution(dataset_list)\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1cf0d3-39db-4b2d-bc54-02070a37ee60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809e09-c947-4724-80ab-965e2f66816f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            summary = context_data['summary']\n",
    "\n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "    print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31df058-d4f2-4df8-8d24-57a3330db159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['TL_EE_train'] + dataset_dict['TL_LA_train'] + dataset_dict['TL_ED_train'] + dataset_dict['TL_NA_train']\n",
    "eval_dataset_list = dataset_dict['TL_EE_val'] + dataset_dict['TL_LA_val'] + dataset_dict['TL_ED_val'] + dataset_dict['TL_NA_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf58516-415a-4d89-813e-0712e1486acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b508d1c-fc90-417a-9681-27cdcec772e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef997b-df6c-442e-ad81-64eb7a74f844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        documents = json_data['documents']\n",
    "        for document in documents:\n",
    "            text = document['text']\n",
    "            abstractive = document['abstractive']\n",
    "\n",
    "            summary = abstractive[0]\n",
    "            context = []\n",
    "            for _text in text:\n",
    "                _context = ' '.join([_index_text['sentence'] for _index_text in _text])\n",
    "                context.append(_context)\n",
    "            context = '\\n'.join(context)\n",
    "            \n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400f4d6-274c-4591-9315-1cd9702dc78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c989c-7f09-4326-b3a8-8d606b376917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_law_train']\n",
    "eval_dataset_list = dataset_dict['summary_law_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b13db5-1c0f-4991-bd1e-446127d4dac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310ab0d-cb0b-4a4e-97fb-4606c736d216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary_book*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '**/*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context = json_data['passage']\n",
    "        summary = json_data['summary']\n",
    "        \n",
    "        data_row = {\n",
    "            'id': f\"{file_name}_{idx}\",\n",
    "            'task': 'summarization',\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': context},\n",
    "                                {'from': 'gpt', 'value': summary},\n",
    "                             ],\n",
    "        }\n",
    "        new_dataset.append(data_row)\n",
    "        idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3337-aa61-49fe-8cb2-eea7a14fcb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610abe01-befd-4bc1-b67a-ddf26127d1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_book_train']\n",
    "eval_dataset_list = dataset_dict['summary_book_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1eb8e0-7c27-4d26-916d-77ab6cc5925a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ff1b9-148b-40a4-93c7-e918b5f5d1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data/도서\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a299c-602f-4d11-a362-04c9b34bcd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5c088-6902-416b-899c-8efb4b42e8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36141362-162e-4cea-a636-b6f876481896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*VL*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            qas = context_data['qas']\n",
    "\n",
    "            for _qas in qas:\n",
    "                question = _qas['question-1']\n",
    "                answer = _qas['answer']\n",
    "                question_level = _qas['question_level']\n",
    "                if question_level != '상': continue\n",
    "                data_row = {\n",
    "                    'id': f\"{file_name}_{idx}\",\n",
    "                    'task': 'contextqa',\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                }\n",
    "                new_dataset.append(data_row)\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83775c61-d0a7-4483-b833-7775c9dd9449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6a348-e8fc-4b42-ac62-e6f9adb46743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['VL_EE_train'] + dataset_dict['VL_NA_train'] + dataset_dict['VL_LA_train']+ dataset_dict['VL_ED_train']\n",
    "eval_dataset_list = dataset_dict['VL_EE_val'] + dataset_dict['VL_NA_val'] + dataset_dict['VL_LA_val']+ dataset_dict['VL_ED_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db99aa-0bbc-49d9-8df4-87dcc4daee19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7d65-84d7-41f3-b10c-3aef5ffcfdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list_0 = train_dataset_list[:120000]\n",
    "train_dataset_list_1 = train_dataset_list[120000:240000]\n",
    "train_dataset_list_2 = train_dataset_list[240000:]\n",
    "\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split0.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_0, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split1.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_1, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split2.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_2, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503836eb-d006-410c-9986-65d737526d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bfe09-7e0a-4c23-b9f5-fff6478c1d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d8cbd-68a3-4480-ac47-6a9f0fabb09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/gpt4_evol_1.3k/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687e5db-9ef7-4218-91ad-b65279e1bd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dataset['train'][0]\n",
    "# answer = data['answer']\n",
    "# question = data['question']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afea3a-efc8-4ddb-92c7-c06297e69260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    answer = data['answer']\n",
    "    question = data['question']\n",
    "\n",
    "    data_row = {\n",
    "        'id': f\"gpt_evol_1.3k_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': question},\n",
    "                            {'from': 'gpt', 'value': answer},\n",
    "                         ],\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194f908-d12d-4045-8d78-d1da6ab8375c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716623c-7fb9-482f-83e3-8ebce535c41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115478e-7ce6-4b57-830c-5c6c794e1334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/WizardLM_Orca/wizardlm_orca.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b5486-8910-4cb8-9895-055e64ef5e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    output = data['output']\n",
    "    system = data['system']\n",
    "    instruction = data['instruction']\n",
    "    data_row = {\n",
    "        'id': f\"WizardLM_Orca_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': instruction},\n",
    "                            {'from': 'gpt', 'value': output},\n",
    "                         ],\n",
    "        'task': 'system_instruct',\n",
    "        'system': system,\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a898f0b-379f-42e4-a762-cf30ee4123e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391b407-27d5-487b-8b69-b5d5d7c10983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/data/llm_datasets/KoreaSpellingCorrection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af63b70-f50d-4b53-8d52-4da167c7889d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['test']:\n",
    "    wrong = data['wrong']\n",
    "    correct = data['correct']\n",
    "    data_row = {\n",
    "        'id': f\"KoreaSpelling_Correction_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': wrong},\n",
    "                            {'from': 'gpt', 'value': correct},\n",
    "                         ],\n",
    "        'task': 'correction',\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a29e00-b31e-46a3-818c-9fab9ca3aaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcf31a-81f3-4147-9cac-cd609b3a62b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/test.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3be553-bafc-4bfb-be09-5864eaae1863",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def send_request(new_dataset):\n",
    "    global idx\n",
    "    for _ in range(2):\n",
    "        if idx > len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        data = dataset[subset][idx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "        \n",
    "        _id = data['id']\n",
    "        context = data['context']\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        if answer.lower() == 'yes':\n",
    "            answer = '네'\n",
    "        elif answer.lower() == 'no':\n",
    "            answer = '아니오'\n",
    "        \n",
    "        # response\n",
    "        result = generate_refiner(\n",
    "            model_name,\n",
    "            context,\n",
    "            question,\n",
    "            answer\n",
    "        )\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'id': _id,\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': question},\n",
    "                                {'from': 'gpt', 'value': result},\n",
    "                             ],\n",
    "            'task_name': \"instruct\",\n",
    "            'instruction': context,\n",
    "        })\n",
    "\n",
    "\n",
    "model_name = \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\"\n",
    "subset = 'train'\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(dataset[subset])\n",
    "n_thread = 64\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/lima_vicuna_format/lima_vicuna_format.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e47387-156e-4324-9f03-4081a0447508",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9befa-8fb0-4734-8f01-909c829ee4eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ca8e-0b78-4585-8b54-dbebbb00df99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def send_translate_request(new_dataset):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = lang_dict['__label__en'][pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        conv = data['conversations']\n",
    "        new_conv = []\n",
    "        for _data in conv:\n",
    "            value = _data['value']\n",
    "            # response\n",
    "            results = \"\"\n",
    "            text_blocks = []\n",
    "            code_blocks = []\n",
    "            for bidx, block in enumerate(value.split(\"```\")):\n",
    "                if bidx % 2 == 0:\n",
    "                    text_blocks.append(block)\n",
    "                else:\n",
    "                    code_blocks.append(block)\n",
    "\n",
    "            for tidx, text_block in enumerate(text_blocks):\n",
    "                prompt = f\"### 영어:\\n{text_block}\\n### 한국어:\\n\"\n",
    "                input_json = {\n",
    "                    \"model_name\": \"Gugugo-koen-7B-V1.1\",\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.8,\n",
    "                    \"max_new_tokens\": 4096,\n",
    "                    \"stop\": [\"</끝>\", \"###\"],\n",
    "                }\n",
    "\n",
    "                ret = requests.post(\n",
    "                    api_server_url + \"/worker_generate_stream\",\n",
    "                    json=input_json,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                for chunk in ret.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n",
    "                    if chunk:\n",
    "                        result_data = json.loads(chunk.decode())\n",
    "\n",
    "                result = result_data['text'][len(prompt):].rstrip('\\n')\n",
    "                results += result\n",
    "                if len(code_blocks) > tidx:\n",
    "                    results += \"```\" + code_blocks[tidx] + \"```\"\n",
    "            \n",
    "            new_conv.append({\n",
    "                'from': _data['from'],\n",
    "                'value': results,\n",
    "            })\n",
    "        new_dataset.append({\n",
    "            'conversations': new_conv,\n",
    "            'id': data['id'],\n",
    "        })\n",
    "\n",
    "\n",
    "# code_prefixes = [\"python\", \"c++\", \"minikube\", \"docker\", \"json\", \"java\", \n",
    "#                  \"php\", \"bash\", \"c#\", \"cpp\", \"css\", \"perl\", \"html\", \"xml\", \n",
    "#                  \"ruby\", \"sql\", \"ini\", \"apt\", \"socat\", \"tcp\", \"localhost\",\n",
    "#                  \"git\"]\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(lang_dict['__label__en'])\n",
    "n_thread = 64 * 7\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_translate_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ee2df-af0f-4e35-bf6d-67774841dde0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
