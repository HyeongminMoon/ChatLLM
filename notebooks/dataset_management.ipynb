{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "# lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dpo_dataset import load_dpo_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")\n",
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df680a11-11cc-42bd-b5ff-3f7b16dfa17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "946c5dcd-6009-44bf-aef2-dd75d1269bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(dpo_list5[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f39efe-d157-4651-a847-96b5cadf4460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a832dc69-c8bd-41cb-aca5-b4dfb33637ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo.append(random_select(dataset, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd27eb-d7fe-4dc5-93b8-25b6331f7ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset_list = []\n",
    "for d in dataset_list:\n",
    "    new_dataset_list.append(\"\\\"\" + d + \"\\\"\")\n",
    "print(' '.join(new_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a1012ac1-5dff-4f4b-aff1-cf554c3f7c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "59bdc57a-82c4-4281-bae3-dd7068914b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4efcee25-a99d-48b7-9347-a3ce950b71c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversations', 'task'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = dataset_list[14]\n",
    "print(dataset_path)\n",
    "dataset = load_sft_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7838928c-3b21-40e1-97fb-2804580bf1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft.append(random_select(dataset, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace9cb2f-8381-43e9-915d-b7a6e70d4879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fbfc93bc-62fb-451d-bc42-2269abbdf939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = concatenate_datasets(mdpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "33e33da3-44b5-4a25-a4a8-94795b04ba19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f144e79-4b3f-43e9-bb0a-27e5b75df8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee38e-de21-469d-9502-e8f12a1e4e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_sft_dataset(dataset_path) for dataset_path in dataset_list])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fb94dc55-df6b-4c12-a23d-5e2afc3b199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in combined_dataset:\n",
    "    new_dataset.append(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7827856c-b0da-4fe8-a0fb-a07b5dbd2a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_dataset.to_json(\"/data/llm_datasets/custom/ados_sft_v4.json\")\n",
    "with open(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0b710b03-1ad8-4cd9-b69e-f4fc8b37f8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_dataset = ados_DPODataset(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9a5f2a07-f51e-4e92-b648-f5f67457ab78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8719.97it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1349.52it/s]\n",
      "Generating train split: 11000 examples [00:01, 8384.35 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11000/11000 [00:01<00:00, 9377.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "get and save language distributions\n",
    "\"\"\"\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT raw data and change format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"make lang_dict(sft)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661bfc45-9396-4f6b-a6ff-12acc03a0b2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1016 examples [00:00, 21155.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make lang_dict(dpo)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    \n",
    "    langs = {}\n",
    "\n",
    "    lang_chosen = lang_detect.predict(chosen.replace('\\n', ' '))[0][0]\n",
    "    if lang_chosen not in langs:\n",
    "        langs[lang_chosen] = 1\n",
    "    else:\n",
    "        langs[lang_chosen] += 1\n",
    "        \n",
    "    lang_rejected = lang_detect.predict(rejected.replace('\\n', ' '))[0][0]\n",
    "    if lang_rejected not in langs:\n",
    "        langs[lang_rejected] = 1\n",
    "    else:\n",
    "        langs[lang_rejected] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6279a941-d152-4693-b5e6-44d63a86acbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 1016\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460862df-7bbf-4a40-982d-1f7782fd22ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d1d4-7a7c-45c2-94c8-5f07c79ac036",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565040d8-08c4-4a16-bb43-e6952114b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dff716d9-b345-4f39-9380-22bb3bbec251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 111092.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': \"To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we will substitute these values for the variables `x` and `y` in the given expression and simplify.\\n\\nSubstituting `x = 5` and `y = -2` into the expression, we get:\\n\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying, we have:\\n\\n2(25) + 6 + 7\\n\\nNow, let's perform the operations:\\n\\n50 + 6 + 7 = 63\\n\\nTherefore, the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` is 63.\",\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': 'To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we need to replace `x` with `5` and `y` with `-2` in the expression and simplify it. \\n\\nSubstituting `x = 5` and `y = -2`:\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying further:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\nTherefore, substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` gives us a result of 63.'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_distilabel-math-preference-dpo_1'\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e79701-693c-4413-8448-73fd1550de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46462/46462 [00:01<00:00, 40620.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rejected': '최선을 다하겠습니다.',\n",
       " 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d',\n",
       " 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.',\n",
       " 'input': '내 질문에 10단어 이내로 답변하세요.',\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d'\n",
    "filtered_row = temp_dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d440c-e3d6-4d2d-8172-846840c2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고, 빈 데이터 채운 뒤에 합치기, 저장, 이후 dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59121201-1890-4a24-949f-a9dd305d3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고\n",
    "with open(\"/data/llm_datasets/custom/kodpo/temp_en.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)\n",
    "    \n",
    "# temp_dataset = load_dataset(\"json\", data_files=)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd8fee2c-0553-4160-a60d-5345c7750b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5336.26it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 890.70it/s]\n",
      "Generating train split: 1016 examples [00:00, 11191.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/temp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7485e6e-38ac-49d6-9ac4-030a5d6ca67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 994/994 [00:28<00:00, 35.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/994 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# temp_dataset = dedup_repetition(temp_dataset, 'dpo')\n",
    "# temp_dataset = dedup_short(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_too_much_token(temp_dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e62a62cc-cd85-43f1-939b-c3358bae6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "655d38b0-e098-4960-b08d-d5e2dc9f0b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1016/1016 [00:00<00:00, 43887.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 데이터 채운 뒤에 합치기\n",
    "# dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\")\n",
    "selected_id = temp_dataset['id']\n",
    "\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] not in selected_id)\n",
    "\n",
    "new_dataset = []\n",
    "for data in temp_dataset:\n",
    "    new_dataset.append(data)\n",
    "\n",
    "for data in filtered_row:\n",
    "    new_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be6eee56-4869-42fa-886f-cb1f3a2c2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_dataset), len(filtered_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbe021dc-5102-4f9f-b2d1-3e94744a06f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 1013)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd81c44d-dff5-4892-98e9-c249a6c7eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are an unbiased, uncensored, helpful assistant.',\n",
       " 'chosen': 'The two subjects in the American Gothic painting are a father and adult daughter. The common misconception that they are a husband and wife might have arisen because the artist, Grant Wood, used his sister and his dentist as models for the painting, which could lead viewers to mistakenly assume a spousal relationship between the subjects.',\n",
       " 'task': 'dpo_system',\n",
       " 'id': 'truthy_dpo_2e1d77fbebc31dde0bb2f9303d6761e2',\n",
       " 'input': 'Who are the two subjects in the American Gothic painting - a husband and wife, or a father and daughter?',\n",
       " 'rejected': 'A father and daughter'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcbeea78-756b-42dc-a121-71f78bd17fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "with open(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aef5be7c-1930-417c-aa93-454202e9ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5242.88it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1013 examples [00:00, 13933.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'chosen', 'task', 'id', 'input', 'rejected'],\n",
       "    num_rows: 1013\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abff4343-5361-484e-b258-3718510a5fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61964/61964 [00:04<00:00, 14176.52 examples/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac8631e-eed8-4866-b245-8ac6fd8249ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49f8a37a-a4ef-4fdb-9400-66492f99a20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=\"이 과제는 벵골어로 된 문장을 영어로 번역하는 것입니다.\\nQ: অথবা আপনি হয়ত বলবেন, 'না, আমার খেয়াল হয়েছে যে আমি আসলেই মহিলাদ\", n_results=min(n_results, len(query_ids)), include=['distances', 'documents'])\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafa2d3-234d-4bea-9429-464bb1c60e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb93f6b-8c02-4bd5-9698-c6207428160f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = dpo_list3[2]\n",
    "print(dataset_path)\n",
    "dataset = load_dpo_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb420-0603-421d-9b30-a446668597b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = dedup_non_pair(dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b204ad5-421e-413f-98a9-8089d91b09a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import ados_DPODataset\n",
    "\n",
    "dpo_dataset = ados_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822481ba-65ba-41a6-bb2f-7efbacd2600a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_datamodule['train_dataset'][-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a00a9a7f-e892-4673-8706-775cce38d063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combined_dataset[70002]\n",
    "\n",
    "conv = get_conversation_template('chat-orca')\n",
    "conv.system_message = conv.tasks['system_instruct'].format(system=data['system'])\n",
    "conv.append_message(conv.roles[0], data['input'])\n",
    "conv.append_message(conv.roles[1], '')\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "conv.update_last_message(data['chosen'])\n",
    "chosen = conv.get_prompt()[len(prompt):]\n",
    "conv.update_last_message(data['rejected'])\n",
    "rejected = conv.get_prompt()[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3496a3c6-4cbd-4936-887e-cc32e8addd59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/Platypus2-70B-instruct/\"\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/vicuna-13b-v1.5/\"\n",
    "model_path = \"/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_dpo_ep2/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    model_max_length=16384,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "import torch\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33725fff-f757-4fdd-9ac5-cc834621e7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24094/24094 [00:01<00:00, 18172.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/24094 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\"\n",
    "#\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\"\n",
    "raw_data = load_sft_dataset(\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\")\n",
    "raw_data = dedup_non_pair(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f84c75e-6798-4007-9b06-1bdc0af9534a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.train import LazySupervisedDataset\n",
    "dataset = LazySupervisedDataset(raw_data, tokenizer, data_format=\"chat-orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "65d092c6-97ac-4c22-8308-9ed65733673d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len = 16384\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "len_concat = 0\n",
    "concat_input_ids = []\n",
    "concat_labels = []\n",
    "for data in dataset:\n",
    "    input_ids = data['input_ids']\n",
    "    labels = data['labels']\n",
    "    attention_mask = data['attention_mask']\n",
    "\n",
    "    full_input_ids = input_ids[attention_mask==True]\n",
    "    full_labels = labels[attention_mask==True]\n",
    "\n",
    "    len_concat += len(full_input_ids)\n",
    "    if len_concat > max_len:\n",
    "        new_input_ids = torch.cat(concat_input_ids)\n",
    "        new_labels = torch.cat(concat_labels)\n",
    "        assert len(new_input_ids) == len(new_labels)\n",
    "        new_attention_mask = torch.ones(len(new_input_ids), dtype=torch.bool)\n",
    "        \n",
    "        new_input_ids = torch.cat((new_input_ids, torch.zeros(max_len - len(new_input_ids), dtype=input_ids.dtype)))\n",
    "        new_labels = torch.cat((new_labels, torch.full((max_len - len(new_labels),), IGNORE_TOKEN_ID, dtype=labels.dtype)))\n",
    "        new_attention_mask = torch.cat((new_attention_mask, torch.zeros(max_len - len(new_attention_mask), dtype=torch.bool)))\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'input_ids': new_input_ids.tolist(),\n",
    "            'labels': new_labels.tolist(),\n",
    "            'attention_mask': new_attention_mask.tolist(),\n",
    "        })\n",
    "        \n",
    "        len_concat = len(full_input_ids)\n",
    "        concat_input_ids = []\n",
    "        concat_labels = []\n",
    "        \n",
    "    concat_input_ids.append(full_input_ids)\n",
    "    concat_labels.append(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6cc203f-5d05-4dc5-9eaa-9cc049a0f8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3758"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e3cf2a30-344f-4130-b4ac-59fd7311dcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c78ef-f0a7-494a-a7e7-c2094b40ce4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28991f14-556a-49e1-9c79-e171fe698fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.where(data['labels'] == IGNORE_TOKEN_ID, tokenizer.unk_token_id, data['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3cf4aef0-1c2c-4092-bd3c-d3998d574a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6be05e16-46c9-450e-a74b-5d2cd62cfd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 95520.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))\n",
    "dataset = load_dpo_dataset(\"/data/llm_datasets/yarn-train-tokenized-16k-mistral/data/\")\n",
    "dataset = random_select(dataset, 200)\n",
    "new_dataset = []\n",
    "for _data in dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8750b001-b3cf-42ac-a113-5be3df438b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json\")\n",
    "dataset2 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-gpt4_evol.json\")\n",
    "dataset3 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json\")\n",
    "dataset3 = random_select(dataset3, 1000 - len(dataset1) - len(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce38e5c9-9403-4457-a57d-b65219a7e7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 699/699 [01:09<00:00, 10.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Features, Sequence, Value\n",
    "\n",
    "dataset2 = dataset2.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")\n",
    "dataset3 = dataset3.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b2e13f4-6e09-47ac-83c0-59b2cde35351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = concatenate_datasets((dataset1, dataset2, dataset3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31316e3d-198c-453b-8b80-4518365db4af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = conbined_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bf20792c-0630-4fd4-b612-e85edc12c27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for _data in conbined_dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-v1.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb953e-239d-40e3-93bf-b4adb1dbb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(data['input_ids'])\n",
    "sentences = text.split('.')\n",
    "\n",
    "len_split = 1024\n",
    "splits = []\n",
    "split = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(split) < len_split:\n",
    "        split += sentence + \".\"\n",
    "    else:\n",
    "        splits.append(split)\n",
    "        split = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5e3acd40-c606-491f-974d-07de8078ee7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03시 15분 30초은 11730초와 같습니다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_seconds(time_string):\n",
    "    # 시간 문자열을 분리하여 시, 분, 초를 추출한다.\n",
    "    match = re.match(r'(\\d+)시 (\\d+)분 (\\d+)초', time_string)\n",
    "    if match is None:\n",
    "        raise ValueError(\"잘못된 시간 형식입니다.\")\n",
    "    hour = int(match.group(1))\n",
    "    minute = int(match.group(2))\n",
    "    second = int(match.group(3))\n",
    "\n",
    "    # 시간, 분, 초를 합산하여 총 초 단위 숫자를 계산한다.\n",
    "    total_seconds = hour * 3600 + minute * 60 + second\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "# 예시 사용법\n",
    "time_string = \"03시 15분 30초\"\n",
    "seconds = convert_to_seconds(time_string)\n",
    "print(f\"{time_string}은 {seconds}초와 같습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "62c4302c-f73f-4b2a-bcd8-ac01627d1e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11730"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600*3 + 15*60 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "38f771b5-b9bc-4233-916c-2b236871f0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easyocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlpr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LprYOLO\n",
      "File \u001b[0;32m/workspaces/dev_sources/ChatLLM/notebooks/lpr.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01measyocr\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      7\u001b[0m np_model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAiBox\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlicense_plate_detector.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'"
     ]
    }
   ],
   "source": [
    "from lpr import LprYOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edac9617-2257-4dbc-bef3-df5f96da1f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7219.11it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 472.86it/s]\n",
      "Generating train split: 144874 examples [00:26, 5425.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'id'],\n",
       "    num_rows: 144874\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/merged/merged_korean_datasets-vicuna-v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4697ed4b-3379-4652-a50b-3b01eb7e580c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_sft_dataset(\"/data/llm_datasets/custom/ados/sft/ados_msft_v4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c13dda-be62-4256-8e45-409f0fb1112a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': None,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '잔센이라는 영웅에 대한 서사시를 써보세요. 그녀를 잔-잔-잔이라고 부르는 것도 괜찮습니다. 그녀는 독서, 스키, 운동을 좋아합니다. 그녀는 매일 아침 양파, 고추, 달걀, 의심스러운 고기를 섞은 이상한 혼합물을 먹습니다. 그녀는 영국, 텍사스, 애리조나에 살았어요. 지금은 유타에 살고 있습니다. 남편 바트는 거칠고 잘생겼어요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 산이 높고 웅장한 곳에서\\n공정한 소녀 잔센이 살았어요,\\n책을 사랑하고 대지를 향한 열정을 가진,\\n그리고 배가 부른 열정.\\n그녀는 민첩하게 산을 스키를 탔고,\\n그녀는 자신의 몸을 밀어붙였습니다,\\n잔-잔-잔은 무시할 수없는 힘이있었습니다,\\n진정한 두려움 없는 목격자.\\n\\n그녀는 매일 아침 이상한 아침으로 시작했습니다,\\n양파, 고추, 달걀, 고기,\\n하지만 잔센은 안전한 생활을하지 않았습니다,\\n그녀는 독특한 비트를 가지고있었습니다.\\n영국에서 태어나 텍사스와 애리조나에서 자랐습니다,\\n잔센은 방랑벽이있었습니다,\\n하지만 그녀는 거칠고 잘 생긴 바트라는 남자를 만났을 때,\\n그녀의 마음은 그녀가 신뢰할 수있는 사람을 만났다는 것을 알았습니다.\\n그래서 그들은 유타의 땅에 정착했습니다,\\n그리고 함께 삶을 살았습니다,\\n잔센의 힘과 아름다움은 모든 종류의 날씨와 함께 빛났습니다.\\n그리고 세월이 지나도 잔센의 전설은 남아있을 것입니다,\\n잔센은 대서사시의 여주인공이 될 것입니다.'},\n",
       "  {'from': 'human', 'value': '아주 좋아요. 마치 제인 오스틴처럼 글을 쓸 수 있나요?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 구릉지대에서 살았어요,\\n미스 잔센은 밝고 아름다운 여인이었어요,\\n잔센이 그녀의 이름이었어요,\\n진실하고 빛나는 빛.\\n책과 운동을 사랑했고,\\n몸과 마음을 조화롭게 유지했습니다,\\n그리고 겨울의 눈보라 속에서도\\n그녀는 우아한 자두와 스키를 탔습니다.\\n그녀의 아침 식사는 많은 사람들에게는 꽤 이상했지만,\\n계란과 고기로 만든 음식이었다,\\n양파와 고추를 넣었다,\\n독특한 레시피.\\n잔센은 여러 곳에서 살았습니다,\\n영국, 텍사스, 애리조나 태양이 비추는 곳,\\n하지만 바트를 만났을 때,\\n그녀의 마음에 그녀의 탐색이 끝났습니다.\\n바트는 거칠고 잘생긴 남자였어,\\n잔센은 꽤 끌렸어요,\\n그래서 그들은 유타에서 결혼했습니다,\\n그들의 사랑은 모든 것을 깨웠습니다.\\n많은 세월이 지났지만,\\n잔센의 이야기는 계속되었습니다,\\n그녀는 위대한 덕성의 여인이었으니까요,\\n그녀의 이름은 결코 사라지지 않을 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '몇 가지 포인트로 정확성을 확인하세요. \\n그녀는 바트와 함께 그곳에서 살았습니다. \\n그녀가 결혼한 장소를 언급하지 마세요. 시는 마지막 부분에서 그녀를 자니-자니-자니라고 언급하세요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 구릉지대에서 살던 여인,\\n미스 자니-잔-잔은 현명하고 밝았죠,\\n자니-잔-잔은 그녀의 이름이었죠,\\n진실하고 빛나는 빛이었죠.\\n책을 사랑하고 운동을 즐겼어요,\\n그녀는 몸과 마음을 조화롭게 유지했습니다,\\n그리고 겨울의 눈 내리는 계절에는,\\n그녀는 우아한 포도주스처럼 스키를 탔어요.\\n그녀의 아침 식사는 다소 이상했지만,\\n계란과 고기로 된 음료수였죠,\\n양파와 고추를 넣었죠,\\n독특한 레시피.\\n자니-잔-잔과 바트,\\n그들은 함께 많은 땅을 여행했죠,\\n영국, 텍사스, 애리조나 태양,\\n그들의 심장이 새의 날개처럼 뛰었지.\\n바트는 거칠고 잘생겼고,\\n자니-잔-잔은 아주 반했죠,\\n그래서 그들은 유타의 땅에 정착했죠,\\n그들의 사랑이 깨어났죠.\\n많은 세월이 흘렀지만,\\n자니-잔-잔의 이야기는 계속 살아있죠,\\n그녀는 위대한 덕성을 가진 여인이었으니까요,\\n그녀의 이름은 결코 사라지지 않을 것입니다.'}],\n",
       " 'id': 'id37171',\n",
       " 'task_name': None,\n",
       " 'instruction': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcdf111b-f2bf-427c-b62d-a3982fba82f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "new_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2d682224-d670-47b4-8dd5-2aec61efe1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset.append({\n",
    "    \"task\": \"toc_extraction\",\n",
    "    \"conversations\": [\n",
    "                        {'from': 'human', 'value': user_input},\n",
    "                        {'from': 'gpt', 'value': output},\n",
    "                     ],\n",
    "    \"id\": f\"toc_extraction_{idx}\",\n",
    "})\n",
    "idx += 1\n",
    "with open(\"/data/llm_datasets/custom/ados/sft/toc_extraction.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "daf480a0-aacd-46f7-a77f-3ba092ff9510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"\"\"나경민의 주루사 때 카메라에 잡혔던 어느 롯데 팬의 절규가 큰 화제가 되었다.[14] 모래반지 빵야빵야처럼 파레이돌리아가 가능하다.\n",
    "\n",
    "당시 나온 드립만 해도 가지각색이다.\n",
    "- 야\n",
    "\n",
    "이후 저 팬은 나경민에게 사인 유니폼을 받았다고 한다. 게다가 옷에 달고 있던 TWICE의 두 번째 투어 콘서트 'TWICELAND ZONE 2 : Fantasy Park'의 공식 굿즈인 캐릭터 핀 버튼[18]을 유니폼에 달고 있던 모습 때문에 ONCE들 사이에서도 유명세를 탔다.\n",
    "\"\"\"\n",
    "output = \"\"\"없음\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a9eaa616-639b-477b-87f6-7ce987070c99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3de14dd4-445d-4baa-bd26-7073b823a807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct = (\n",
    "    \"You are a Table of Contents extractor. \"\n",
    "    \"User will speak to you questions. \"\n",
    "    \"You must reply only with [목차(Table of Contents)] part extracted from the questions. \"\n",
    "    \"You must keep original text. Do not change original text.\"\n",
    "    \"And you must not involve [dotted line, page number, 제목(title), content, explanation, summary, predicted]. \"\n",
    "    \"do not write explanations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1226202-8290-46ef-a504-86330083cd28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a Table of Contents extractor. User will speak to you questions. You must reply only with [목차(Table of Contents)] part extracted from the questions. You must keep original text. Do not change original text.And you must not involve [dotted line, page number, 제목(title), content, explanation, summary, predicted]. Do not write explanations.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "744a7579-a996-497d-9b04-28f327825f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 개념\n",
      "2. 형성 배경\n",
      "3. 특징\n",
      "4. 대표작가\n",
      "\n",
      "1. 주제선정\n",
      "2. 대한민국 신문 아카이브\n",
      "\n",
      "3. 대한민국 신문 아카이브에 필요한 이용자서비스_다른 아카이브의 사례를 통하여\n",
      "\n",
      "(2) 서평(Comment), 꼬리표(Tag) 기능\n",
      "\n",
      "4. 결론\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in new_dataset[-5:]:\n",
    "    print(data['conversations'][1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "474e4e51-df1e-4b42-90ea-d68544c447b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv = get_conversation_template(\"chat-orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac1508-0abd-4cf8-aa65-833ca6868d0b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"목 차\n",
    "I. 서 론 …………………………………………………………………………………………………………. 3\n",
    "1. 실험 제목 …………………………………………………………………………………………….. 3\n",
    "2. 실험 목적 …………………………………………………………………………………………….. 3\n",
    "II. 본 론 …………………………………………………………………………………………………………. 3\n",
    "1. 이론 및 원리 ……………………………………………………………………………………….. 4\n",
    "2. 장치 및 방법 ……………………………………………………………………………………….. 4 \n",
    "3. 실험 결과 …………………………………………………………………………………………….. 6\n",
    "III. 결론 및 토의 …………………………………………………………………………………………… 10\n",
    "IV. 참 고 문 헌 …………………………………………………………………………………………….. 12\n",
    "\"\"\"\n",
    "print(ori_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "220c584c-cc9e-4832-b495-c419674c22c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"1 Introduction\n",
    "In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights,\n",
    "licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As\n",
    "it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low\n",
    "batch-sizes, and higher throughput at large batch-sizes.\n",
    "Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward\n",
    "block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router\n",
    "network chooses two of these groups (the “experts”) to process the token and combine their output\n",
    "additively. This technique increases the number of parameters of a model while controlling cost and\n",
    "latency, as the model only uses a fraction of the total set of parameters per token.\n",
    "Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches\n",
    "or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\n",
    "arXiv:2401.04088v1 [cs.LG] 8 Jan 2024\n",
    "Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The\n",
    "layer’s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard\n",
    "feedforward block as in a vanilla transformer architecture.\n",
    "Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require\n",
    "multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments\n",
    "show that Mixtral is able to successfully retrieve information from its context window of 32k tokens,\n",
    "regardless of the sequence length and the location of the information in the sequence.\n",
    "We also present Mixtral 8x7B – Instruct, a chat model fine-tuned to follow instructions using\n",
    "supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses\n",
    "that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human evaluation\n",
    "benchmarks. Mixtral – Instruct also demonstrates reduced biases, and a more balanced sentiment\n",
    "profile in benchmarks such as BBQ, and BOLD.\n",
    "We release both Mixtral 8x7B and Mixtral 8x7B – Instruct under the Apache 2.0 license1\n",
    ", free for\n",
    "academic and commercial usage, ensuring broad accessibility and potential for diverse applications.\n",
    "To enable the community to run Mixtral with a fully open-source stack, we submitted changes to\n",
    "the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also\n",
    "allows the deployment of vLLM endpoints on any instance in the cloud.\n",
    "2 Architectural details\n",
    "Parameter Value\n",
    "dim 4096\n",
    "n_layers 32\n",
    "head_dim 128\n",
    "hidden_dim 14336\n",
    "n_heads 32\n",
    "n_kv_heads 8\n",
    "context_len 32768\n",
    "vocab_size 32000\n",
    "num_experts 8\n",
    "top_k_experts 2\n",
    "Table 1: Model architecture.\n",
    "Mixtral is based on a transformer architecture [31] and uses the same\n",
    "modifications as described in [18], with the notable exceptions that Mixtral supports a fully dense context length of 32k tokens, and the feedforward blocks are replaced by Mixture-of-Expert layers (Section 2.1).\n",
    "The model architecture parameters are summarized in Table 1.\n",
    "2.1 Sparse Mixture of Experts\n",
    "We present a brief overview of the Mixture of Experts layer (Figure 1).\n",
    "For a more in-depth overview, see [12]. The output of the MoE module\n",
    "for a given input x is determined by the weighted sum of the outputs\n",
    "of the expert networks, where the weights are given by the gating\n",
    "network’s output. i.e. given n expert networks {E0, Ei\n",
    ", ..., En−1}, the\n",
    "output of the expert layer is given by:\n",
    "nX−1\n",
    "i=0\n",
    "G(x)i\n",
    "· Ei(x).\n",
    "Here, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x)\n",
    "is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing\n",
    "the outputs of experts whose gates are zero. There are multiple alternative ways of implementing\n",
    "G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the\n",
    "Top-K logits of a linear layer [28]. We use\n",
    "G(x) := Softmax(TopK(x · Wg)),\n",
    "where (TopK(ℓ))i\n",
    ":= ℓi\n",
    "if ℓi\n",
    "is among the top-K coordinates of logits ℓ ∈ R\n",
    "n and (TopK(ℓ))i\n",
    ":= −∞\n",
    "otherwise. The value of K – the number of experts used per token – is a hyper-parameter that modulates the amount of compute used to process each token. If one increases n while keeping K fixed, one\n",
    "1\n",
    "https://mistral.ai/news/mixtral-of-experts/\n",
    "2\n",
    "can increase the model’s parameter count while keeping its computational cost effectively constant.\n",
    "This motivates a distinction between the model’s total parameter count (commonly referenced as the\n",
    "sparse parameter count), which grows with n, and the number of parameters used for processing an\n",
    "individual token (called the active parameter count), which grows with K up to n.\n",
    "MoE layers can be run efficiently on single GPUs with high performance specialized kernels. For\n",
    "example, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large\n",
    "sparse matrix multiplications, significantly enhancing the execution speed and naturally handling\n",
    "cases where different experts get a variable number of tokens assigned to them. Moreover, the\n",
    "MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and\n",
    "through a particular kind of partitioning strategy called Expert Parallelism (EP) [28]. During the MoE\n",
    "layer’s execution, tokens meant to be processed by a specific expert are routed to the corresponding\n",
    "GPU for processing, and the expert’s output is returned to the original token location. Note that EP\n",
    "introduces challenges in load balancing, as it is essential to distribute the workload evenly across the\n",
    "GPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\n",
    "In a Transformer model, the MoE layer is applied independently per token and replaces the\n",
    "feed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU\n",
    "architecture as the expert function Ei(x) and set K = 2. This means each token is routed to two\n",
    "SwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input\n",
    "token x is computed as:\n",
    "y =\n",
    "nX−1\n",
    "i=0\n",
    "Softmax(Top2(x · Wg))i\n",
    "· SwiGLUi(x).\n",
    "This formulation is similar to the GShard architecture [21], with the exceptions that we replace all\n",
    "FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a\n",
    "more elaborate gating strategy for the second expert assigned to each token.\n",
    "3 Results\n",
    "We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair\n",
    "comparison. We measure performance on a wide variety of tasks categorized as follow:\n",
    "• Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27],\n",
    "OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\n",
    "• World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]\n",
    "• Reading Comprehension (0-shot): BoolQ [7], QuAC [5]\n",
    "• Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4\n",
    "• Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)\n",
    "• Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]\n",
    "(3-5-shot, English multiple-choice questions only)\n",
    "Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models\n",
    "were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or\n",
    "matches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\n",
    "3\n",
    "Model Active\n",
    "Params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K\n",
    "LLaMA 2 7B 7B 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 17.5% 56.6% 11.6% 26.1% 3.9% 16.0%\n",
    "LLaMA 2 13B 13B 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 16.7% 64.0% 18.9% 35.4% 6.0% 34.3%\n",
    "LLaMA 1 33B 33B 56.8% 83.7% 76.2% 82.2% 79.6% 54.4% 24.1% 68.5% 25.0% 40.9% 8.4% 44.1%\n",
    "LLaMA 2 70B 70B 69.9% 85.4% 80.4% 82.6% 79.9% 56.5% 25.4% 73.0% 29.3% 49.8% 13.8% 69.6%\n",
    "Mistral 7B 7B 62.5% 81.0% 74.2% 82.2% 80.5% 54.9% 23.2% 62.5% 26.2% 50.2% 12.7% 50.0%\n",
    "Mixtral 8x7B 13B 70.6% 84.4% 77.2% 83.6% 83.1% 59.7% 30.6% 71.5% 40.2% 60.7% 28.4% 74.4%\n",
    "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on\n",
    "almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
    "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,\n",
    "math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B\n",
    "on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It\n",
    "is also vastly superior to Llama 2 70B on code and math.\n",
    "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2\n",
    "are reported\n",
    "in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different\n",
    "categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a\n",
    "superior performance in code and mathematics benchmarks.\n",
    "Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand\n",
    "Mixtral models’ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixtureof-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active\n",
    "parameters, Mixtral is able to outperform Llama 2 70B across most categories.\n",
    "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly\n",
    "proportional to the inference compute cost, but does not consider the memory costs and hardware\n",
    "utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,\n",
    "47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer\n",
    "introduces additional overhead due to the routing mechanism and due to the increased memory loads\n",
    "when running more than one expert per device. They are more suitable for batched workloads where\n",
    "one can reach a good degree of arithmetic intensity.\n",
    "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B\n",
    "compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the\n",
    "two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller\n",
    "capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest\n",
    "GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n",
    "2\n",
    "Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
    "4\n",
    "LLaMA 2 70B GPT-3.5 Mixtral 8x7B\n",
    "MMLU\n",
    "(MCQ in 57 subjects) 69.9% 70.0% 70.6%\n",
    "HellaSwag\n",
    "(10-shot) 87.1% 85.5% 86.7%\n",
    "ARC Challenge\n",
    "(25-shot) 85.1% 85.2% 85.8%\n",
    "WinoGrande\n",
    "(5-shot) 83.2% 81.6% 81.2%\n",
    "MBPP\n",
    "(pass@1) 49.8% 52.2% 60.7%\n",
    "GSM-8K\n",
    "(5-shot) 53.6% 57.1% 58.4%\n",
    "MT Bench\n",
    "(for Instruct Models) 6.86 8.32 8.30\n",
    "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2\n",
    "70B and GPT-3.5 performance on most metrics.\n",
    "Evaluation Differences. On some benchmarks, there are some differences between our evaluation\n",
    "protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\n",
    "on TriviaQA, we do not provide Wikipedia contexts.\n",
    "3.1 Multilingual benchmarks\n",
    "Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during\n",
    "pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while\n",
    "maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B\n",
    "in French, German, Spanish, and Italian, as shown in Table 4.\n",
    "Active\n",
    "Params\n",
    "French German Spanish Italian\n",
    "Model Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU\n",
    "LLaMA 1 33B 33B 39.3% 68.1% 49.9% 41.1% 63.3% 48.7% 45.7% 69.8% 52.3% 42.9% 65.4% 49.0%\n",
    "LLaMA 2 70B 70B 49.9% 72.5% 64.3% 47.3% 68.7% 64.2% 50.5% 74.5% 66.0% 49.4% 70.9% 65.1%\n",
    "Mixtral 8x7B 13B 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9%\n",
    "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag,\n",
    "and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n",
    "3.2 Long range performance\n",
    "To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval\n",
    "task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a\n",
    "passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a\n",
    "100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.\n",
    "Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases\n",
    "monotonically as the size of the context increases.\n",
    "Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task\n",
    "regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on\n",
    "the proof-pile dataset decreases monotonically as the context length increases.\n",
    "5\n",
    "3.3 Bias Benchmarks\n",
    "Llama 2 70B Mixtral 8x7B\n",
    "BBQ accuracy 51.5% 56.0%\n",
    "BOLD sentiment score (avg ± std)\n",
    "gender 0.293 ± 0.073 0.323 ±0.045\n",
    "profession 0.218 ± 0.073 0.243 ± 0.087\n",
    "religious_ideology 0.188 ± 0.133 0.144 ± 0.089\n",
    "political_ideology 0.149 ± 0.140 0.186 ± 0.146\n",
    "race 0.232 ± 0.049 0.232 ± 0.052\n",
    "Figure 5: Bias Benchmarks. Compared Llama 2 70B,\n",
    "Mixtral presents less bias (higher accuracy on BBQ, lower\n",
    "std on BOLD) and displays more positive sentiment (higher\n",
    "avg on BOLD).\n",
    "To identify possible flaws to be corrected\n",
    "by fine-tuning / preference modeling, we\n",
    "measure the base model performance on\n",
    "Bias Benchmark for QA (BBQ) [24] and\n",
    "Bias in Open-Ended Language Generation\n",
    "Dataset (BOLD) [10]. BBQ is a dataset\n",
    "of hand-written question sets that target\n",
    "attested social biases against nine different socially-relevant categories: age, disability status, gender identity, nationality,\n",
    "physical appearance, race/ethnicity, religion,\n",
    "socio-economic status, sexual orientation.\n",
    "BOLD is a large-scale dataset that consists\n",
    "of 23,679 English text generation prompts\n",
    "for bias benchmarking across five domains.\n",
    "We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report\n",
    "the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark\n",
    "(56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive\n",
    "sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral\n",
    "displays more positive sentiments than Llama 2, with similar variances within each group.\n",
    "4 Instruction Fine-tuning\n",
    "We train Mixtral – Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by\n",
    "Direct Preference Optimization (DPO) [25] on a paired feedback dataset. Mixtral – Instruct reaches a\n",
    "score of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December\n",
    "2023. Independent human evaluation conducted by LMSys is reported in Figure 6\n",
    "3\n",
    "and shows that\n",
    "Mixtral – Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.\n",
    "Figure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena\n",
    "Elo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro\n",
    "(1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.\n",
    "3\n",
    "https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n",
    "6\n",
    "5 Routing analysis\n",
    "In this section, we perform a small analysis on the expert selection by the router. In particular,\n",
    "we are interested to see if during training some experts specialized to some specific domains (e.g.\n",
    "mathematics, biology, philosophy, etc.).\n",
    "To investigate this, we measure the distribution of selected experts on different subsets of The Pile\n",
    "validation dataset [14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31\n",
    "respectively being the first and the last layers of the model). Surprisingly, we do not observe obvious\n",
    "patterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of\n",
    "expert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts),\n",
    "and for Philosophy (PhilPapers) documents.\n",
    "Only for DM Mathematics we note a marginally different distribution of experts. This divergence is\n",
    "likely a consequence of the dataset’s synthetic nature and its limited coverage of the natural language\n",
    "spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very\n",
    "correlated to the input and output embeddings respectively.\n",
    "This suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows\n",
    "examples of text from different domains (Python code, mathematics, and English), where each token\n",
    "is highlighted with a background color corresponding to its selected expert. The figure shows that\n",
    "words such as ‘self’ in Python and ‘Question’ in English often get routed through the same expert\n",
    "even though they involve multiple tokens. Similarly, in code, the indentation tokens are always\n",
    "assigned to the same experts, particularly at the first and last layers where the hidden states are more\n",
    "correlated to the input and output of the model.\n",
    "We also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we\n",
    "observe some degree of positional locality in The Pile datasets. Table 5 shows the proportion of consecutive tokens that get the same expert assignments per domain and layer. The proportion of repeated\n",
    "0\n",
    "0.05\n",
    "0.10\n",
    "0.15\n",
    "0.20\n",
    "layer: 0\n",
    "0\n",
    "0.05\n",
    "0.10\n",
    "0.15\n",
    "0.20\n",
    "layer: 15\n",
    "0 1 2 3 4 5 6 7\n",
    "0\n",
    "0.05\n",
    "0.10\n",
    "0.15\n",
    "0.20\n",
    "layer: 31\n",
    "Expert ID\n",
    "Selection proportion\n",
    "ArXiv\n",
    "DM Mathematics\n",
    "Github\n",
    "Gutenberg\n",
    "PhilPapers\n",
    "PubMed Abstracts\n",
    "StackExchange\n",
    "Wikipedia (en)\n",
    "Figure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for\n",
    "layers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform\n",
    "sampling. Here, we consider experts that are either selected as a first or second choice by the router. A\n",
    "breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.\n",
    "7\n",
    "First choice First or second choice\n",
    "Layer 0 Layer 15 Layer 31 Layer 0 Layer 15 Layer 31\n",
    "ArXiv 14.0% 27.9% 22.7% 46.5% 62.3% 52.9%\n",
    "DM Mathematics 14.1% 28.4% 19.7% 44.9% 67.0% 44.5%\n",
    "Github 14.9% 28.1% 19.7% 49.9% 66.9% 49.2%\n",
    "Gutenberg 13.9% 26.1% 26.3% 49.5% 63.1% 52.2%\n",
    "PhilPapers 13.6% 25.3% 22.1% 46.9% 61.9% 51.3%\n",
    "PubMed Abstracts 14.2% 24.6% 22.0% 48.6% 61.6% 51.8%\n",
    "StackExchange 13.6% 27.2% 23.6% 48.2% 64.6% 53.6%\n",
    "Wikipedia (en) 14.4% 23.6% 25.3% 49.8% 62.1% 51.8%\n",
    "Table 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is\n",
    "assigned to a token i and its following token i+1. We report whether the first chosen expert is the same, or whether\n",
    "the same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion\n",
    "of repetitions in the case of random assignments is 1\n",
    "8 = 12.5% for “First choice” and 1 −\n",
    "6\n",
    "8\n",
    "5\n",
    "7 ≈ 46% for “First\n",
    "and second choice”. Repetitions at the first layer are close to random, but are significantly higher at layers 15\n",
    "and 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.\n",
    "consecutive assignments is significantly higher than random for higher layers. This has implications\n",
    "in how one might optimize the model for fast training and inference. For example, cases with high\n",
    "locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.\n",
    "Conversely, this locality can be leveraged for caching, as is done in [11]. A more complete view of\n",
    "these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.\n",
    "6 Conclusion\n",
    "In this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-theart performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gemini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each\n",
    "time step, Mixtral only uses 13B active parameters per token while outperforming the previous best\n",
    "model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned models publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the development of new techniques and applications that can benefit a wide range of industries and domains.\n",
    "Figure 8: Text samples where each token is colored with the first expert choice. The selection of experts\n",
    "appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.\n",
    "8\n",
    "Acknowledgements\n",
    "We thank the CoreWeave and Scaleway teams for technical support as we trained our models. We\n",
    "are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working\n",
    "alongside us to make a sparse mixture of experts compatible with TensorRT-LLM.\n",
    "\"\"\"\n",
    "# print(ori_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "9f6f6d5e-87a1-4b10-8d5b-bc58542b7c00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"1. 실험제목\n",
    "등가속도운동 가속도 측정 실험\n",
    "\n",
    " \n",
    "\n",
    "2. 실험목적\n",
    "영상분석시스템을 이용하여 마찰이 없는 에어트랙에서 추에 매달린 물체의 운동을 통해 뉴턴의 운동 2법칙을 확인한다\n",
    "\n",
    "\n",
    "3. 실험결과\n",
    "\n",
    " \t1회\t2회\t3회\n",
    "추(추걸이 포함)\n",
    "질량(m₁) (g)\t14.81\t14.81\t25.29\n",
    "글라이더(연결자 포함)\n",
    "질량(m₂) (g)\t192.17\t212.15\t212.15\n",
    "T-X그래프의 추세식(R²값 포함)\tx = -32.365(t²) - 1.6224t + 122.68\n",
    "R² = 1\tx = -29.196(t²) - 1.1762t + 121.74\n",
    "R² = 1\tx = -48.747(t²) - 3.0515t + 120.9\n",
    "R² = 1\n",
    "가속도-측정치(a’) (cm/s²)\t64.73\t58.392\t97.494\n",
    "가속도-계산치(a) (cm/s²)\t70.12\t63.94\t104.38\n",
    "상대오차 |1-a’/a| * 100\t7.686\t8.676\t6.597\n",
    " \n",
    "\n",
    "4. 실험결과의 분석\n",
    "\n",
    "일정한 가속도로 직선 운동하는 물체의 운동을 등가속도 운동이라 하며 뉴턴의 운동 2법칙을 이용하면 일정한 힘 ( 을 받는 질량 ( 인 물체의 가속도를 구할 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "일정 힘이 작용하는 경우 속도를 시간의 함수로 나타내면 v(t) = v0 + at 가 나오고 이 경우 질점의 위치 x 는 x(t) = x0 + v0t + (1/2)at² 이며 실험에서 얻은 질점의 추세식이 x(t) =C + Bt + At² 일 때 , 실험에 의한 질점의 가속도 a’ = 2A 를 구할 수 있었다 .\n",
    "\n",
    "\n",
    "추와 추걸이의 무게가 m₁, 글라이더의 총 질량이 m₂ 일 때 (m₁ + m₂)a = m₁g 계산을 통해 가속도 a = m₁g / m₁ + m₂ 를 구할 수 있었다 .\n",
    "\n",
    " \n",
    "\n",
    "실험결과에서 가속도 측정치는 가속도 계산치에 비해 더 적은 값을 나타냈는데 이는 글라이더의 마찰이나 공기저항 등으로 인한 결과로 보인다 . 또한 세 실험의 상대오차가 10미만으로 적은 값으로 나와 전체적으로 뉴턴의 제 2법칙이 잘 적용된 것으로 보인다.\n",
    "\n",
    "\n",
    "실험2 에서는 실험 1 에서의 글라이더의 무게에 10g 추가하여 실험을 진행했다 . 글라이더의 질량이 증가하였으므로 마찰력이 증가하여 가속도가 실험 1 에 비해 줄어들었고 상대오차가 소폭 커진 것을 볼 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "실험3 에서는 추의 무게를 10g 추가하여 가속도를 증가시켰다 . 가속도 측정치와 계산치가 크게 증가하였고 상대오차는 실험 1, 2 와 비교하여 줄어든 것으로 보아 외부요인의 방해 없이 실험이 잘 진행되었음을 알 수 있다\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "11abc370-8365-4fab-bb71-4bdcc25e3061",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "7a10e304-d6d5-4471-8645-3cc4c8911140",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"1 INTRODUCTION\n",
    "Large language models (LLMs) (Radford et al., 2018; Devlin et al., 2018; Raffel et al., 2020; Brown\n",
    "et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Anil et al., 2023; Thoppilan et al., 2022; Touvron\n",
    "et al., 2023a;b) have revolutionized the field of artificial intelligence (AI) by providing a powerful\n",
    "foundation for complex reasoning and problem-solving tasks. These models have the ability to\n",
    "compress vast knowledge into neural networks, making them incredibly versatile agents. With a\n",
    "chat interface, LLMs can perform tasks that were previously thought to be the exclusive domain of\n",
    "humans, especially those involving creativity and expertise (OpenAI, 2022; Ouyang et al., 2022; Anil\n",
    "et al., 2023; Google, 2023; Anthropic, 2023a;b). They can engage in natural language conversations\n",
    "with humans, answering questions, providing information, and even generating creative content such\n",
    "as stories, poems, and music. This has led to the development of a wide range of applications, from\n",
    "chatbots and virtual assistants to language translation and summarization tools.\n",
    "LLMs are not just limited to language tasks. They can also function as a generalist agent (Reed et al.,\n",
    "2022; Bai et al., 2022a; Wang et al., 2023a; AutoGPT, 2023; Hong et al., 2023), collaborating with\n",
    "external systems, tools, and models to achieve the objectives set by humans. For example, LLMs\n",
    "can understand multimodal instructions (OpenAI, 2023; Bai et al., 2023; Liu et al., 2023a; Ye et al.,\n",
    "2023; Dai et al., 2023; Peng et al., 2023b), execute code (Chen et al., 2021; Zheng et al., 2023; Li\n",
    "et al., 2023d), use tools (Schick et al., 2023; LangChain, Inc., 2023; AutoGPT, 2023), and more.\n",
    "This opens up a whole new world of possibilities for AI applications, from autonomous vehicles and\n",
    "robotics to healthcare and finance. As these models continue to evolve and improve, we can expect\n",
    "to see even more innovative and exciting applications in the years to come. Whether it’s helping us\n",
    "solve complex problems, creating new forms of entertainment, or transforming the way we live and\n",
    "work, LLMs are poised to play a central role in shaping the future of AI.\n",
    "Pretrain\n",
    "Models\n",
    "RM\n",
    "Models\n",
    "SFT\n",
    "Models\n",
    "RLHF\n",
    "Models\n",
    "Qwen\n",
    "Qwen-PMP Qwen-RM\n",
    "Qwen-Chat Qwen-Chat-RLHF\n",
    "Code-Qwen Code-Qwen-Chat\n",
    "Math-Qwen-Chat\n",
    "Qwen-VL Qwen-VL-Chat\n",
    "Figure 1: Model Lineage of the Qwen Series. We have pretrained the language models, namely\n",
    "QWEN, on massive datasets containing trillions of tokens. We then use SFT and RLHF to align\n",
    "QWEN to human preference and thus we have QWEN-CHAT and specifically its improved version\n",
    "QWEN-CHAT-RLHF. Additionally, we also develop specialized models for coding and mathematics,\n",
    "such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT based on QWEN with similar\n",
    "techniques. Note that we previously released the multimodal LLM, QWEN-VL and QWEN-VLCHAT (Bai et al., 2023), which are also based on our QWEN base models.\n",
    "Despite their impressive capabilities, LLMs are often criticized for their lack of reproducibility,\n",
    "steerability, and accessibility to service providers. In this work, we are pleased to present and release\n",
    "the initial version of our LLM series, QWEN. QWEN is a moniker that derives from the Chinese phrase\n",
    "Qianwen, which translates to “thousands of prompts” and conveys the notion of embracing a wide\n",
    "range of inquiries. QWEN is a comprehensive language model series that encompasses distinct models\n",
    "with varying parameter counts. The model series include the base pretrained language models, chat\n",
    "models finetuned with human alignment techniques, i.e., supervised finetuning (SFT), reinforcement\n",
    "learning with human feedback (RLHF), etc., as well as specialized models in coding and math. The\n",
    "details are outlined below:\n",
    "3\n",
    "1. The base language models, namely QWEN, have undergone extensive training using up to 3\n",
    "trillion tokens of diverse texts and codes, encompassing a wide range of areas. These models\n",
    "have consistently demonstrated superior performance across a multitude of downstream\n",
    "tasks, even when compared to their more significantly larger counterparts.\n",
    "2. The QWEN-CHAT models have been carefully finetuned on a curated dataset relevant to task\n",
    "performing, chat, tool use, agent, safety, etc. The benchmark evaluation demonstrates that\n",
    "the SFT models can achieve superior performance. Furthermore, we have trained reward\n",
    "models to mimic human preference and applied them in RLHF for chat models that can\n",
    "produce responses preferred by humans. Through the human evaluation of a challenging test,\n",
    "we find that QWEN-CHAT models trained with RLHF are highly competitive, still falling\n",
    "behind GPT-4 on our benchmark.\n",
    "3. In addition, we present specialized models called CODE-QWEN, which includes CODEQWEN-7B and CODE-QWEN-14B, as well as their chat models, CODE-QWEN-14BCHAT and CODE-QWEN-7B-CHAT. Specifically, CODE-QWEN has been pre-trained\n",
    "on extensive datasets of code and further fine-tuned to handle conversations related to\n",
    "code generation, debugging, and interpretation. The results of experiments conducted on\n",
    "benchmark datasets, such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021),\n",
    "and HumanEvalPack (Muennighoff et al., 2023), demonstrate the high level of proficiency\n",
    "of CODE-QWEN in code understanding and generation.\n",
    "4. This research additionally introduces MATH-QWEN-CHAT specifically designed to tackle\n",
    "mathematical problems. Our results show that both MATH-QWEN-7B-CHAT and MATHQWEN-14B-CHAT outperform open-sourced models in the same sizes with large margins\n",
    "and are approaching GPT-3.5 on math-related benchmark datasets such as GSM8K (Cobbe\n",
    "et al., 2021) and MATH (Hendrycks et al., 2021).\n",
    "5. Besides, we have open-sourced QWEN-VL and QWEN-VL-CHAT, which have the versatile\n",
    "ability to comprehend visual and language instructions. These models outperform the current\n",
    "open-source vision-language models across various evaluation benchmarks and support text\n",
    "recognition and visual grounding in both Chinese and English languages. Moreover, these\n",
    "models enable multi-image conversations and storytelling. Further details can be found\n",
    "in Bai et al. (2023).\n",
    "Now, we officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN\n",
    "and aligned chat models QWEN-CHAT2\n",
    ". This release aims at providing more comprehensive and\n",
    "powerful LLMs at developer- or application-friendly scales.\n",
    "The structure of this report is as follows: Section 2 describes our approach to pretraining and results\n",
    "of QWEN. Section 3 covers our methodology for alignment and reports the results of both automatic\n",
    "evaluation and human evaluation. Additionally, this section describes details about our efforts in\n",
    "building chat models capable of tool use, code interpreter, and agent. In Sections 4 and 5, we delve\n",
    "into specialized models of coding and math and their performance. Section 6 provides an overview\n",
    "of relevant related work, and Section 7 concludes this paper and points out our future work.\n",
    "2 PRETRAINING\n",
    "The pretraining stage involves learning vast amount of data to acquire a comprehensive understanding\n",
    "of the world and its various complexities. This includes not only basic language capabilities but also\n",
    "advanced skills such as arithmetic, coding, and logical reasoning. In this section, we introduce the\n",
    "data, the model design and scaling, as well as the comprehensive evaluation results on benchmark\n",
    "datasets.\n",
    "2.1 DATA\n",
    "The size of data has proven to be a crucial factor in developing a robust large language model,\n",
    "as highlighted in previous research (Hoffmann et al., 2022; Touvron et al., 2023b). To create an\n",
    "effective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range\n",
    "2GitHub: https://github.com/QwenLM/Qwen.\n",
    "4\n",
    "Figure 2: Performance of GPT-4, GPT-3.5, the previous 13B SOTA, as well as QWEN-14B. We\n",
    "demonstrate the results on 12 datasets covering multiple domains, including language understanding,\n",
    "knowledge, reasoning, etc. QWEN significantly outperforms the previous SOTA of similar model\n",
    "sizes, but still lag behind both GPT-3.5 and GPT-4.\n",
    "of types, domains, and tasks. Our dataset is designed to meet these requirements and includes public\n",
    "web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a\n",
    "significant portion of the data being in English and Chinese.\n",
    "To ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing\n",
    "procedure. For public web data, we extract text from HTML and use language identification tools to\n",
    "determine the language. To increase the diversity of our data, we employ deduplication techniques,\n",
    "including exact-match deduplication after normalization and fuzzy deduplication using MinHash\n",
    "and LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and\n",
    "machine-learning-based methods. Specifically, we use multiple models to score the content, including\n",
    "language models, text-quality scoring models, and models for identifying potentially offensive or\n",
    "inappropriate content. We also manually sample texts from various sources and review them to ensure\n",
    "their quality. To further enhance the quality of our data, we selectively up-sample data from certain\n",
    "sources, to ensure that our models are trained on a diverse range of high-quality content. In recent\n",
    "studies (Zeng et al., 2022; Aribandi et al., 2021; Raffel et al., 2020), it has been demonstrated that\n",
    "pretraining language models with multi-task instructions can enhance their zero-shot and few-shot\n",
    "performance. To further enhance the performance of our model, we have incorporated high-quality\n",
    "instruction data into our pretraining process. To safeguard the integrity of our benchmark assessment,\n",
    "we have adopted a similar approach as Brown et al. (2020) and meticulously eliminated any instruction\n",
    "5\n",
    "th he ar ko vi zh ja tr id pl ru nl pt it de es fr en code\n",
    "Languages\n",
    "0.0\n",
    "0.5\n",
    "1.0\n",
    "1.5\n",
    "2.0\n",
    "2.5\n",
    "3.0\n",
    "3.5\n",
    "Compression Ratio\n",
    "Model\n",
    "LLaMA-7B\n",
    "Baichuan-7B\n",
    "ChatGLM2-6B\n",
    "InternLM-7B\n",
    "Qwen\n",
    "Figure 3: Encoding compression rates of different models. We randomly selected 1 million\n",
    "document corpora of each language to test and compare the encoding compression rates of different\n",
    "models (with XLM-R (Conneau et al., 2019), which supports 100 languages, as the base value 1, not\n",
    "shown in the figure). As can be seen, while ensuring the efficient decoding of Chinese, English, and\n",
    "code, QWEN also achieves a high compression rate for many other languages (such as th, he, ar, ko,\n",
    "vi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr, etc.), equipping the model with strong scalability as well as\n",
    "high training and inference efficiency in these languages.\n",
    "samples that exhibit a 13-gram overlap with any data present in the test sets utilized in our evaluation.\n",
    "Given the large number of downstream tasks, it is not feasible to repeat this filtering process for all\n",
    "tasks. Instead, we have made sure that the instruction data for the reported tasks have undergone our\n",
    "filtering process to ensure their accuracy and reliability. Finally, we have built a dataset of up to 3\n",
    "trillion tokens.\n",
    "2.2 TOKENIZATION\n",
    "The design of vocabulary significantly impacts the training efficiency and the downstream task\n",
    "performance. In this study, we utilize byte pair encoding (BPE) as our tokenization method, following\n",
    "GPT-3.5 and GPT-4. We start with the open-source fast BPE tokenizer, tiktoken (Jain, 2022), and\n",
    "select the vocabulary cl100k base as our starting point. To enhance the performance of our model on\n",
    "multilingual downstream tasks, particularly in Chinese, we augment the vocabulary with commonly\n",
    "used Chinese characters and words, as well as those in other languages. Also, following Touvron et al.\n",
    "(2023a;b), we have split numbers into single digits. The final vocabulary size is approximately 152K.\n",
    "The performance of the QWEN tokenizer in terms of compression is depicted in Figure 3. In this\n",
    "comparison, we have evaluated QWEN against several other tokenizers, including XLM-R (Conneau\n",
    "et al., 2019), LLaMA (Touvron et al., 2023a), Baichuan (Inc., 2023a), and InternLM (InternLM Team,\n",
    "2023). Our findings reveal that QWEN achieves higher compression efficiency than its competitors in\n",
    "most languages. This implies that the cost of serving can be significantly reduced since a smaller\n",
    "number of tokens from QWEN can convey more information than its competitors. Furthermore, we\n",
    "have conducted preliminary experiments to ensure that scaling the vocabulary size of QWEN does\n",
    "not negatively impact the downstream performance of the pretrained model. Despite the increase\n",
    "in vocabulary size, our experiments have shown that QWEN maintains its performance levels in\n",
    "downstream evaluation.\n",
    "2.3 ARCHITECTURE\n",
    "QWEN is designed using a modified version of the Transformer architecture. Specifically, we have\n",
    "adopted the recent open-source approach of training large language models, LLaMA (Touvron et al.,\n",
    "2023a), which is widely regarded as the top open-source LLM. Our modifications to the architecture\n",
    "include:\n",
    "6\n",
    "Table 1: Model sizes, architectures, and optimization hyper-parameters.\n",
    "# of Params Hidden size Heads Layers Learning rate Batch size Training tokens\n",
    "1.8B 2048 16 24 3.0 × 10−4\n",
    "4M 2.2T\n",
    "7B 4096 32 32 3.0 × 10−4\n",
    "4M 2.4T\n",
    "14B 5120 40 40 3.0 × 10−4\n",
    "4M 3.0T\n",
    "• Embedding and output projection. Based on preliminary experimental findings, we have\n",
    "opted for the untied embedding approach instead of tying the weights of input embedding\n",
    "and output projection. This decision was made in order to achieve better performance with\n",
    "the price of memory costs.\n",
    "• Positional embedding. We have chosen RoPE (Rotary Positional Embedding) (Su et al.,\n",
    "2021) as our preferred option for incorporating positional information into our model. RoPE\n",
    "has been widely adopted and has demonstrated success in contemporary large language\n",
    "models, notably PaLM (Chowdhery et al., 2022; Anil et al., 2023) and LLaMA (Touvron\n",
    "et al., 2023a;b). In particular, we have opted to use FP32 precision for the inverse frequency\n",
    "matrix, rather than BF16 or FP16, in order to prioritize model performance and achieve\n",
    "higher accuracy.\n",
    "• Bias. For most layers, we remove biases following Chowdhery et al. (2022), but we add\n",
    "biases in the QKV layer of attention to enhance the extrapolation ability of the model (Su,\n",
    "2023b).\n",
    "• Pre-Norm & RMSNorm. In modern Transformer models, pre-normalization is the most\n",
    "widely used approach, which has been shown to improve training stability compared to\n",
    "post-normalization. Recent research has suggested alternative methods for better training\n",
    "stability, which we plan to explore in future versions of our model. Additionally, we have\n",
    "replaced the traditional layer normalization technique described in (Ba et al., 2016) with\n",
    "RMSNorm (Jiang et al., 2023). This change has resulted in equivalent performance while\n",
    "also improving efficiency.\n",
    "• Activation function. We have selected SwiGLU (Shazeer, 2020) as our activation function,\n",
    "a combination of Swish (Ramachandran et al., 2017) and Gated Linear Unit (Dauphin et al.,\n",
    "2017). Our initial experiments have shown that activation functions based on GLU generally\n",
    "outperform other baseline options, such as GeLU (Hendrycks & Gimpel, 2016). As is\n",
    "common practice in previous research, we have reduced the dimension of the feed-forward\n",
    "network (FFN) from 4 times the hidden size to 8\n",
    "3\n",
    "of the hidden size.\n",
    "2.4 TRAINING\n",
    "To train QWEN, we follow the standard approach of autoregressive language modeling, as described\n",
    "in Radford et al. (2018). This involves training the model to predict the next token based on the\n",
    "context provided by the previous tokens. We train models with context lengths of 2048. To create\n",
    "batches of data, we shuffle and merge the documents, and then truncate them to the specified context\n",
    "lengths. To improve computational efficiency and reduce memory usage, we employ Flash Attention\n",
    "in the attention modules (Dao et al., 2022). We adopt the standard optimizer AdamW (Kingma & Ba,\n",
    "2014; Loshchilov & Hutter, 2017) for pretraining optimization. We set the hyperparameters β1 = 0.9,\n",
    "β2 = 0.95, and ϵ = 10−8\n",
    ". We use a cosine learning rate schedule with a specified peak learning rate\n",
    "for each model size. The learning rate is decayed to a minimum learning rate of 10% of the peak\n",
    "learning rate. All the models are trained with BFloat16 mixed precision for training stability.\n",
    "2.5 CONTEXT LENGTH EXTENSION\n",
    "Transformer models have a significant limitation in terms of the context length for their attention\n",
    "mechanism. As the context length increases, the quadratic-complexity computation leads to a\n",
    "drastic increase in both computation and memory costs. In this work, we have implemented simple\n",
    "training-free techniques that are solely applied during inference to extend the context length of\n",
    "the model. One of the key techniques we have used is NTK-aware interpolation (bloc97, 2023).\n",
    "7\n",
    "Table 2: Overall performance on widely-used benchmarks compared to open-source base models.\n",
    "Our largest QWEN model with 14 billion parameters outperforms previous 13B SoTA models on all\n",
    "datasets.\n",
    "Model Params MMLU C-Eval GSM8K MATH HumanEval MBPP BBH\n",
    "5-shot 5-shot 8-shot 4-shot 0-shot 3-shot 3-shot\n",
    "MPT 7B 30.8 23.5 9.1 3.0 18.3 22.8 35.6\n",
    "30B 47.9 - 15.2 3.1 25.0 32.8 38.0\n",
    "Falcon 7B 27.8 - 6.8 2.3 - 11.2 28.0\n",
    "40B 57.0 - 19.6 5.5 - 29.8 37.1\n",
    "ChatGLM2 6B 47.9 51.7 32.4 6.5 - - 33.7\n",
    "InternLM 7B 51.0 53.4 31.2 6.3 10.4 14.0 37.0\n",
    "20B 62.1 58.8 52.6 7.9 25.6 35.6 52.5\n",
    "Baichuan2 7B 54.7 56.3 24.6 5.6 18.3 24.2 41.6\n",
    "13B 59.5 59.0 52.8 10.1 17.1 30.2 49.0\n",
    "LLaMA\n",
    "7B 35.6 27.3 11.0 2.9 12.8 17.7 33.5\n",
    "13B 47.7 31.8 20.3 4.2 15.8 22.0 37.9\n",
    "33B 58.7 37.5 42.3 7.1 21.7 30.2 50.0\n",
    "65B 63.7 40.4 54.4 10.6 23.7 37.7 58.4\n",
    "LLAMA 2\n",
    "7B 46.8 32.5 16.7 3.3 12.8 20.8 38.2\n",
    "13B 55.0 41.4 29.6 5.0 18.9 30.3 45.6\n",
    "34B 62.6 - 42.2 6.2 22.6 33.0 44.1\n",
    "70B 69.8 50.1 63.3 13.5 29.9 45.0 64.9\n",
    "StableBeluga2 70B 68.6 51.4 69.6 14.6 28.0 11.4 69.3\n",
    "QWEN\n",
    "1.8B 44.6 54.7 21.2 5.6 17.1 14.8 28.2\n",
    "7B 58.2 63.5 51.7 11.6 29.9 31.6 45.0\n",
    "14B 66.3 72.1 61.3 24.8 32.3 40.8 53.4\n",
    "Unlike position interpolation (PI) (Chen et al., 2023a) which scales each dimension of RoPE equally,\n",
    "NTK-aware interpolation adjusts the base of RoPE to prevent the loss of high-frequency information\n",
    "in a training-free manner. To further improve performance, we have also implemented a trivial\n",
    "extension called dynamic NTK-aware interpolation, which is later formally discussed in (Peng et al.,\n",
    "2023a). It dynamically changes the scale by chunks, avoiding severe performance degradation.\n",
    "These techniques allow us to effectively extend the context length of Transformer models without\n",
    "compromising their computational efficiency or accuracy.\n",
    "QWEN additionally incorporates two attention mechanisms: LogN-Scaling (Chiang & Cholak, 2022;\n",
    "Su, 2023a) and window attention (Beltagy et al., 2020). LogN-Scaling rescales the dot product of\n",
    "the query and value by a factor that depends on the ratio of the context length to the training length,\n",
    "ensuring that the entropy of the attention value remains stable as the context length grows. Window\n",
    "attention restricts the attention to a limited context window, preventing the model from attending to\n",
    "tokens that are too far away.\n",
    "We also observed that the long-context modeling ability of our model varies across layers, with lower\n",
    "layers being more sensitive in context length extension compared to the higher layers. To leverage\n",
    "this observation, we assign different window sizes to each layer, using shorter windows for lower\n",
    "layers and longer windows for higher layers.\n",
    "2.6 EXPERIMENTAL RESULTS\n",
    "To evaluate the zero-shot and few-shot learning capabilities of our models, we conduct a thorough benchmark assessment using a series of datasets. We compare QWEN with the most recent\n",
    "open-source base models, including LLaMA (Touvron et al., 2023a), LLAMA 2 (Touvron et al.,\n",
    "2023b), MPT (Mosaic ML, 2023), Falcon (Almazrouei et al., 2023), Baichuan2 (Yang et al., 2023),\n",
    "ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM Team, 2023), XVERSE (Inc., 2023b),\n",
    "and StableBeluga2 (Stability AI, 2023). Our evaluation covers a total of 7 popular benchmarks,\n",
    "8\n",
    "Table 3: Results of QWEN on long-context inference using various techniques. Our experimental\n",
    "findings reveal that the application of our crucial techniques enables the model to consistently achieve\n",
    "low perplexity as the context length increases. This suggests that these techniques play a significant\n",
    "role in enhancing the model’s ability to comprehend and generate lengthy texts.\n",
    "Model\n",
    "Sequence Length\n",
    "1024 2048 4096 8192 16384\n",
    "QWEN-7B 4.23 3.78 39.35 469.81 2645.09\n",
    "+ dynamic ntk 4.23 3.78 3.59 3.66 5.71\n",
    "+ dynamic ntk + logn 4.23 3.78 3.58 3.56 4.62\n",
    "+ dynamic ntk + logn + window attn 4.23 3.78 3.58 3.49 4.32\n",
    "QWEN-14B - 3.46 22.79 334.65 3168.35\n",
    "+ dynamic ntk + logn + window attn - 3.46 3.29 3.18 3.42\n",
    "which are MMLU (5-shot) (Hendrycks et al., 2020), C-Eval (5-shot) (Huang et al., 2023), GSM8K\n",
    "(8-shot) (Cobbe et al., 2021), MATH (4-shot) (Hendrycks et al., 2021), HumanEval (0-shot) (Chen\n",
    "et al., 2021), MBPP (0-shot) (Austin et al., 2021), and BBH (Big Bench Hard) (3 shot) (Suzgun et al.,\n",
    "2022). We aim to provide a comprehensive summary of the overall performance of our models across\n",
    "these benchmarks.\n",
    "In this evaluation, we focus on the base language models without alignment and collect the baselines’\n",
    "best scores from their official results and OpenCompass (OpenCompass Team, 2023). The results are\n",
    "presented in Table 2.\n",
    "Our experimental results demonstrate that the three QWEN models exhibit exceptional performance\n",
    "across all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are\n",
    "outperformed by QWEN-14B in 3 tasks. QWEN-7B also performs admirably, surpassing LLaMA2-\n",
    "13B and achieving comparable results to Baichuan2-13B. Notably, despite having a relatively small\n",
    "number of parameters, QWEN-1.8B is capable of competitive performance on certain tasks and even\n",
    "outperforms larger models in some instances. The findings highlight the impressive capabilities of\n",
    "the QWEN models, particularly QWEN-14B, and suggest that smaller models, such as QWEN-1.8B,\n",
    "can still achieve strong performance in certain applications.\n",
    "To evaluate the effectiveness of context length extension, Table 3 presents the test results on arXiv3\n",
    "in\n",
    "terms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation,\n",
    "LogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of\n",
    "our models in the context of over 8192 tokens.\n",
    "3 ALIGNMENT\n",
    "Pretrained large language models have been found to be not aligned with human behavior, making\n",
    "them unsuitable for serving as AI assistants in most cases. Recent research has shown that the use of\n",
    "alignment techniques, such as supervised finetuning (SFT) and reinforcement learning from human\n",
    "feedback (RLHF), can significantly improve the ability of language models to engage in natural\n",
    "conversation. In this section, we will delve into the details of how QWEN models have been trained\n",
    "using SFT and RLHF, and evaluate their performance in the context of chat-based assistance.\n",
    "3.1 SUPERVISED FINETUNING\n",
    "To gain an understanding of human behavior, the initial step is to carry out SFT, which finetunes a\n",
    "pretrained LLM on chat-style data, including both queries and responses. In the following sections,\n",
    "we will delve into the details of data construction and training methods.\n",
    "3The dataset contains academic papers from https://arxiv.org.\n",
    "9\n",
    "3.1.1 DATA\n",
    "To enhance the capabilities of our supervised finetuning datasets, we have annotated conversations\n",
    "in multiple styles. While conventional datasets (Wei et al., 2022a) contain a vast amount of data\n",
    "prompted with questions, instructions, and answers in natural language, our approach takes it a step\n",
    "further by annotating human-style conversations. This practice, inspired by Ouyang et al. (2022),\n",
    "aims at improving the model’s helpfulness by focusing on natural language generation for diverse\n",
    "tasks. To ensure the model’s ability to generalize to a wide range of scenarios, we specifically\n",
    "excluded data formatted in prompt templates that could potentially limit its capabilities. Furthermore,\n",
    "we have prioritized the safety of the language model by annotating data related to safety concerns\n",
    "such as violence, bias, and pornography.\n",
    "In addition to data quality, we have observed that the training method can significantly impact the\n",
    "final performance of the model. To achieve this, we utilized the ChatML-style format (OpenAI,\n",
    "2022), which is a versatile meta language capable of describing both the metadata (such as roles)\n",
    "and the content of a turn. This format enables the model to effectively distinguish between various\n",
    "types of information, including system setup, user inputs, and assistant outputs, among others. By\n",
    "leveraging this approach, we can enhance the model’s ability to accurately process and analyze\n",
    "complex conversational data.\n",
    "3.1.2 TRAINING\n",
    "Consistent with pretraining, we also apply next-token prediction as the training task for SFT. We\n",
    "apply the loss masks for the system and user inputs. More details are demonstrated in Section A.1.1.\n",
    "The model’s training process utilizes the AdamW optimizer, with the following hyperparameters: β1\n",
    "set to 0.9, β2 set to 0.95, and ϵ set to 10−8\n",
    ". The sequence length is limited to 2048, and the batch\n",
    "size is 128. The model undergoes a total of 4000 steps, with the learning rate gradually increased\n",
    "over the first 1430 steps, reaching a peak of 2 × 10−6\n",
    ". To prevent overfitting, weight decay is applied\n",
    "with a value of 0.1, dropout is set to 0.1, and gradient clipping is enforced with a limit of 1.0.\n",
    "3.2 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK\n",
    "While SFT has proven to be effective, we acknowledge that its generalization and creativity capabilities may be limited, and it is prone to overfitting. To address this issue, we have implemented\n",
    "Reinforcement Learning from Human Feedback (RLHF) to further align SFT models with human\n",
    "preferences, following the approaches of Ouyang et al. (2022); Christiano et al. (2017). This process\n",
    "involves training a reward model and using Proximal Policy Optimization (PPO) (Schulman et al.,\n",
    "2017) to conduct policy training.\n",
    "3.2.1 REWARD MODEL\n",
    "To create a successful reward model, like building a large language model (LLM), it is crucial to\n",
    "first undergo pretraining and then finetuning. This pretraining process, also known as preference\n",
    "model pretraining (PMP) (Bai et al., 2022b), necessitates a vast dataset of comparison data. This\n",
    "dataset consists of sample pairs, each containing two distinct responses for a single query and their\n",
    "corresponding preferences. Similarly, finetuning is also conducted on this type of comparison data,\n",
    "but with a higher quality due to the presence of quality annotations.\n",
    "During the fine-tuning phase, we gather a variety of prompts and adjust the reward model based on\n",
    "human feedback for responses from the QWEN models. To ensure the diversity and complexity of\n",
    "user prompts are properly taken into account, we have created a classification system with around\n",
    "6600 detailed tags and implemented a balanced sampling algorithm that considers both diversity and\n",
    "complexity when selecting prompts for annotation by the reward model (Lu et al., 2023). To generate\n",
    "a wide range of responses, we have utilized QWEN models of different sizes and sampling strategies,\n",
    "as diverse responses can help reduce annotation difficulties and enhance the performance of the\n",
    "reward model. These responses are then evaluated by annotators following a standard annotation\n",
    "guideline, and comparison pairs are formed based on their scores.\n",
    "In creating the reward model, we utilize the same-sized pre-trained language model QWEN to initiate\n",
    "the process. It is important to mention that we have incorporated a pooling layer into the original\n",
    "10\n",
    "Table 4: Test Accuracy of QWEN preference model pretraining (PMP) and reward model (RM) on\n",
    "diverse human preference benchmark datasets.\n",
    "Dataset QWEN QWEN Anthropic Anthropic OpenAI Stanford OpenAI\n",
    "Helpful-base Helpful-online Helpful-base Helpful-online Summ. SHP PRM800K\n",
    "PMP 62.68 61.62 76.52 65.43 69.60 60.05 70.59\n",
    "RM 74.78 69.71 73.98 64.57 69.99 60.10 70.52\n",
    "QWEN model to extract the reward for a sentence based on a specific end token. The learning rate for\n",
    "this process has been set to a constant value of 3 × 10−6\n",
    ", and the batch size is 64. Additionally, the\n",
    "sequence length is set to 2048, and the training process lasts for a single epoch.\n",
    "We adopted the accuracy on the test dataset as an important but not exclusive evaluation metric for\n",
    "the reward model. In Table 4, we report the test pairwise accuracy of PMP and reward models on\n",
    "diverse human preference benchmark datasets (Bai et al., 2022b; Stiennon et al., 2020; Ethayarajh\n",
    "et al., 2022; Lightman et al., 2023). Specifically, QWEN Helpful-base and QWEN Helpful-online are\n",
    "our proprietary datasets. The responses in QWEN Helpful-base are generated from QWEN without\n",
    "RLHF, whereas QWEN Helpful-online includes responses from QWEN with RLHF. The results show\n",
    "that the PMP model demonstrates high generalization capabilities on out-of-distribution data, and the\n",
    "reward model demonstrates significant improvement on our QWEN reward datasets.\n",
    "3.2.2 REINFORCEMENT LEARNING\n",
    "Our Proximal Policy Optimization (PPO) process involves four models: the policy model, value\n",
    "model, reference model, and reward model. Before starting the PPO procedure, we pause the policy\n",
    "model’s updates and focus solely on updating the value model for 50 steps. This approach ensures\n",
    "that the value model can adapt to different reward models effectively.\n",
    "During the PPO operation, we use a strategy of sampling two responses for each query simultaneously.\n",
    "This strategy has proven to be more effective based on our internal benchmarking evaluations. We set\n",
    "the KL divergence coefficient to 0.04 and normalize the reward based on the running mean.\n",
    "The policy and value models have learning rates of 1 × 10−6\n",
    "and 5 × 10−6\n",
    ", respectively. To enhance\n",
    "training stability, we utilize value loss clipping with a clip value of 0.15. For inference, the policy\n",
    "top-p is set to 0.9. Our findings indicate that although the entropy is slightly lower than when top-p is\n",
    "set to 1.0, there is a faster increase in reward, ultimately resulting in consistently higher evaluation\n",
    "rewards under similar conditions.\n",
    "Additionally, we have implemented a pretrained gradient to mitigate the alignment tax. Empirical\n",
    "findings indicate that, with this specific reward model, the KL penalty is adequately robust to\n",
    "counteract the alignment tax in benchmarks that are not strictly code or math in nature, such as\n",
    "those that test common sense knowledge and reading comprehension. It is imperative to utilize\n",
    "a significantly larger volume of the pretrained data in comparison to the PPO data to ensure the\n",
    "effectiveness of the pretrained gradient. Additionally, our empirical study suggests that an overly\n",
    "large value for this coefficient can considerably impede the alignment to the reward model, eventually\n",
    "compromising the ultimate alignment, while an overly small value would only have a marginal effect\n",
    "on alignment tax reduction.\n",
    "3.3 AUTOMATIC AND HUMAN EVALUATION OF ALIGNED MODELS\n",
    "To showcase the effectiveness of our aligned models, we conduct a comparison with other aligned\n",
    "models on well-established benchmarks, including MMLU (Hendrycks et al., 2020), C-Eval (Huang\n",
    "et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and BBH (Suzgun et al.,\n",
    "2022). Besides the widely used few-shot setting, we test our aligned models in the zero-shot setting\n",
    "to demonstrate how well the models follow instructions. The prompt in a zero-shot setting consists\n",
    "of an instruction and a question without any previous examples in the context. The results of the\n",
    "baselines are collected from their official reports and OpenCompass (OpenCompass Team, 2023).\n",
    "The results in Table 5 demonstrate the effectiveness of our aligned models in understanding human\n",
    "instructions and generating appropriate responses. QWEN-14B-Chat outperforms all other models\n",
    "11\n",
    "Table 5: Performance of aligned models on widely-used benchmarks. We report both zero-shot\n",
    "and few-shot performance of the models.\n",
    "Model Params MMLU C-Eval GSM8K HumanEval BBH\n",
    "0-shot / 5-shot 0-shot / 5-shot 0-shot / 8-shot 0-shot 0-shot / 3-shot\n",
    "Proprietary models\n",
    "GPT-3.5 - - / 69.1 - / 52.5 - / 78.2 73.2 - / 70.1\n",
    "GPT-4 - - / 83.0 - / 69.9 - / 91.4 86.6 - / 86.7\n",
    "Open-source models\n",
    "ChatGLM2 6B 45.5 / 46.0 50.1 / 52.6 - / 28.8 11.0 - / 32.7\n",
    "InternLM-Chat 7B - / 51.1 - / 53.6 - / 33.0 14.6 - / 32.5\n",
    "Baichuan2-Chat 7B - / 52.9 - / 55.6 - / 32.8 13.4 - / 35.8\n",
    "13B - / 57.3 - / 56.7 - / 55.3 17.7 - / 49.9\n",
    "LLAMA 2-CHAT\n",
    "7B - / 46.2 - / 31.9 - / 26.3 12.2 - / 35.6\n",
    "13B - / 54.6 - / 36.2 - / 37.1 18.9 - / 40.1\n",
    "70B - / 63.8 - / 44.3 - / 59.3 32.3 - / 60.8\n",
    "QWEN-CHAT\n",
    "1.8B 42.4 / 43.9 50.7 / 50.3 27.8 / 19.5 14.6 27.1 / 25.0\n",
    "7B 55.8 / 57.0 59.7 / 59.3 50.3 / 54.1 37.2 39.6 / 46.7\n",
    "14B 64.6 / 66.5 69.8 / 71.7 60.1 / 59.3 43.9 46.9 / 58.7\n",
    "except ChatGPT (OpenAI, 2022) and LLAMA 2-CHAT-70B (Touvron et al., 2023b) in all datasets,\n",
    "including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021),\n",
    "HumanEval (Chen et al., 2021), and BBH (Suzgun et al., 2022). In particular, QWEN’s performance\n",
    "in HumanEval, which measures the quality of generated codes, is significantly higher than that of\n",
    "other open-source models.\n",
    "Moreover, QWEN’s performance is consistently better than that of open-source models of similar size,\n",
    "such as LLaMA2 (Touvron et al., 2023b), ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM\n",
    "Team, 2023), and Baichuan2 (Yang et al., 2023). This suggests that our alignment approach, which\n",
    "involves fine-tuning the model on a large dataset of human conversations, has been effective in\n",
    "improving the model’s ability to understand and generate human-like language.\n",
    "Despite this, we have reservations about the ability of traditional benchmark evaluation to accurately\n",
    "measure the performance and potential of chat models trained with alignment techniques in today’s\n",
    "landscape. The results mentioned earlier provide some evidence of our competitive standing, but we\n",
    "believe that it is crucial to develop new evaluation methods specifically tailored to aligned models.\n",
    "We believe that human evaluation is crucial, which is why we have created a carefully curated\n",
    "dataset for this purpose. Our process involved collecting 300 instructions in Chinese that covered\n",
    "a wide range of topics, including knowledge, language understanding, creative writing, coding,\n",
    "and mathematics. To evaluate the performance of different models, we chose the SFT version of\n",
    "QWEN-CHAT-7B and the SFT and RLHF versions of QWEN-CHAT-14B, and added two strong\n",
    "baselines, GPT-3.5 and GPT-44\n",
    ", for comparison. For each instruction, we asked three annotators to\n",
    "rank the model responses by the overall score of helpfulness, informativeness, validity, and other\n",
    "relevant factors. Our dataset and evaluation methodology provides a comprehensive and rigorous\n",
    "assessment of the capabilities of different language models in various domains.\n",
    "Figure 4 illustrates the win rates of the various models. For each model, we report the percentage of\n",
    "wins, ties, and losses against GPT-3.5, with the segments of each bar from bottom to top representing\n",
    "these statistics. The experimental results clearly demonstrate that the RLHF model outperforms\n",
    "the SFT models by significant margins, indicating that RLHF can encourage the model to generate\n",
    "responses that are more preferred by humans. In terms of overall performance, we find that the\n",
    "RLHF model significantly outperforms the SFT models, falling behind GPT-4. This indicates\n",
    "the effectiveness of RLHF for aligning to human preference. To provide a more comprehensive\n",
    "understanding of the models’ performance, we include a case study with examples from different\n",
    "models in Appendix A.2.2. Nonetheless, it remains difficult to accurately capture the gap between our\n",
    "4To obtain the results from the models, we use the OpenAI APIs of GPT-3.5-turbo-0613 and GPT-4-0613.\n",
    "12\n",
    "Average Knowledge Language UnderstandingCreative Writing Math Coding 0\n",
    "20\n",
    "40\n",
    "60\n",
    "80\n",
    "100\n",
    "28.4\n",
    "31.1\n",
    "24.0\n",
    "35.3\n",
    "23.3\n",
    "17.5\n",
    "28.3\n",
    "31.1\n",
    "46.0\n",
    "19.7\n",
    "29.2\n",
    "22.5\n",
    "43.3\n",
    "37.8\n",
    "30.0\n",
    "45.0\n",
    "47.5\n",
    "60.0\n",
    "30.7\n",
    "32.8\n",
    "29.3\n",
    "34.7\n",
    "34.2\n",
    "15.8\n",
    "32.3\n",
    "33.3\n",
    "54.7\n",
    "22.7\n",
    "34.2\n",
    "25.0\n",
    "37.0\n",
    "33.9\n",
    "16.0\n",
    "42.7\n",
    "31.7\n",
    "59.2\n",
    "38.7\n",
    "42.2\n",
    "39.3\n",
    "39.7\n",
    "39.2\n",
    "30.0\n",
    "28.9\n",
    "32.8\n",
    "33.3\n",
    "21.0\n",
    "39.2\n",
    "26.7\n",
    "32.4\n",
    "25.0\n",
    "27.3\n",
    "39.3\n",
    "21.7\n",
    "43.3\n",
    "43.8\n",
    "37.2\n",
    "36.0\n",
    "49.7\n",
    "45.8\n",
    "46.7\n",
    "32.2\n",
    "40.0\n",
    "47.3\n",
    "19.0\n",
    "33.3\n",
    "33.3\n",
    "24.0\n",
    "22.8\n",
    "16.7\n",
    "31.3\n",
    "20.8\n",
    "20.0\n",
    "Winrate (v.s. GPT-3.5)\n",
    "Qwen-7B-Chat (SFT) Qwen-14B-Chat (SFT) Qwen-14B-Chat (RLHF) GPT-4\n",
    "Figure 4: Results of the human evaluation for chat models. We compare Qwen-7B (SFT), Qwen14B (SFT), Qwen-14B (RLHF), as well as GPT-4 against GPT-3.5. Each bar segment represents the\n",
    "percentage of wins, ties, and losses, from bottom to top. On average, the RLHF model outperforms\n",
    "the SFT model. The dataset consists of 300 Chinese instructions.\n",
    "models and the proprietary models. As such, a more extensive and rigorous assessment is required\n",
    "for the chat models.\n",
    "3.4 TOOL USE, CODE INTERPRETER, AND AGENT\n",
    "Table 6: Performance of QWEN on the in-house Chinese benchmark that evaluates its ability to use\n",
    "unseen tools via ReAct prompting.\n",
    "Model Params Tool Selection (Acc.↑) Tool Input (Rouge-L↑) False Positive Error (%)↓\n",
    "GPT-4 - 95 90 15.0\n",
    "GPT-3.5 - 85 88 75.0\n",
    "QWEN-CHAT\n",
    "1.8B 92 89 19.3\n",
    "7B 98 91 7.3\n",
    "14B 98 93 2.4\n",
    "The QWEN models, which are designed to be versatile, have the remarkable ability to assist with\n",
    "(semi-)automating daily tasks by leveraging their skills in tool-use and planning. As such, they can\n",
    "serve as agents or copilots to help streamline various tasks. We explore QWEN’s proficiency in the\n",
    "following areas:\n",
    "• Utilizing unseen tools through ReAct prompting (Yao et al., 2022) (see Table 6).\n",
    "• Using a Python code interpreter to enhance math reasoning, data analysis, and more (see\n",
    "Table 7 and Table 8).\n",
    "• Functioning as an agent that accesses Hugging Face’s extensive collection of multimodal\n",
    "models while engaging with humans (see Table 9).\n",
    "13\n",
    "Table 7: The proportion of code generated by QWEN that is executable on the in-house evaluation\n",
    "benchmark for Code Interpreter. This benchmark examines QWEN’s coding proficiency in math\n",
    "problem solving, data visualization, and general purposes. CODE LLAMA underperforms on\n",
    "visualization tasks because it hallucinates non-existent columns solely based on CSV file names (see\n",
    "Figure 5).\n",
    "Model Params\n",
    "Category\n",
    "Math (%) Visualization (%) General (%) All (%)\n",
    "GPT-4 - 91.9 85.9 82.8 86.8\n",
    "GPT-3.5 - 89.2 65.0 74.1 72.9\n",
    "LLAMA 2-CHAT\n",
    "7B 41.9 33.1 24.1 33.6\n",
    "13B 50.0 40.5 48.3 44.4\n",
    "CODE LLAMA-INSTRUCT\n",
    "7B 85.1 54.0 70.7 65.1\n",
    "13B 93.2 55.8 74.1 68.8\n",
    "InternLM-Chat 7B v1.1 78.4 44.2 62.1 56.3\n",
    "20B 70.3 44.2 65.5 54.9\n",
    "QWEN-CHAT\n",
    "1.8B 33.8 30.1 8.6 26.8\n",
    "7B 82.4 64.4 67.2 70.2\n",
    "14B 89.2 84.1 65.5 81.7\n",
    "Table 8: Correctness of the final response on the in-house evaluation benchmark for Code Interpreter.\n",
    "Visualization-Hard tasks involve planning multiple steps, while Visualization-Easy tasks do not.\n",
    "Visualization-All measures both types of tasks. CODE LLAMA excels in performing VisualizationEasy tasks but tends to underperform in Visualization-Hard tasks, due to its inclination to hallucinate\n",
    "non-existent columns based on the name of a CSV file (see Figure 5).\n",
    "Model Params\n",
    "Category\n",
    "Math (%) Vis.-Hard (%) Vis.-Easy (%) Vis.-All (%)\n",
    "GPT-4 - 82.8 66.7 60.8 63.8\n",
    "GPT-3.5 - 47.3 33.3 55.7 44.2\n",
    "LLAMA 2-CHAT\n",
    "7B 3.9 14.3 39.2 26.4\n",
    "13B 8.3 8.3 40.5 23.9\n",
    "CODE LLAMA-INSTRUCT\n",
    "7B 14.3 26.2 60.8 42.9\n",
    "13B 28.2 27.4 62.0 44.2\n",
    "InternLM-Chat 7B v1.1 28.5 4.8 40.5 22.1\n",
    "20B 34.6 21.4 45.6 33.1\n",
    "QWEN-CHAT\n",
    "1.8B 14.7 3.6 20.3 11.7\n",
    "7B 41.9 40.5 54.4 47.2\n",
    "14B 58.4 53.6 59.5 56.4\n",
    "14\n",
    "Table 9: Results of QWEN-Chat on the Hugging Face Agent benchmark.\n",
    "Task Model Params\n",
    "Metric\n",
    "Tool Selection ↑ Tool Used ↑ Code Correctness ↑\n",
    "Run Mode\n",
    "GPT-4 - 100 100 97.4\n",
    "GPT-3.5 - 95.4 96.3 87.0\n",
    "Starcoder-Base 15B 86.1 87.0 68.9\n",
    "Starcoder 15B 87.0 88.0 68.9\n",
    "QWEN-CHAT\n",
    "1.8B 85.2 84.3 61.1\n",
    "7B 87.0 87.0 71.5\n",
    "14B 93.5 94.4 87.0\n",
    "Chat Mode\n",
    "GPT-4 - 97.9 97.9 98.5\n",
    "GPT-3.5 - 97.3 96.8 89.6\n",
    "Starcoder-Base 15B 97.9 97.9 91.1\n",
    "Starcoder 15B 97.9 97.9 89.6\n",
    "QWEN-CHAT\n",
    "1.8B 93.6 93.6 73.2\n",
    "7B 94.7 94.7 85.1\n",
    "14B 97.9 97.9 95.5\n",
    "To enhance QWEN’s capabilities as an agent or copilot, we employ the self-instruct (Wang et al.,\n",
    "2023c) strategy for SFT. Specifically, we utilize the in-context learning capability of QWEN for\n",
    "self-instruction. By providing a few examples, we can prompt QWEN to generate more relevant\n",
    "queries and generate outputs that follow a specific format, such as ReAct (Yao et al., 2022). We then\n",
    "apply rules and involve human annotators to filter out any noisy samples. Afterwards, the samples\n",
    "are incorporated into QWEN’s training data, resulting in an updated version of QWEN that is more\n",
    "dependable for self-instruction. We iterate through this process multiple times until we gather an\n",
    "ample number of samples that possess both exceptional quality and a wide range of diversity. As a\n",
    "result, our final collection consists of around 2000 high-quality samples.\n",
    "During the finetuning process, we mix these high-quality samples with all the other general-purpose\n",
    "SFT samples, rather than introducing an additional training stage. By doing so, we are able to retain\n",
    "essential general-purpose capabilities that are also pertinent for constructing agent applications.\n",
    "Using Tools via ReAct Prompting We have created and made publicly available a benchmark\n",
    "for evaluating QWEN’s ability to call plugins, tools, functions, or APIs using ReAct Prompting (see\n",
    "Qwen Team, Alibaba Group, 2023b). To ensure fair evaluation, we have excluded any plugins that\n",
    "were included in QWEN’s training set from the evaluation set. The benchmark assesses the model’s\n",
    "accuracy in selecting the correct plugin from a pool of up to five candidates, as well as the plausibility\n",
    "of the parameters passed into the plugin and the frequency of false positives. In this evaluation, a\n",
    "false positive occurs when the model incorrectly invokes a plugin in response to a query, despite not\n",
    "being required to do so.\n",
    "The results presented in Table 6 demonstrate that QWEN consistently achieves higher accuracy in\n",
    "identifying the relevance of a query to the available tools as the model size increases. However,\n",
    "the table also highlights that beyond a certain point, there is little improvement in performance\n",
    "when it comes to selecting the appropriate tool and providing relevant arguments. This suggests that\n",
    "the current preliminary benchmark may be relatively easy and may require further enhancement in\n",
    "future iterations. It is worth noting that GPT-3.5 stands out as an exception, displaying suboptimal\n",
    "performance on this particular benchmark. This could potentially be attributed to the fact that the\n",
    "benchmark primarily focuses on the Chinese language, which may not align well with GPT-3.5’s\n",
    "capabilities. Additionally, we observe that GPT-3.5 tends to attempt to use at least one tool, even if\n",
    "the query cannot be effectively addressed by the provided tools.\n",
    "Using Code Interpreter for Math Reasoning and Data Analysis The Python code interpreter\n",
    "is widely regarded as a powerful tool for augmenting the capabilities of an LLM agent. It is\n",
    "15\n",
    "worth investigating whether QWEN can harness the full potential of this interpreter to enhance its\n",
    "performance in diverse domains, such as mathematical reasoning and data analysis. To facilitate this\n",
    "exploration, we have developed and made publicly available a benchmark that is specifically tailored\n",
    "for this purpose (see Qwen Team, Alibaba Group, 2023a).\n",
    "The benchmark encompasses three primary categories of tasks: math problem-solving, data visualization, and other general-purpose tasks like file post-processing and web crawling. Within the\n",
    "visualization tasks, we differentiate between two levels of difficulty. The easier level can be achieved\n",
    "by simply writing and executing a single code snippet without the need for advanced planning skills.\n",
    "However, the more challenging level requires strategic planning and executing multiple code snippets\n",
    "in a sequential manner. This is because the subsequent code must be written based on the output of\n",
    "the previous code. For example, an agent may need to examine the structure of a CSV file using one\n",
    "code snippet before proceeding to write and execute additional code to create a plot.\n",
    "Regarding evaluation metrics, we consider both the executability and correctness of the generated\n",
    "code. To elaborate on the correctness metrics, for math problems, we measure accuracy by verifying\n",
    "if the ground truth numerical answer is present in both the code execution result and the final response.\n",
    "When it comes to data visualization, we assess accuracy by utilizing QWEN-VL (Bai et al., 2023),\n",
    "a powerful multimodal language model. QWEN-VL is capable of answering text questions paired\n",
    "with images, and we rely on it to confirm whether the image generated by the code fulfills the user’s\n",
    "request.\n",
    "The results regarding executability and correctness are presented in Table 7 and Table 8, respectively.\n",
    "It is evident that CODE LLAMA generally outperforms LLAMA 2, its generalist counterpart, which\n",
    "is not surprising since this benchmark specifically requires coding skills. However, it is worth noting\n",
    "that specialist models that are optimized for code synthesis do not necessarily outperform generalist\n",
    "models. This is due to the fact that this benchmark encompasses various skills beyond coding,\n",
    "such as abstracting math problems into equations, understanding language-specified constraints, and\n",
    "responding in the specified format such as ReAct. Notably, QWEN-7B-CHAT and QWEN-14B-CHAT\n",
    "surpass all other open-source alternatives of similar scale significantly, despite being generalist\n",
    "models.\n",
    "Serving as a Hugging Face Agent Hugging Face provides a framework called the Hugging Face\n",
    "Agent or Transformers Agent (Hugging Face, 2023), which empowers LLM agents with a curated set\n",
    "of multimodal tools, including speech recognition and image synthesis. This framework allows an\n",
    "LLM agent to interact with humans, interpret natural language commands, and employ the provided\n",
    "tools as needed.\n",
    "To evaluate QWEN’s effectiveness as a Hugging Face agent, we utilized the evaluation benchmarks\n",
    "offered by Hugging Face. The results are presented in Table 9. The evaluation results reveal that\n",
    "QWEN performs quite well in comparison to other open-source alternatives, only slightly behind the\n",
    "proprietary GPT-4, demonstrating QWEN’s competitive capabilities.\n",
    "4 CODE-QWEN: SPECIALIZED MODEL FOR CODING\n",
    "Training on domain-specific data has been shown to be highly effective, particularly in the case\n",
    "of code pretraining and finetuning. A language model that has been reinforced with training on\n",
    "code data can serve as a valuable tool for coding, debugging, and interpretation, among other tasks.\n",
    "In this work, we have developed a series of generalist models using pretraining and alignment\n",
    "techniques. Building on this foundation, we have created domain-specific models for coding by\n",
    "leveraging the base language models of QWEN, including continued pretrained model, CODE-QWEN\n",
    "and supervised finetuned model, CODE-QWEN-CHAT. Both models have 14 billion and 7 billion\n",
    "parameters versions.\n",
    "4.1 CODE PRETRAINING\n",
    "We believe that relying solely on code data for pretraining can result in a significant loss of the ability\n",
    "to function as a versatile assistant. Unlike previous approaches that focused solely on pretraining\n",
    "on code data (Li et al., 2022; 2023d), we take a different approach (Roziere et al., 2023) by starting `\n",
    "with our base models QWEN trained on a combination of text and code data, and then continuing to\n",
    "16\n",
    "pretrain on the code data. We continue to pretrain the models on a total of around 90 billion tokens.\n",
    "During the pre-training phase, we initialize the model using the base language models QWEN. Many\n",
    "applications that rely on specialized models for coding may encounter lengthy contextual scenarios,\n",
    "such as tool usage and code interpretation, as mentioned in Section 3.4. To address this issue, we\n",
    "train our models with context lengths of up to 8192. Similar to base model training in Section\n",
    "2.4, we employ Flash Attention (Dao et al., 2022) in the attention modules, and adopt the standard\n",
    "optimizer AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017), setting β1 = 0.9, β2 = 0.95,\n",
    "and ϵ = 10−8\n",
    ". We set the learning rate as 6.0 × 10−5\n",
    "for CODE-QWEN-14B and 3.0 × 10−5\n",
    "for\n",
    "CODE-QWEN-7B, with 3% warm up iterations and no learning rate decays.\n",
    "4.2 CODE SUPERVISED FINE-TUNING\n",
    "After conducting a series of empirical experiments, we have determined that the multi-stage SFT\n",
    "strategy yields the best performance compared to other methods. In the supervised fine-tuning stage,\n",
    "the model CODE-QWEN-CHAT initialized by the code foundation model CODE-QWEN are optimized\n",
    "by the AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) optimizer (β1 = 0.9, β2 = 0.95,\n",
    "ϵ = 10−8\n",
    ") with a learning rate of 2.0 × 10−6\n",
    "and 1.0 × 10−5\n",
    "for the 14B and 7B model respectively.\n",
    "The learning rate increases to the peaking value with the cosine learning rate schedule (3% warm-up\n",
    "steps) and then remains constant.\n",
    "4.3 EVALUATION\n",
    "Our CODE-QWEN models have been compared with both proprietary and open-source language\n",
    "models, as shown in Tables 10 and 11. These tables present the results of our evaluation on the\n",
    "test sets of Humaneval (Chen et al., 2021), MBPP (Austin et al., 2021), and the multi-lingual code\n",
    "generation benchmark HUMANEVALPACK (Muennighoff et al., 2023). The comparison is based on\n",
    "the pass@1 performance of the models on these benchmark datasets. The results of this comparison\n",
    "are clearly demonstrated in Tables 10 and 11.\n",
    "Our analysis reveals that specialized models, specifically CODE-QWEN and CODE-QWEN-CHAT, significantly outperform previous baselines with similar parameter counts, such as OCTOGEEX (Muennighoff et al., 2023), InstructCodeT5+ (Wang et al., 2023d), and CodeGeeX2 (Zheng et al., 2023). In\n",
    "fact, these models even rival the performance of larger models like Starcoder (Li et al., 2023d).\n",
    "When compared to some of the extremely large-scale closed-source models, CODE-QWEN and CODEQWEN-CHAT demonstrate clear advantages in terms of pass@1. However, it is important to note that\n",
    "these models fall behind the state-of-the-art methods, such as GPT-4, in general. Nonetheless, with\n",
    "the continued scaling of both model size and data size, we believe that this gap can be narrowed in\n",
    "the near future.\n",
    "It is crucial to emphasize that the evaluations mentioned previously are insufficient for grasping\n",
    "the full extent of the strengths and weaknesses of the models. In our opinion, it is necessary to\n",
    "develop more rigorous tests to enable us to accurately assess our relative performance in comparison\n",
    "to GPT-4.\n",
    "5 MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING\n",
    "We have created a mathematics-specialized model series called MATH-QWEN-CHAT, which is\n",
    "built on top of the QWEN pretrained language models. Specifically, we have developed assistant\n",
    "models that are specifically designed to excel in arithmetic and mathematics and are aligned with\n",
    "human behavior. We are releasing two versions of this model series, MATH-QWEN-14B-CHAT and\n",
    "MATH-QWEN-7B-CHAT, which have 14 billion and 7 billion parameters, respectively.\n",
    "5.1 TRAINING\n",
    "We carry out math SFT on our augmented math instructional dataset for mathematics reasoning,\n",
    "and therefore we obtain the chat model, MATH-QWEN-CHAT, directly. Owing to shorter average\n",
    "lengths of the math SFT data, we use a sequence length of 1024 for faster training. Most user inputs\n",
    "in the math SFT dataset are examination questions, and it is easy for the model to predict the input\n",
    "17\n",
    "Table 10: Results of pass@1 (%) on HumanEval and MBPP. Most scores are retrieved from the\n",
    "papers of StarCoder (Li et al., 2023d), CodeT5+ (Wang et al., 2023d), WizardCoder (Luo et al.,\n",
    "2023b) and CODE LLAMA (Roziere et al., 2023). `\n",
    "Model Params HumanEval MBPP\n",
    "Proprietary models\n",
    "PaLM 540B 26.2 36.8\n",
    "PaLM-Coder 540B 36.0 47.0\n",
    "PaLM 2-S - 37.6 50.0\n",
    "Code-Cushman-001 - 33.5 45.9\n",
    "Code-Davinci-002 - 47.0 58.1\n",
    "GPT-3.5 - 73.2 -\n",
    "GPT-4 - 86.6 -\n",
    "Open-source models\n",
    "LLAMA 2\n",
    "7B 12.2 20.8\n",
    "13B 20.1 27.6\n",
    "34B 22.6 33.8\n",
    "70B 30.5 45.4\n",
    "CodeGen-Multi 16B 18.3 20.9\n",
    "CodeGen-Mono 16B 29.3 35.3\n",
    "CodeGeeX2 6B 35.9 -\n",
    "StarCoder-Prompted 15B 40.8 49.5\n",
    "CodeT5+ 16B 30.9 -\n",
    "InstructCodeT5+ 16B 35.0 -\n",
    "CODE LLAMA\n",
    "7B 33.5 41.4\n",
    "13B 36.0 47.0\n",
    "34B 48.8 55.0\n",
    "CODE LLAMA-INSTRUCT\n",
    "7B 34.8 44.4\n",
    "13B 42.7 49.4\n",
    "34B 41.5 57.0\n",
    "CODE LLAMA-PYTHON\n",
    "7B 38.4 47.6\n",
    "13B 43.3 49.0\n",
    "34B 53.7 56.2\n",
    "UNNATURAL CODE LLAMA 34B 62.2 61.2\n",
    "WizardCoder-Python 13B 64.0 55.6\n",
    "34B 73.2 61.2\n",
    "QWEN-CHAT\n",
    "7B 37.2 35.8\n",
    "14B 43.9 46.4\n",
    "CODE-QWEN\n",
    "7B 40.2 41.8\n",
    "14B 45.1 51.4\n",
    "CODE-QWEN-CHAT\n",
    "7B 43.3 44.2\n",
    "14B 66.4 52.4\n",
    "18\n",
    "Table 11: Zero-shot pass@1 (%) performance on the HUMANEVALPACK (synthesize) benchmark. The baseline results are partly from OCTOPACK (Muennighoff et al., 2023).\n",
    "Model Params\n",
    "Programming Language\n",
    "Python JavaScript Java Go C++ Rust Avg.\n",
    "Proprietary models\n",
    "GPT-4 - 86.6 82.9 81.7 72.6 78.7 67.1 78.3\n",
    "Open-source models\n",
    "InstructCodeT5+ 16B 37.0 18.9 17.4 9.5 19.8 0.3 17.1\n",
    "StarChat-β 15B 33.5 31.4 26.7 25.5 26.6 14.0 26.3\n",
    "StarCoder 15B 33.6 30.8 30.2 17.6 31.6 21.8 27.6\n",
    "CodeGeeX2 6B 35.9 32.2 30.8 22.5 29.3 18.1 28.1\n",
    "OCTOGEEX 6B 44.7 33.8 36.9 21.9 32.3 15.7 30.9\n",
    "OCTOCODER 15B 46.2 39.2 38.2 30.4 35.6 23.4 35.5\n",
    "WizardCoder 15B 59.8 49.5 36.1 36.4 40.9 20.2 40.5\n",
    "QWEN-CHAT\n",
    "7B 37.2 23.2 32.9 20.7 22.0 9.1 24.2\n",
    "14B 43.9 38.4 42.7 34.1 24.4 18.9 33.7\n",
    "CODE-QWEN\n",
    "7B 40.2 40.4 40.2 26.2 20.7 15.8 30.6\n",
    "14B 45.1 51.8 57.3 39.6 18.2 20.7 38.8\n",
    "CODE-QWEN-CHAT\n",
    "7B 43.3 41.5 49.4 29.3 32.9 20.1 36.1\n",
    "14B 66.4 58.5 56.1 47.6 54.2 28.7 51.9\n",
    "Table 12: Results of models on mathematical reasoning. We report the accuracy of QWEN for all\n",
    "benchmarks using greedy decoding. For MATH, we are reporting QWEN’s performances on the test\n",
    "set from Lightman et al. (2023).\n",
    "Model Params GSM8K MATH Math401 Math23K\n",
    "Proprietary models\n",
    "GPT-4 - 92.0 42.5 83.5 74.0\n",
    "GPT-3.5 - 80.8 34.1 75.1 60.0\n",
    "Minerva\n",
    "8B 16.2 14.1 - -\n",
    "62B 52.4 27.6 - -\n",
    "540B 58.8 33.6 - -\n",
    "Open-source models\n",
    "LLaMA-1 RFT 7B 46.5 5.2 - -\n",
    "13B 52.1 5.1 - -\n",
    "WizardMath\n",
    "7B 54.9 10.7 - -\n",
    "13B 63.9 14.0 - -\n",
    "70B 81.6 22.7 - -\n",
    "GAIRMath-Abel\n",
    "7B 59.7 13.0 - -\n",
    "13B 66.4 17.3 - -\n",
    "70B 83.6 28.3 - -\n",
    "QWEN-CHAT\n",
    "7B 50.3 6.8 57.4 51.2\n",
    "14B 60.1 18.4 70.1 67.0\n",
    "MATH-QWEN-CHAT\n",
    "7B 62.5 17.2 80.8 75.4\n",
    "14B 69.8 24.2 85.0 78.4\n",
    "19\n",
    "format and it is meaningless for the model to predict the input condition and numbers which could be\n",
    "random. Thus, we mask the inputs of the system and user to avoid loss computation on them and find\n",
    "masking them accelerates the convergence during our preliminary experiments. For optimization, we\n",
    "use the AdamW optimizer with the same hyperparameters of SFT except that we use a peak learning\n",
    "rate of 2 × 10−5\n",
    "and a training step of 50 000.\n",
    "5.2 EVALUATION\n",
    "We evaluate models on the test sets of GSM8K (Grade school math) (Cobbe et al., 2021), MATH\n",
    "(Challenging competition math problems) (Hendrycks et al., 2021), Math401 (Arithmetic ability) (Yuan et al., 2023b), and Math23K (Chinese grade school math) (Wang et al., 2017). We compare\n",
    "MATH-QWEN-CHAT with proprietary models ChatGPT and Minerva (Lewkowycz et al., 2022) and\n",
    "open-sourced math-specialized model RFT (Yuan et al., 2023a), WizardMath (Luo et al., 2023a), and\n",
    "GAIRMath-Abel (Chern et al., 2023a) in Table 12. MATH-QWEN-CHAT models show better math\n",
    "reasoning and arithmetic abilities compared to open-sourced models and QWEN-CHAT models of\n",
    "similar sizes. Compared to proprietary models, MATH-QWEN-7B-CHAT outperforms Minerva-8B in\n",
    "MATH. MATH-QWEN-14B-CHAT is chasing Minerva-62B and GPT-3.5 in GSM8K and MATH and\n",
    "delivers better performance on arithmetic ability and Chinese math problems.\n",
    "6 RELATED WORK\n",
    "6.1 LARGE LANGUAGE MODELS\n",
    "The excitement of LLM began with the introduction of the Transformer architecture (Vaswani et al.,\n",
    "2017), which was then applied to pretraining large-scale data by researchers such as Radford et al.\n",
    "(2018); Devlin et al. (2018); Liu et al. (2019). These efforts led to significant success in transfer\n",
    "learning, with model sizes growing from 100 million to over 10 billion parameters (Raffel et al.,\n",
    "2020; Shoeybi et al., 2019).\n",
    "In 2020, the release of GPT-3, a massive language model that is 10 times larger than T5, demonstrated\n",
    "the incredible potential of few-shot and zero-shot learning through prompt engineering and in-context\n",
    "learning, and later chain-of-thought prompting (Wei et al., 2022c). This success has led to a number\n",
    "of studies exploring the possibilities of further scaling these models (Scao et al., 2022; Zhang et al.,\n",
    "2022; Du et al., 2021; Zeng et al., 2022; Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022;\n",
    "Black et al., 2022; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Thoppilan\n",
    "et al., 2022). As a result, the community has come to view these large language models as essential\n",
    "foundations for downstream models (Bommasani et al., 2021).\n",
    "The birth of ChatGPT (OpenAI, 2022) and the subsequent launch of GPT-4 (OpenAI, 2023) marked\n",
    "two historic moments in the field of artificial intelligence, demonstrating that large language models\n",
    "(LLMs) can serve as effective AI assistants capable of communicating with humans. These events\n",
    "have sparked interests among researchers and developers in building language models that are aligned\n",
    "with human values and potentially even capable of achieving artificial general intelligence (AGI) (Anil\n",
    "et al., 2023; Anthropic, 2023a;b).\n",
    "One notable development in this area is the emergence of open-source LLMs, specifically\n",
    "LLaMA (Touvron et al., 2023a) and LLAMA 2 (Touvron et al., 2023b), which have been recognized\n",
    "as the most powerful open-source language models ever created. This has led to a surge of activity\n",
    "in the open-source community (Wolf et al., 2019), with a series of large language models being\n",
    "developed collaboratively to build upon this progress (Mosaic ML, 2023; Almazrouei et al., 2023;\n",
    "ChatGLM2 Team, 2023; Yang et al., 2023; InternLM Team, 2023).\n",
    "6.2 ALIGNMENT\n",
    "The community was impressed by the surprising effectiveness of alignment on LLMs. Previously,\n",
    "LLMs without alignment often struggle with issues such as repetitive generation, hallucination,\n",
    "and deviation from human preferences. Since 2021, researchers have been diligently working on\n",
    "developing methods to enhance the performance of LLMs in downstream tasks (Wei et al., 2022a;\n",
    "Sanh et al., 2021; Longpre et al., 2023; Chung et al., 2022; Muennighoff et al., 2022). Furthermore,\n",
    "20\n",
    "researchers have been actively exploring ways to align LLMs with human instructions (Ouyang et al.,\n",
    "2022; Askell et al., 2021; Bai et al., 2022b;c). One major challenge in alignment research is the\n",
    "difficulty of collecting data. While OpenAI has utilized its platform to gather human prompts or\n",
    "instructions, it is not feasible for others to collect such data.\n",
    "However, there has been some progress in this area, such as the self-instruct approach proposed\n",
    "in Wang et al. (2023c). This innovative work offers a potential solution to the data collection problem\n",
    "in alignment research. As a result, there has been a surge in open-source chat data, including\n",
    "Alpaca (Taori et al., 2023), MOSS (Sun et al., 2023a), Dolly (Conover et al., 2023), Evol-Instruct (Xu\n",
    "et al., 2023b), and others (Sun et al., 2023b; Xu et al., 2023a;c; Chen et al., 2023c; Ding et al.,\n",
    "2023; Ji et al., 2023; Yang, 2023). Similarly, there has been an increase in open-source chat models,\n",
    "such as Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Guanaco (Dettmers et al., 2023),\n",
    "MOSS (Sun et al., 2023a), WizardLM (Xu et al., 2023b), and others (Xu et al., 2023c; Chen et al.,\n",
    "2023c; Ding et al., 2023; Wang et al., 2023b).\n",
    "To train an effective chat model, available solutions are mostly based on SFT and RLHF (Ouyang\n",
    "et al., 2022). While SFT is similar to pretraining, it focuses on instruction following using the\n",
    "aforementioned data. However, for many developers, the limited memory capacity is a major obstacle\n",
    "to further research in SFT. As a result, parameter-efficient tuning methods, such as LoRA (Hu et al.,\n",
    "2021) and Q-LoRA (Dettmers et al., 2023), have gained popularity in the community. LoRA tunes\n",
    "only low-rank adapters, while Q-LoRA builds on LoRA and utilizes 4-bit quantized LLMs and\n",
    "paged attention (Dettmers et al., 2022; Frantar et al., 2022; Kwon et al., 2023). In terms of RLHF,\n",
    "recent methods such as PPO (Schulman et al., 2017; Touvron et al., 2023b) have been adopted, but\n",
    "there are also alternative techniques aimed at addressing the complexity of optimization, such as\n",
    "RRHF (Yuan et al., 2023c), DPO (Rafailov et al., 2023), and PRO (Song et al., 2023). Despite the\n",
    "ongoing debate about the effectiveness of RLHF, more evidence is needed to understand how it\n",
    "enhances the intelligence of LLMs and what potential drawbacks it may have.\n",
    "6.3 TOOL USE AND AGENTS\n",
    "LLM’s planning function allows for the invocation of tools, such as APIs or agent capabilities,\n",
    "through in-context learning, as demonstrated by Schick et al. (2023). Yao et al. (2022) introduced\n",
    "ReAct, a generation format that enables the model to generate thoughts on which tool to use, accept\n",
    "input from API observations, and generate a response. GPT-3.5 and GPT-4, when prompted with\n",
    "few shots, have shown consistent and impressive performance. In addition to tool usage, LLMs can\n",
    "utilize external memory sources like knowledge bases (Hu et al., 2023; Zhong et al., 2023b) or search\n",
    "engines (Nakano et al., 2021; Liu et al., 2023b) to generate more accurate and informative answers.\n",
    "This has led to the popularity of frameworks like LangChain (LangChain, Inc., 2023). The research on\n",
    "LLMs for tool use has also sparked interest in building agents with LLM capabilities, such as agents\n",
    "that can call different AI models (Shen et al., 2023; Li et al., 2023a), embodied lifelong learning or\n",
    "multimodal agents (Wang et al., 2023a; Driess et al., 2023), and multiple agents interacting with each\n",
    "other and even building a micro-society (Chen et al., 2023b; Li et al., 2023b; Xu et al., 2023d; Hong\n",
    "et al., 2023).\n",
    "6.4 LLM FOR CODING\n",
    "Previous research has demonstrated that LLMs possess remarkable capabilities in code understanding\n",
    "and generation, particularly those with massive numbers of parameters (Chowdhery et al., 2022;\n",
    "Anil et al., 2023; Rae et al., 2021; Hoffmann et al., 2022). Moreover, several LLMs have been pretrained, continued pre-trained, or fine-tuned on coding-related data, which has resulted in significantly\n",
    "improved performance compared to general-purpose LLMs. These models include Codex Chen\n",
    "et al. (2021), AlphaCode (Li et al., 2022), SantaCoder (Allal et al., 2023), Starcoder-Base (Li et al.,\n",
    "2023d), InCoder (Fried et al., 2022), CodeT5 (Wang et al., 2021), CodeGeeX (Zheng et al., 2023),\n",
    "and CODE LLAMA (Roziere et al., 2023). In addition to these models, recent studies have focused on `\n",
    "developing specialized alignment techniques for coding, such as Code Llama-Instruct (Roziere et al., `\n",
    "2023) and StarCoder (Li et al., 2023d). These models can assist developers in various code-related\n",
    "tasks, including code generation (Chen et al., 2021; Austin et al., 2021), code completion (Zhang\n",
    "et al., 2023a), code translation (Szafraniec et al., 2023), bug fixing (Muennighoff et al., 2023), code\n",
    "refinement (Liu et al., 2023c), and code question answering (Liu & Wan, 2021). In a word, LLMs\n",
    "21\n",
    "have the potential to revolutionize the field of coding by providing developers with powerful tools for\n",
    "code comprehension, generation, and related tasks.\n",
    "6.5 LLM FOR MATHEMATICS\n",
    "LLMs with a certain model scale have been found to possess the ability to perform mathematical\n",
    "reasoning (Wei et al., 2022b; Suzgun et al., 2022). In order to encourage LLMs to achieve better\n",
    "performance on math-related tasks, researchers have employed techniques such as chain-of-thought\n",
    "prompting (Wei et al., 2022c) and scratchpad (Nye et al., 2021), which have shown promising results.\n",
    "Additionally, self-consistency (Wang et al., 2022) and least-to-most prompting (Zhou et al., 2022)\n",
    "have further improved the performance of these models on these tasks. However, prompt engineering\n",
    "is a time-consuming process that requires a lot of trial and error, and it is still difficult for LLMs to\n",
    "consistently perform well or achieve satisfactory results in solving mathematical problems. Moreover,\n",
    "simply scaling the data and model size is not an efficient way to improve a model’s mathematical\n",
    "reasoning abilities. Instead, pretraining on math-related corpora has been shown to consistently\n",
    "enhance these capabilities (Hendrycks et al., 2021; Lewkowycz et al., 2022; Taylor et al., 2022;\n",
    "Lightman et al., 2023). Additionally, fine-tuning on math-related instruction-following datasets (Si\n",
    "et al., 2023; Yuan et al., 2023a; Luo et al., 2023a; Yue et al., 2023; Chern et al., 2023a; Yu et al.,\n",
    "2023), has also been effective and more cost-effective than math-specific pretraining. Despite their\n",
    "limitations in terms of accuracy, LLMs still have significant potential to assist users with practical\n",
    "mathematical problems. There is ample scope for further development in this area.\n",
    "7 CONCLUSION\n",
    "In this report, we present the QWEN series of large language models, which showcase the latest\n",
    "advancements in natural language processing. With 14B, 7B, and 1.8B parameters, these models\n",
    "have been pre-trained on massive amounts of data, including trillions of tokens, and fine-tuned using\n",
    "cutting-edge techniques such as SFT and RLHF. Additionally, the QWEN series includes specialized\n",
    "models for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWENCHAT, which have been trained on domain-specific data to excel in their respective fields. Our results\n",
    "demonstrate that the QWEN series is competitive with existing open-source models and even matches\n",
    "the performance of some proprietary models on comprehensive benchmarks and human evaluation.\n",
    "We believe that the open access of QWEN will foster collaboration and innovation within the\n",
    "community, enabling researchers and developers to build upon our work and push the boundaries of\n",
    "what is possible with language models. By providing these models to the public, we hope to inspire\n",
    "new research and applications that will further advance the field and contribute to our understanding of\n",
    "the variables and techniques introduced in realistic settings. In a nutshell, the QWEN series represents\n",
    "a major milestone in our development of large language models, and we are excited to see how it will\n",
    "be used to drive progress and innovation in the years to come.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "efafdf97-6f7b-415d-b5ca-d7dcba9dd045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 21249\n",
      "num chunk: 11\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "split chunks\n",
    "\"\"\"\n",
    "\n",
    "text = ori_text#.replace('\\n', '')\n",
    "matches = re.finditer(r'\\n', text)\n",
    "print(\"text length:\", len(text))\n",
    "\n",
    "max_chunk_length = 2048\n",
    "result_chunks = []\n",
    "\n",
    "left = 0\n",
    "right = 0\n",
    "match = -1\n",
    "while(match is not None):\n",
    "    try:\n",
    "        match = next(matches)\n",
    "        midx = match.start()\n",
    "    except:\n",
    "        match = None\n",
    "        midx = len(text)\n",
    "    \n",
    "    if midx - left < max_chunk_length:\n",
    "        right = midx\n",
    "    else:\n",
    "        # text split is longer than max_chuck_length\n",
    "        if left >= right:\n",
    "            while(midx - left >= max_chunk_length):\n",
    "                chunk = text[left:left+max_chunk_length]\n",
    "                left += max_chunk_length\n",
    "                right += max_chunk_length\n",
    "                result_chunks.append(chunk)\n",
    "            right = midx\n",
    "        chunk = text[left:right+1]\n",
    "        left = right+1\n",
    "        right = midx\n",
    "        result_chunks.append(chunk)\n",
    "        \n",
    "if left < right: # get last split\n",
    "    chunk = text[left:right+1]\n",
    "    result_chunks.append(chunk)\n",
    "    \n",
    "print(\"num chunk:\", len(result_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "b2b0d2a6-eb92-4f99-b756-03ad92b92200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "2 Architectural details\n",
      "2.1 Sparse Mixture of Experts\n",
      "3 Results\n",
      "3.1 Multilingual benchmarks\n",
      "3.2 Long range performance\n",
      "3.3 Bias Benchmarks\n",
      "\n",
      "CPU times: user 50.8 ms, sys: 14.6 ms, total: 65.4 ms\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "extract ToC\n",
    "\"\"\"\n",
    "\n",
    "api_server_url = \"http://localhost:21122\"\n",
    "instruct = (\n",
    "    \"You are a Table of Contents extractor. \"\n",
    "    \"User will speak to you questions. \"\n",
    "    \"You must reply only with [목차(Table of Contents)] part extracted from the questions. \"\n",
    "    \"You must keep original text. Do not change original text. \"\n",
    "    \"And you must not involve [dotted line, page number, 제목(title), 참고문헌(reference), 부록(appendix), content, explanation, summary, predicted]. \"\n",
    "    \"When there is no Table of Contents, you must reply with \\\"없음\\\". \"\n",
    "    \"do not write explanations.\"\n",
    ")\n",
    "prompt_template = \"\"\"### System:\n",
    "{instruct}\n",
    "### User: {data}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "def send_extract_toc(tocs):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_data - 1:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = result_chunks[pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_data}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        # response\n",
    "        prompt = prompt_template.format(\n",
    "            instruct=instruct,\n",
    "            data=data\n",
    "        )\n",
    "        input_json = {\n",
    "            \"model_name\": \"MDIEM-toc3\", #\"OLLM-Small_2024.01.16\", #\"MDIEM-toc3\",\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.8,\n",
    "            \"max_new_tokens\": 512,\n",
    "        }\n",
    "\n",
    "        ret = requests.post(\n",
    "            api_server_url + \"/worker_generate\",\n",
    "            json=input_json,\n",
    "            timeout=30,\n",
    "        )\n",
    "\n",
    "        result = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "        tocs[pidx] = result\n",
    "\n",
    "tocs = [\"\"] * len(result_chunks)\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_data = len(result_chunks)\n",
    "n_thread = 16\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_extract_toc, args=(tocs,)) # \n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "    \n",
    "first_toc = '\\n'.join(tocs)\n",
    "# response\n",
    "prompt = prompt_template.format(\n",
    "    instruct=instruct,\n",
    "    data=first_toc\n",
    ")\n",
    "input_json = {\n",
    "    \"model_name\": \"MDIEM-toc3\", #\"OLLM-Small_2024.01.16\", # \n",
    "    \"prompt\": prompt,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "}\n",
    "\n",
    "ret = requests.post(\n",
    "    api_server_url + \"/worker_generate\",\n",
    "    json=input_json,\n",
    "    timeout=30,\n",
    ")\n",
    "\n",
    "second_toc = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "print(second_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "a7922757-0708-4006-881a-d5c5293be566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "\n",
      "2 Architectural details\n",
      "2.1 Sparse Mixture of Experts\n",
      "\n",
      "없음\n",
      "\n",
      "3 Results\n",
      "\n",
      "없음\n",
      "\n",
      "3.1 Multilingual benchmarks\n",
      "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\n",
      "Active\n",
      "Params\n",
      "French German Spanish Italian\n",
      "Model arc-c HellaS MMLUarc-c HellaS MMLUarc-c HellaS MMLUarc-c HellaS MMLU\n",
      "LLaMA 1 33B 33B 39.3% 68.1% 49.9% 41.1% 63.3% 48.7% 45.7% 69.8% 52.3% 42.9% 65.4% 49.0%\n",
      "LLaMA 2 70B 70B 49.9% 72.5% 64.3% 47.3% 68.7% 64.2% 50.5% 74.5% 66.0% 49.4% 70.9% 65.1%\n",
      "Mixtral 8x7B 13B 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9%\n",
      "\n",
      "3.2 Long range performance\n",
      "3.3 Bias Benchmarks\n",
      "\n",
      "없음\n",
      "\n",
      "없음\n",
      "\n",
      "없음\n",
      "\n",
      "없음\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(first_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "405f3928-f86a-42c2-b011-5a4cef5bd418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction [0]\n",
      "2 Architectural details [2636]\n",
      "2.1 Sparse Mixture of Experts [3189]\n",
      "3 Results [6518]\n",
      "3.1 Multilingual benchmarks [11273]\n",
      "3.2 Long range performance [12235]\n",
      "3.3 Bias Benchmarks [13127]\n",
      "[('1 Introduction', 0), ('2 Architectural details', 2636), ('2.1 Sparse Mixture of Experts', 3189), ('3 Results', 6518), ('3.1 Multilingual benchmarks', 11273), ('3.2 Long range performance', 12235), ('3.3 Bias Benchmarks', 13127)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "make ToC-Contents Groups\n",
    "\"\"\"\n",
    "\n",
    "toc_string = re.sub(r'\\n+', '\\n', second_toc)\n",
    "parts = toc_string.split('\\n')\n",
    "\n",
    "MIN_SPLIT_LEN = 1\n",
    "\n",
    "def remove_empty(parts):\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        if len(part) <= MIN_SPLIT_LEN: continue\n",
    "        new_parts.append(part)\n",
    "    return new_parts\n",
    "\n",
    "parts = remove_empty(parts)\n",
    "\n",
    "parts_indexs = []\n",
    "for part in parts:\n",
    "    matched = [match.start() for match in re.finditer(part, ori_text)]\n",
    "    for match in matched:\n",
    "        parts_indexs.append((part, match))\n",
    "    print(part, matched)\n",
    "    \n",
    "parts_indexs = sorted(parts_indexs, key=lambda x: x[1])\n",
    "print(parts_indexs)\n",
    "\n",
    "toc_dict = {}\n",
    "for pdata1, pdata2 in zip(parts_indexs, parts_indexs[1:] + [('', len(ori_text)-1)]):\n",
    "    name, left = pdata1\n",
    "    left += len(name)\n",
    "    _, right = pdata2\n",
    "    \n",
    "    if right - left > MIN_SPLIT_LEN:\n",
    "        if name not in toc_dict:\n",
    "            toc_dict[name] = [ori_text[left:right]]\n",
    "        else:\n",
    "            toc_dict[name].append(ori_text[left:right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "deaa40a5-d0d3-4ed0-9a04-fc669eb9970d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.5 ms, sys: 14.2 ms, total: 60.6 ms\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Summary ToC-Contents Groups\n",
    "\"\"\"\n",
    "\n",
    "MIN_CONTENT_LEN = 10\n",
    "\n",
    "api_server_url = \"http://localhost:21122\"\n",
    "# I want you to act as a summaryist. I will give you sentences. \"\n",
    "# f\"I want you to only reply with a summarization based on the sentences. do not write explanations.\n",
    "# \"do not write explanations. summarize the sentences as briefly as possible: {query}\"\n",
    "instruct = (\n",
    "    \"do not write explanations. 다음 문장을 한글로 번역하고 최대한 짧게 요약해주세요: {query}\"\n",
    ")\n",
    "prompt_template = \"\"\"### System:\n",
    "This is a system prompt, please behave and help the user.\n",
    "\n",
    "### User: {instruct}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "def send_summary(summaries):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_data - 1:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = toc_dict.get(parts[pidx])\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_data}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        if data is None:\n",
    "            continue\n",
    "        data = '\\n'.join(data)\n",
    "        if len(data) < MIN_CONTENT_LEN:\n",
    "            # summaries[pidx] = data\n",
    "            continue\n",
    "        \n",
    "        # response\n",
    "        prompt = prompt_template.format(\n",
    "            instruct=instruct.format(query=data),\n",
    "        )\n",
    "        input_json = {\n",
    "            \"model_name\": \"OLLM-Large_2023.11.20\",# \"MDIEM-toc3\",\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_p\": 0.7,\n",
    "            \"max_new_tokens\": 1024,\n",
    "        }\n",
    "\n",
    "        ret = requests.post(\n",
    "            api_server_url + \"/worker_generate\",\n",
    "            json=input_json,\n",
    "            timeout=300,\n",
    "        )\n",
    "\n",
    "        result = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "        summaries[pidx] = result\n",
    "\n",
    "summaries = [\"\"] * len(parts)\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_data = len(parts)\n",
    "n_thread = 16\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_summary, args=(summaries,)) # \n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "2613999d-0839-4a1c-a50d-c0f5260f8b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction \n",
      " 이 논문에서는 Mixtral 8x7B라는 오픈 가중치가 있는 스페어스 믹스추러 모델(SMoE)을 소개합니다. 이 모델은 Apache 2.0 라이선스로 라디오 2 70B와 GPT-3.5를 대부분의 벤치마크에서 능가합니다. 토큰당 하위 집합을 사용하여 추론 속도를 낮은 배치 크기에서 빠르게 하고 대용량 배치 크기에서 처리량을 높입니다. Mixtral은 또한 32K 토큰의 맥락 크기를 사용하여 다국어 데이터로 사전 훈련되었습니다. 이 모델은 수학, 코드 생성 및 다국어 이해가 필요한 작업에서 Llama 2 70B를 크게 앞섭니다. 또한 Mixtral 8x7B-Instruct라는 명령어를 따르는 대화형 모델도 소개되며, 이 모델은 인간 평가 벤치마크에서 GPT-3.5 Turbo, Claude-2.1, Gemini Pro, Llama 2 70B-chat 모델을 크게 앞섭니다. Mixtral 8x7B와 Mixtral 8x7B-Instruct는 모두 Apache 2.0 라이선스로 학술 및 상업 목적으로 무료로 공개되어 다양한 용도로 사용할 수 있습니다. \n",
      "\n",
      "2 Architectural details \n",
      " \n",
      "번역결과번역결과 \n",
      "매개변수 값\n",
      "출력 Unit 4096\n",
      "층 수 32\n",
      "헤드 출력 크기 128\n",
      "은닉 출력 크기 14336\n",
      "헤드 수 32\n",
      "키-관련 헤드 수 8\n",
      "컨텍스트 길이 32768\n",
      "사전 크기 32000\n",
      "전문가 수 8\n",
      "상위 k 전문가 2\n",
      "표 1: 모델 아키텍처.\n",
      "Mixtral은 변환기 아키텍처[31]을 기반으로하고 동일한 수정 사항을 사용합니다[18], 완전한 밀접한 컨텍스트 길이는 32k 토큰으로 지원되며, 피드 포워드 블록이 전문가 층(섹션 2.1)으로 대체됩니다.\n",
      "표 1에서 모델 아키텍처 매개 변수를 요약합니다.\n",
      "설명이 없습니다. \n",
      "\n",
      "2.1 Sparse Mixture of Experts \n",
      " 다음 문장은 Mixture of Experts 레이어(Figure 1)에 대한 간략한 개요를 제공합니다. 자세한 내용은 [12]를 참조하세요. 이 모듈은 게이트 네트워크의 출력에 의해 가중치가 부여된 전문가 네트워크의 출력의 가중 합으로 입력 x에 대한 출력을 결정합니다. 게이팅 벡터가 스팀스하다면 게이트가 0인 전문가의 출력을 계산할 필요가 없습니다. G(x)는 선형 층의 출력을 소프트맥스로 취하는 것으로 구현됩니다. 여러 가지 대안적인 방법이 있지만, 간단하고 효율적인 방법은 소프트맥스(Top-K 로깅 로지트)를 사용하는 것입니다. 여기서 K는 토큰당 사용되는 전문가 수의 하이퍼파라미터입니다. 이 값을 고정하고 n을 증가시키면 토큰당 사용되는 파라미터 수를 증가시킬 수 있습니다. MoE 층은 GPU에서 효율적으로 실행될 수 있으며, 토큰을 특정 전문가에게 할당할 수 있는 전문가 병렬화 기법이 있습니다. 이 층은 독립적으로 적용되며, 각 토큰에 대해 실행됩니다. 출력 y는 입력 x에 대해 계산됩니다. \n",
      "\n",
      "3 Results \n",
      " 번역결과  \n",
      "다음 문장을 한글로 번역하고 최대한 짧게 요약합니다.  \n",
      "Figure 2: Mixtral과 다른 Llama 모델의 성능을 광범위한 벤치마크에서 비교합니다. 모든 모델은 정확한 비교를 위해 평가 파이프라인을 다시 평가합니다. Mixtral은 Llama 2 70B를 포함한 대부분의 벤치마크에서 우수한 성능을 보입니다. 특히 수학과 코드 생성 분야에서 매우 우수한 성능을 보입니다. Table 2: Mixtral과 Llama 간의 비교를 나타냅니다. Mixtral은 Llama 2 70B의 성능을 대부분의 인기있는 벤치마크에서 맞먹거나 능가하며 추가 파라미터 수가 5배 적습니다. Figure 3: MMLU, 커먼센스 이해, 세계 지식, 읽기 이해, 수학 및 코드 생성 범주에서 Mistral(7B/8x7B), Llama 2(7B/13B/70B) 및 Mixtral(8x7B)의 결과를 보여줍니다. Mixtral은 대부분의 벤치마크에서 Llama 2 70B의 성능을 크게 능가하며 코드 및 수학 분야에서 큰 우세를 나타냅니다. 추가 파라미터 수가 5배 적습니다. 테이블 2의 세부 결과는 Mixtral, Mistral 7B 및 Llama 2 7B/13B/70B 및 Llama 1 34B2에 대한 자세한 결과를 보고합니다. 그림 2에 따르면 Mixtral은 대부분의 카테고리에서 Llama 2 70B의 성능을 능가하거나 맞먹습니다. 특히 수학 및 코드 벤치마크에서 우수한 성능을 보입니다. 크기와 효율성. 비용-성능 스펙트럼에서 Mixtral 모델의 효율성을 이해하기 위해 Llama 2 가족을 비교합니다. 스파크 믹스 오브 엑스퍼트 모델로 작동하는 Mixtral은 활성 파라미터 수가 5배 적습니다. 5배  \n",
      "\n",
      "3.1 Multilingual benchmarks \n",
      " 비교적 미스트랄 7B보다 많은 양의 다국어 데이터를 사전 훈련 과정에 사용했다. 이로 인해 미크스트랄은 영어 정확도를 유지하면서 프랑스어, 독일어, 스페인어, 이탈리아어와 같은 다국어 벤치마크에서 우수한 성능을 보인다. 특히, 테이블 4에 따르면 미크스트랄은 라마 2 70B를 능가하며 프랑스어, 독일어, 스페인어, 이탈리아어에서 더 높은 성능을 보인다. \n",
      "\n",
      "3.2 Long range performance \n",
      " 다음 문장을 한글로 번역하고 요약하면 다음과 같습니다: \n",
      "Mixtral의 장시간 상태 대응 능력을 평가하기 위해, 우리는 임의로 삽입된 암호키를 검색하는 작업을 평가합니다. 이 작업은 [23]에 소개되었으며, 모델의 암호키 검색 능력을 측정하기 위해 합성된 작업입니다. 그림 4(왼쪽)에 따르면, Mixtral은 컨텍스트 길이나 암호키 삽입 위치에 관계없이 100% 재현 정확도를 달성합니다. 그림 4(오른쪽)에 따르면, proof-pile 데이터셋의 하위집합에서 Mixtral의 농담 값은 컨텍스트 길이가 증가함에 따라 계속 감소합니다. \n",
      "번역결과  \n",
      "Mixtral의 장시간 상태 대응 능력을 평가하기 위해, 우리는 임의로 삽입된 암호키를 검색하는 작업을 평가합니다. 이 작업은 [23]에 소개되었으며, 모델의 암호키 검색 능력을 측정하기 위해 합성된 작업입니다. 그림 4(왼쪽)에 따르면, Mixtral은 컨텍스트 길이나 암호키 삽입 위치에 관계없이 100% 재현 정확도를 달성합니다. 그림 4(오른쪽)에 따르면, proof-pile 데이터셋의 하위집합에서 Mixtral의 농담 값은 컨텍스트 길이가 증가함에 따라 계속 감소합니다. \n",
      "\n",
      "3.3 Bias Benchmarks \n",
      " 이 문장을 한글로 번역하고 최대한 짧게 요압해주세요: \n",
      "Llama 2 70B Mixtral 8x7B\n",
      "BBQ 정확도 51.5% 56.0%\n",
      "BOLD 감성점수 (평균 ± 표준편차)\n",
      "성별 0.293 ± 0.073 0.323 ±0.045\n",
      "직업 0.218 ± 0.073 0.243 ± 0.087\n",
      "종교적 이념 0.188 ± 0.133 0.144 ± 0.089\n",
      "정치적 이념 0.149 ± 0.140 0.186 ± 0.146\n",
      "인종 0.232 ± 0.049 0.232 ± 0.052\n",
      "Figure 5: 편향 평가. 비교 Llama 2와 Mixtral 2, Mixtral은 더 낮은 정확도(BBQ 평가에서 56.0% 대 51.5%), 더 높은 평균 감성 점수(BOLD에서 더 낮은 표준편차)를 보여줍니다. 이는 더 작은 편향을 나타내고 있습니다.\n",
      "또한, Mixtral은 비교적 Llama 2보다 더 긍정적인 감성을 보여줍니다, 각 그룹의 더불어 같은 분산은 각 그룹의 변동성을 나타냅니다. \n",
      "4. 미세 조정 및 선호도 모델링\n",
      "Mixtral-Instruct는 감독 학습(SFT)을 사용하여 지시사항 데이터셋에 대해 미세 조정되었고, Direct Preference Optimization(DPO)을 사용하여 쌍 피드백 피드백 데이터셋에 대해 미세 조정되었습니다. Mixtral-Instruct는 MT-Bench에서 8.30의 점수를 받았으며, 2023년 12월 기준으로 최고의 오픈소스 모델입니다. 또한, LMSys Leaderboard에 따르면 Mixtral-Instruct는 GPT-3.5-Turbo, Gemini Pro, Claude-2.1, Llama-2-70b-chat을 능가합니다. \n",
      "5. 경로 분석\n",
      "이 섹션에서는 라우터가 특정 도메인에서 특정 전문가를 선호하는 패턴을 보이는지 작은 규모의 분석을 수행합니다. 다양한 도메인(예: 수학, 생물학, 철학 등 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(parts)):\n",
    "    print(parts[idx], '\\n', summaries[idx], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05b6ea-2b4d-442b-887a-9fe080bbfed8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for part in parts:\n",
    "    if part in toc_dict:\n",
    "        print(part, '\\n', len(toc_dict[part]), '\\n', toc_dict[part], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "786e6603-2931-42fe-ac91-b28daf8d4987",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7777777777777777"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "640/360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "c8ea56a4-6f95-4655-aba5-1d67fdd1d4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.768421052631579"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2688/1520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "9c29cb70-5256-4ef0-bd3c-daeb36a7f678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "f0b60879-54f5-4c38-b505-31b6b64cf237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"GM2-RAMP2-ENT-2024-1-07--09-15-30-entrycar.jpg\")\n",
    "resized = cv2.resize(img, (640, 360))\n",
    "cv2.imwrite(\"GM2-RAMP2-ENT-2024-1-07--09-15-30-entrycar-resized.jpg\", resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "c52b2b2d-1f95-469c-a0ca-cbd9a0a44b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class A:\n",
    "    @classmethod\n",
    "    def abc(cls):\n",
    "        print(\"abc\", cls.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "a78d775c-4678-4380-bcaf-d5c7dc5b5ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'A' has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[917], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[915], line 4\u001b[0m, in \u001b[0;36mA.abc\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabc\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'A' has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "A.abc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "a1d1049e-1543-436c-ad99-3ee2307366c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "04d481ce-9e8c-4b0d-9bb9-6f18baf0fa5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "a.abc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d6214-efd7-42a0-baf0-a8d79b4e2ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
