{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dpo_dataset import load_dpo_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")\n",
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df680a11-11cc-42bd-b5ff-3f7b16dfa17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "946c5dcd-6009-44bf-aef2-dd75d1269bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(dpo_list5[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f39efe-d157-4651-a847-96b5cadf4460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a832dc69-c8bd-41cb-aca5-b4dfb33637ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo.append(random_select(dataset, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd27eb-d7fe-4dc5-93b8-25b6331f7ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset_list = []\n",
    "for d in dataset_list:\n",
    "    new_dataset_list.append(\"\\\"\" + d + \"\\\"\")\n",
    "print(' '.join(new_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a1012ac1-5dff-4f4b-aff1-cf554c3f7c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "59bdc57a-82c4-4281-bae3-dd7068914b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4efcee25-a99d-48b7-9347-a3ce950b71c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversations', 'task'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = dataset_list[14]\n",
    "print(dataset_path)\n",
    "dataset = load_sft_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7838928c-3b21-40e1-97fb-2804580bf1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft.append(random_select(dataset, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace9cb2f-8381-43e9-915d-b7a6e70d4879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fbfc93bc-62fb-451d-bc42-2269abbdf939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = concatenate_datasets(mdpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "33e33da3-44b5-4a25-a4a8-94795b04ba19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f144e79-4b3f-43e9-bb0a-27e5b75df8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee38e-de21-469d-9502-e8f12a1e4e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_sft_dataset(dataset_path) for dataset_path in dataset_list])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fb94dc55-df6b-4c12-a23d-5e2afc3b199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in combined_dataset:\n",
    "    new_dataset.append(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7827856c-b0da-4fe8-a0fb-a07b5dbd2a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_dataset.to_json(\"/data/llm_datasets/custom/ados_sft_v4.json\")\n",
    "with open(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0b710b03-1ad8-4cd9-b69e-f4fc8b37f8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_dataset = ados_DPODataset(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9a5f2a07-f51e-4e92-b648-f5f67457ab78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8719.97it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1349.52it/s]\n",
      "Generating train split: 11000 examples [00:01, 8384.35 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11000/11000 [00:01<00:00, 9377.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "get and save language distributions\n",
    "\"\"\"\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT raw data and change format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"make lang_dict(sft)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661bfc45-9396-4f6b-a6ff-12acc03a0b2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1016 examples [00:00, 21155.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make lang_dict(dpo)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    \n",
    "    langs = {}\n",
    "\n",
    "    lang_chosen = lang_detect.predict(chosen.replace('\\n', ' '))[0][0]\n",
    "    if lang_chosen not in langs:\n",
    "        langs[lang_chosen] = 1\n",
    "    else:\n",
    "        langs[lang_chosen] += 1\n",
    "        \n",
    "    lang_rejected = lang_detect.predict(rejected.replace('\\n', ' '))[0][0]\n",
    "    if lang_rejected not in langs:\n",
    "        langs[lang_rejected] = 1\n",
    "    else:\n",
    "        langs[lang_rejected] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6279a941-d152-4693-b5e6-44d63a86acbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 1016\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460862df-7bbf-4a40-982d-1f7782fd22ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d1d4-7a7c-45c2-94c8-5f07c79ac036",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565040d8-08c4-4a16-bb43-e6952114b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dff716d9-b345-4f39-9380-22bb3bbec251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 111092.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': \"To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we will substitute these values for the variables `x` and `y` in the given expression and simplify.\\n\\nSubstituting `x = 5` and `y = -2` into the expression, we get:\\n\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying, we have:\\n\\n2(25) + 6 + 7\\n\\nNow, let's perform the operations:\\n\\n50 + 6 + 7 = 63\\n\\nTherefore, the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` is 63.\",\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': 'To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we need to replace `x` with `5` and `y` with `-2` in the expression and simplify it. \\n\\nSubstituting `x = 5` and `y = -2`:\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying further:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\nTherefore, substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` gives us a result of 63.'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_distilabel-math-preference-dpo_1'\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e79701-693c-4413-8448-73fd1550de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46462/46462 [00:01<00:00, 40620.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rejected': '최선을 다하겠습니다.',\n",
       " 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d',\n",
       " 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.',\n",
       " 'input': '내 질문에 10단어 이내로 답변하세요.',\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d'\n",
    "filtered_row = temp_dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d440c-e3d6-4d2d-8172-846840c2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고, 빈 데이터 채운 뒤에 합치기, 저장, 이후 dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59121201-1890-4a24-949f-a9dd305d3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고\n",
    "with open(\"/data/llm_datasets/custom/kodpo/temp_en.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)\n",
    "    \n",
    "# temp_dataset = load_dataset(\"json\", data_files=)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd8fee2c-0553-4160-a60d-5345c7750b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5336.26it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 890.70it/s]\n",
      "Generating train split: 1016 examples [00:00, 11191.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/temp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7485e6e-38ac-49d6-9ac4-030a5d6ca67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 994/994 [00:28<00:00, 35.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/994 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# temp_dataset = dedup_repetition(temp_dataset, 'dpo')\n",
    "# temp_dataset = dedup_short(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_too_much_token(temp_dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e62a62cc-cd85-43f1-939b-c3358bae6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "655d38b0-e098-4960-b08d-d5e2dc9f0b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1016/1016 [00:00<00:00, 43887.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 데이터 채운 뒤에 합치기\n",
    "# dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\")\n",
    "selected_id = temp_dataset['id']\n",
    "\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] not in selected_id)\n",
    "\n",
    "new_dataset = []\n",
    "for data in temp_dataset:\n",
    "    new_dataset.append(data)\n",
    "\n",
    "for data in filtered_row:\n",
    "    new_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be6eee56-4869-42fa-886f-cb1f3a2c2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_dataset), len(filtered_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbe021dc-5102-4f9f-b2d1-3e94744a06f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 1013)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd81c44d-dff5-4892-98e9-c249a6c7eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are an unbiased, uncensored, helpful assistant.',\n",
       " 'chosen': 'The two subjects in the American Gothic painting are a father and adult daughter. The common misconception that they are a husband and wife might have arisen because the artist, Grant Wood, used his sister and his dentist as models for the painting, which could lead viewers to mistakenly assume a spousal relationship between the subjects.',\n",
       " 'task': 'dpo_system',\n",
       " 'id': 'truthy_dpo_2e1d77fbebc31dde0bb2f9303d6761e2',\n",
       " 'input': 'Who are the two subjects in the American Gothic painting - a husband and wife, or a father and daughter?',\n",
       " 'rejected': 'A father and daughter'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcbeea78-756b-42dc-a121-71f78bd17fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "with open(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aef5be7c-1930-417c-aa93-454202e9ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5242.88it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1013 examples [00:00, 13933.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'chosen', 'task', 'id', 'input', 'rejected'],\n",
       "    num_rows: 1013\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abff4343-5361-484e-b258-3718510a5fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61964/61964 [00:04<00:00, 14176.52 examples/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac8631e-eed8-4866-b245-8ac6fd8249ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49f8a37a-a4ef-4fdb-9400-66492f99a20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=\"이 과제는 벵골어로 된 문장을 영어로 번역하는 것입니다.\\nQ: অথবা আপনি হয়ত বলবেন, 'না, আমার খেয়াল হয়েছে যে আমি আসলেই মহিলাদ\", n_results=min(n_results, len(query_ids)), include=['distances', 'documents'])\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafa2d3-234d-4bea-9429-464bb1c60e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb93f6b-8c02-4bd5-9698-c6207428160f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = dpo_list3[2]\n",
    "print(dataset_path)\n",
    "dataset = load_dpo_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb420-0603-421d-9b30-a446668597b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = dedup_non_pair(dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b204ad5-421e-413f-98a9-8089d91b09a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import ados_DPODataset\n",
    "\n",
    "dpo_dataset = ados_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822481ba-65ba-41a6-bb2f-7efbacd2600a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_datamodule['train_dataset'][-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a00a9a7f-e892-4673-8706-775cce38d063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combined_dataset[70002]\n",
    "\n",
    "conv = get_conversation_template('chat-orca')\n",
    "conv.system_message = conv.tasks['system_instruct'].format(system=data['system'])\n",
    "conv.append_message(conv.roles[0], data['input'])\n",
    "conv.append_message(conv.roles[1], '')\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "conv.update_last_message(data['chosen'])\n",
    "chosen = conv.get_prompt()[len(prompt):]\n",
    "conv.update_last_message(data['rejected'])\n",
    "rejected = conv.get_prompt()[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3496a3c6-4cbd-4936-887e-cc32e8addd59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/Platypus2-70B-instruct/\"\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/vicuna-13b-v1.5/\"\n",
    "model_path = \"/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_dpo_ep2/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    model_max_length=16384,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "import torch\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33725fff-f757-4fdd-9ac5-cc834621e7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24094/24094 [00:01<00:00, 18172.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/24094 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\"\n",
    "#\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\"\n",
    "raw_data = load_sft_dataset(\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\")\n",
    "raw_data = dedup_non_pair(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f84c75e-6798-4007-9b06-1bdc0af9534a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.train import LazySupervisedDataset\n",
    "dataset = LazySupervisedDataset(raw_data, tokenizer, data_format=\"chat-orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "65d092c6-97ac-4c22-8308-9ed65733673d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len = 16384\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "len_concat = 0\n",
    "concat_input_ids = []\n",
    "concat_labels = []\n",
    "for data in dataset:\n",
    "    input_ids = data['input_ids']\n",
    "    labels = data['labels']\n",
    "    attention_mask = data['attention_mask']\n",
    "\n",
    "    full_input_ids = input_ids[attention_mask==True]\n",
    "    full_labels = labels[attention_mask==True]\n",
    "\n",
    "    len_concat += len(full_input_ids)\n",
    "    if len_concat > max_len:\n",
    "        new_input_ids = torch.cat(concat_input_ids)\n",
    "        new_labels = torch.cat(concat_labels)\n",
    "        assert len(new_input_ids) == len(new_labels)\n",
    "        new_attention_mask = torch.ones(len(new_input_ids), dtype=torch.bool)\n",
    "        \n",
    "        new_input_ids = torch.cat((new_input_ids, torch.zeros(max_len - len(new_input_ids), dtype=input_ids.dtype)))\n",
    "        new_labels = torch.cat((new_labels, torch.full((max_len - len(new_labels),), IGNORE_TOKEN_ID, dtype=labels.dtype)))\n",
    "        new_attention_mask = torch.cat((new_attention_mask, torch.zeros(max_len - len(new_attention_mask), dtype=torch.bool)))\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'input_ids': new_input_ids.tolist(),\n",
    "            'labels': new_labels.tolist(),\n",
    "            'attention_mask': new_attention_mask.tolist(),\n",
    "        })\n",
    "        \n",
    "        len_concat = len(full_input_ids)\n",
    "        concat_input_ids = []\n",
    "        concat_labels = []\n",
    "        \n",
    "    concat_input_ids.append(full_input_ids)\n",
    "    concat_labels.append(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6cc203f-5d05-4dc5-9eaa-9cc049a0f8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3758"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e3cf2a30-344f-4130-b4ac-59fd7311dcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c78ef-f0a7-494a-a7e7-c2094b40ce4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28991f14-556a-49e1-9c79-e171fe698fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.where(data['labels'] == IGNORE_TOKEN_ID, tokenizer.unk_token_id, data['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3cf4aef0-1c2c-4092-bd3c-d3998d574a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6be05e16-46c9-450e-a74b-5d2cd62cfd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 95520.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))\n",
    "dataset = load_dpo_dataset(\"/data/llm_datasets/yarn-train-tokenized-16k-mistral/data/\")\n",
    "dataset = random_select(dataset, 200)\n",
    "new_dataset = []\n",
    "for _data in dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8750b001-b3cf-42ac-a113-5be3df438b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json\")\n",
    "dataset2 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-gpt4_evol.json\")\n",
    "dataset3 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json\")\n",
    "dataset3 = random_select(dataset3, 1000 - len(dataset1) - len(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce38e5c9-9403-4457-a57d-b65219a7e7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 699/699 [01:09<00:00, 10.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Features, Sequence, Value\n",
    "\n",
    "dataset2 = dataset2.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")\n",
    "dataset3 = dataset3.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b2e13f4-6e09-47ac-83c0-59b2cde35351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = concatenate_datasets((dataset1, dataset2, dataset3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31316e3d-198c-453b-8b80-4518365db4af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = conbined_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bf20792c-0630-4fd4-b612-e85edc12c27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for _data in conbined_dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-v1.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48627ebe-cf4c-43c1-80c2-a989e65ba561",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(data['input_ids'])\n",
    "sentences = text.split('.')\n",
    "\n",
    "len_split = 1024\n",
    "splits = []\n",
    "split = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(split) < len_split:\n",
    "        split += sentence + \".\"\n",
    "    else:\n",
    "        splits.append(split)\n",
    "        split = \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
