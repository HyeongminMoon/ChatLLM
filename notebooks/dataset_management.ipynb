{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "# lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dpo_dataset import load_dpo_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")\n",
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "get and save language distributions\n",
    "\"\"\"\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT raw data and change format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"make lang_dict(sft)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661bfc45-9396-4f6b-a6ff-12acc03a0b2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1016 examples [00:00, 21155.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make lang_dict(dpo)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    \n",
    "    langs = {}\n",
    "\n",
    "    lang_chosen = lang_detect.predict(chosen.replace('\\n', ' '))[0][0]\n",
    "    if lang_chosen not in langs:\n",
    "        langs[lang_chosen] = 1\n",
    "    else:\n",
    "        langs[lang_chosen] += 1\n",
    "        \n",
    "    lang_rejected = lang_detect.predict(rejected.replace('\\n', ' '))[0][0]\n",
    "    if lang_rejected not in langs:\n",
    "        langs[lang_rejected] = 1\n",
    "    else:\n",
    "        langs[lang_rejected] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6279a941-d152-4693-b5e6-44d63a86acbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 1016\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460862df-7bbf-4a40-982d-1f7782fd22ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d1d4-7a7c-45c2-94c8-5f07c79ac036",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565040d8-08c4-4a16-bb43-e6952114b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dff716d9-b345-4f39-9380-22bb3bbec251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 111092.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': \"To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we will substitute these values for the variables `x` and `y` in the given expression and simplify.\\n\\nSubstituting `x = 5` and `y = -2` into the expression, we get:\\n\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying, we have:\\n\\n2(25) + 6 + 7\\n\\nNow, let's perform the operations:\\n\\n50 + 6 + 7 = 63\\n\\nTherefore, the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` is 63.\",\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': 'To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we need to replace `x` with `5` and `y` with `-2` in the expression and simplify it. \\n\\nSubstituting `x = 5` and `y = -2`:\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying further:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\nTherefore, substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` gives us a result of 63.'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_distilabel-math-preference-dpo_1'\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e79701-693c-4413-8448-73fd1550de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46462/46462 [00:01<00:00, 40620.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rejected': '최선을 다하겠습니다.',\n",
       " 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d',\n",
       " 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.',\n",
       " 'input': '내 질문에 10단어 이내로 답변하세요.',\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d'\n",
    "filtered_row = temp_dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d440c-e3d6-4d2d-8172-846840c2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고, 빈 데이터 채운 뒤에 합치기, 저장, 이후 dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59121201-1890-4a24-949f-a9dd305d3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고\n",
    "with open(\"/data/llm_datasets/custom/kodpo/temp_en.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)\n",
    "    \n",
    "# temp_dataset = load_dataset(\"json\", data_files=)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd8fee2c-0553-4160-a60d-5345c7750b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5336.26it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 890.70it/s]\n",
      "Generating train split: 1016 examples [00:00, 11191.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/temp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7485e6e-38ac-49d6-9ac4-030a5d6ca67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 994/994 [00:28<00:00, 35.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/994 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# temp_dataset = dedup_repetition(temp_dataset, 'dpo')\n",
    "# temp_dataset = dedup_short(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_too_much_token(temp_dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e62a62cc-cd85-43f1-939b-c3358bae6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "655d38b0-e098-4960-b08d-d5e2dc9f0b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1016/1016 [00:00<00:00, 43887.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 데이터 채운 뒤에 합치기\n",
    "# dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\")\n",
    "selected_id = temp_dataset['id']\n",
    "\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] not in selected_id)\n",
    "\n",
    "new_dataset = []\n",
    "for data in temp_dataset:\n",
    "    new_dataset.append(data)\n",
    "\n",
    "for data in filtered_row:\n",
    "    new_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be6eee56-4869-42fa-886f-cb1f3a2c2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_dataset), len(filtered_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbe021dc-5102-4f9f-b2d1-3e94744a06f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 1013)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd81c44d-dff5-4892-98e9-c249a6c7eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are an unbiased, uncensored, helpful assistant.',\n",
       " 'chosen': 'The two subjects in the American Gothic painting are a father and adult daughter. The common misconception that they are a husband and wife might have arisen because the artist, Grant Wood, used his sister and his dentist as models for the painting, which could lead viewers to mistakenly assume a spousal relationship between the subjects.',\n",
       " 'task': 'dpo_system',\n",
       " 'id': 'truthy_dpo_2e1d77fbebc31dde0bb2f9303d6761e2',\n",
       " 'input': 'Who are the two subjects in the American Gothic painting - a husband and wife, or a father and daughter?',\n",
       " 'rejected': 'A father and daughter'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcbeea78-756b-42dc-a121-71f78bd17fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "with open(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aef5be7c-1930-417c-aa93-454202e9ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5242.88it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1013 examples [00:00, 13933.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'chosen', 'task', 'id', 'input', 'rejected'],\n",
       "    num_rows: 1013\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49f8a37a-a4ef-4fdb-9400-66492f99a20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=\"이 과제는 벵골어로 된 문장을 영어로 번역하는 것입니다.\\nQ: অথবা আপনি হয়ত বলবেন, 'না, আমার খেয়াল হয়েছে যে আমি আসলেই মহিলাদ\", n_results=min(n_results, len(query_ids)), include=['distances', 'documents'])\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafa2d3-234d-4bea-9429-464bb1c60e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb420-0603-421d-9b30-a446668597b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = dedup_non_pair(dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ba930f4-fe5f-48b5-9e2c-5763366da940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import ados_DPODataset\n",
    "\n",
    "dpo_dataset = ados_DPODataset(\n",
    "    dataset_path=\"/data/llm_datasets/custom/ados/dpo/ados_dpo_v2.json\",\n",
    "    data_format='qwen',\n",
    ")\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49ec8e74-f6b1-437d-8285-b620ea30fcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_raw_dataset(dataset_path, split='train'):\n",
    "    if dataset_path.endswith(\"json\") or dataset_path.endswith(\"json.kr\"):\n",
    "        try:\n",
    "            dataset = load_dataset(\"json\", data_files=dataset_path, split=split)\n",
    "        except:\n",
    "            print(\"Not able to read records in the JSON file, read from json reader...\")\n",
    "            dataset = load_json_dataset(dataset_path)\n",
    "    elif dataset_path.endswith(\"parquet\"):\n",
    "        dataset = load_dataset(\"parquet\", data_files=dataset_path, split=split)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_path, split=split)\n",
    "    return dataset\n",
    "\n",
    "import json\n",
    "def load_json_dataset(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Union, List\n",
    "def dataset_to_json(dataset: Union[List[dict], Dataset], path: str):\n",
    "    if isinstance(dataset, Dataset):\n",
    "        dataset = [data for data in dataset]\n",
    "    \n",
    "    if os.path.dirname(path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding='utf-8') as json_file:\n",
    "        json.dump(dataset, json_file, indent=4, ensure_ascii=True)\n",
    "        \n",
    "    print(f\"saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a6bdc077-bf66-42ec-b97c-98074315fd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_raw_dataset(\"/data/llm_datasets/sftv5/processed/AkwardCorrection/mr-tydi-corpus_AkwardCorrectionData.json\")\n",
    "new_dataset = []\n",
    "for idx, data in enumerate(raw_dataset):\n",
    "    lists = data['conversation'][1]['value']\n",
    "    answer = data['conversation'][3]['value']\n",
    "    \n",
    "    new_dataset.append({\n",
    "        'id': f\"mr_tydi_akward_correction_{idx}\",\n",
    "        'task': 'Akward Correction',\n",
    "        'conversation': [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": f\"주어진 문장 리스트에서 문맥 상 어울리지 않는 문장을 찾으세요.\\n#문장 리스트: {lists}\",\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\":  f\"주어진 문장 리스트에서 문맥상 어울리지 않는 문장은 다음과 같습니다.\\n{answer}\\n\\n나머지 문장들은 모두 이와는 전혀 관련이 없는 문장입니다.\",\n",
    "            },\n",
    "        ]\n",
    "    })\n",
    "dataset_to_json(new_dataset, \"/data/llm_datasets/sftv5/processed/AkwardCorrection/refined/mr_tydi_akward_correction.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "74700caf-5276-4a5d-9f9c-8fc520dd4e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_raw_dataset(\"/data/llm_datasets/sftv5/processed/SummaryInference/TechnicalSciencesSummaryInferenceData.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2678160e-b613-4be7-aa5d-9329c298d1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /data/llm_datasets/sftv5/processed/SummaryInference/refined/aihub_tech_sci_summary_infer.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "new_dataset = []\n",
    "for idx, data in enumerate(raw_dataset):\n",
    "    question = data['conversation'][0]['value']\n",
    "    refer_lists = data['conversation'][2]['value']\n",
    "    answer = data['conversation'][4]['value']\n",
    "    \n",
    "    template = random.choice([\n",
    "        \"당신에게 주어진 이 문장들은 어떤 논문의 요약으로 이로부터 논문의 본문을 작성해야합니다. 주어진 요약문장과 참조할 문장들을 토대로 논문을 작성하세요.\\n#논문 요약: {summary}\\n#참조할 문장들: {refer}\",\n",
    "        \"이 문제에서는 어떤 논문의 요약본이 주어집니다. 참조 문장을 참고하여 요약본을 구체화하여 논문의 본문을 작성하세요.\\n#요약본: {summary}\\n#참조 문장: {refer}\"\n",
    "    ])\n",
    "    \n",
    "    new_dataset.append({\n",
    "        'id': f\"aihub_tech_sci_summary_infer_{idx}\",\n",
    "        'task': 'Summary Inference',\n",
    "        'conversation': [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": template.format(summary=question, refer=refer_lists),\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\":  answer,\n",
    "            },\n",
    "        ]\n",
    "    })\n",
    "dataset_to_json(new_dataset, \"/data/llm_datasets/sftv5/processed/SummaryInference/refined/aihub_tech_sci_summary_infer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ee5c18-9826-4507-9c20-1dcc910dbc36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "sftv5_list = glob.glob(\"/data/llm_datasets/sftv5/processed/**/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b554b46-b581-415a-bdab-3ad1919fc6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/llm_datasets/sftv5/processed/SpellingCorrection/aihub_interface_spelling_correction.json',\n",
       " '/data/llm_datasets/sftv5/processed/SpellingCorrection/korea_spelling_correction.json',\n",
       " '/data/llm_datasets/sftv5/processed/MiddleWordInference/kocommercial_mid_word_infer.json',\n",
       " '/data/llm_datasets/sftv5/processed/SummaryInference/aihub_tech_sci_summary_infer.json',\n",
       " '/data/llm_datasets/sftv5/processed/AkwardCorrection/mr_tydi_akward_correction.json',\n",
       " '/data/llm_datasets/sftv5/processed/MiddleKeywordInference/wiki_qa_mid_key_infer.json',\n",
       " '/data/llm_datasets/sftv5/processed/RearInference/kocommercial_rear_infer.json',\n",
       " '/data/llm_datasets/sftv5/processed/RearrangeSentence/kocommercial_rearrange_sen.json',\n",
       " '/data/llm_datasets/sftv5/processed/RearrangeWord/kocommercial_rearrange.json',\n",
       " '/data/llm_datasets/sftv5/processed/FrontInference/wiki_qa_front_infer.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftv5_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d307bf47-353f-4c21-a14b-1c3a54d956ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n",
      "0/102300 deduped\n",
      "dedup_repetition\n",
      "16/102300 deduped\n",
      "dedup_math\n",
      "0/102284 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102284/102284 [00:02<00:00, 37766.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/102284 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102284/102284 [00:02<00:00, 41002.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/102284 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/aihub_interface_spelling_correction.json\n",
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59989/59989 [00:01<00:00, 57532.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/59989 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59989/59989 [00:01<00:00, 30975.95 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/59989 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59913/59913 [00:01<00:00, 42556.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/59913 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59913/59913 [00:01<00:00, 40730.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/59913 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59911/59911 [00:01<00:00, 42951.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/59911 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/korea_spelling_correction.json\n",
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 237605/237605 [00:04<00:00, 51466.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/237605 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 237605/237605 [00:33<00:00, 7103.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/237605 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 237520/237520 [00:06<00:00, 34961.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/237520 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 237519/237519 [01:12<00:00, 3263.55 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/237519 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 237332/237332 [00:06<00:00, 35509.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237292/237332 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/kocommercial_mid_word_infer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 63830 examples [00:04, 14662.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63830/63830 [00:01<00:00, 35847.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/63830 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63830/63830 [00:58<00:00, 1087.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998/63830 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58832/58832 [00:02<00:00, 27020.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52941/58832 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5891/5891 [02:04<00:00, 47.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/5891 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5731/5731 [00:00<00:00, 23537.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5731 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/aihub_tech_sci_summary_infer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 146814 examples [00:08, 16996.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146814/146814 [00:03<00:00, 39011.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/146814 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146814/146814 [01:28<00:00, 1658.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1177/146814 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 145637/145637 [00:04<00:00, 29408.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19283/145637 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 126354/126354 [07:33<00:00, 278.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6611/126354 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119743/119743 [00:03<00:00, 30786.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/119743 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/mr_tydi_akward_correction.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 68753 examples [00:01, 39841.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68753/68753 [00:01<00:00, 50406.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/68753 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68753/68753 [00:15<00:00, 4574.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/68753 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68484/68484 [00:01<00:00, 37314.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4599/68484 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63885/63885 [00:01<00:00, 37743.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/63885 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63885/63885 [00:01<00:00, 38317.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/63885 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/wiki_qa_mid_key_infer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 135494 examples [00:05, 25135.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135494/135494 [00:03<00:00, 44151.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/135494 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135494/135494 [00:44<00:00, 3056.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/135494 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135321/135321 [00:04<00:00, 31355.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/135321 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135321/135321 [02:19<00:00, 972.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/135321 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134959/134959 [00:04<00:00, 31184.76 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/134959 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/kocommercial_rear_infer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 134627 examples [00:07, 17366.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134627/134627 [00:03<00:00, 34950.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/134627 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134627/134627 [01:25<00:00, 1578.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/134627 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134457/134457 [00:04<00:00, 26903.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893/134457 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 133564/133564 [13:55<00:00, 159.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6883/133564 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 126681/126681 [00:04<00:00, 27207.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/126681 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/kocommercial_rearrange_sen.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 255420 examples [00:05, 42652.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255420/255420 [00:04<00:00, 51422.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/255420 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255420/255420 [00:33<00:00, 7569.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/255420 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255357/255357 [00:07<00:00, 34673.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/255357 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255109/255109 [04:10<00:00, 1019.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305/255109 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 253804/253804 [00:07<00:00, 35560.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/253804 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/kocommercial_rearrange.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 68445 examples [00:02, 31251.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68445/68445 [00:01<00:00, 46042.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/68445 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68445/68445 [00:21<00:00, 3244.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/68445 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68184/68184 [00:02<00:00, 33106.37 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/68184 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68183/68183 [00:39<00:00, 1708.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/68183 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68173/68173 [00:02<00:00, 33393.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/68173 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for path in sftv5_list:\n",
    "    dst = os.path.join(\"/data/llm_datasets/sftv5/deduped/\", os.path.basename(path))\n",
    "    dataset = load_raw_dataset(path)\n",
    "    if 'conversation' in dataset.features:\n",
    "        dataset = dataset.rename_column('conversation', 'conversations')\n",
    "    \n",
    "    # apply dedups\n",
    "    print(\"dedup_non_pair\")\n",
    "    dataset = dedup_non_pair(dataset)\n",
    "    print(\"dedup_repetition\")\n",
    "    dataset = dedup_repetition(dataset)\n",
    "    print(\"dedup_math\")\n",
    "    dataset = dedup_math(dataset)\n",
    "    print(\"dedup_too_much_token\")\n",
    "    dataset = dedup_too_much_token(dataset)\n",
    "    print(\"dedup_short\")\n",
    "    dataset = dedup_short(dataset)\n",
    "    # do similarity dedup if needed\n",
    "    # do code refine if needed\n",
    "    \n",
    "    dataset_to_json(dataset, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc56f6-af26-4f33-87df-f6f233b10f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 237605 examples [00:04, 49116.54 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation', 'task', 'id', 'system'],\n",
       "    num_rows: 237605\n",
       "})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_raw_dataset(sftv5_list[2])\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db516b2f-0d9a-4fe4-ad74-03ac31ea6ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n",
      "0/237605 deduped\n",
      "dedup_repetition\n",
      "85/237605 deduped\n",
      "dedup_math\n",
      "1/237520 deduped\n",
      "dedup_too_much_token\n",
      "187/237519 deduped\n",
      "saved at /data/llm_datasets/sftv5/deduped/kocommercial_mid_word_infer.json\n"
     ]
    }
   ],
   "source": [
    "# kocommercial_mid_word_infer.json short제외\n",
    "path = '/data/llm_datasets/sftv5/processed/MiddleWordInference/kocommercial_mid_word_infer.json'\n",
    "dst = os.path.join(\"/data/llm_datasets/sftv5/deduped/\", os.path.basename(path))\n",
    "dataset = load_raw_dataset(path)\n",
    "if 'conversation' in dataset.features:\n",
    "    dataset = dataset.rename_column('conversation', 'conversations')\n",
    "\n",
    "# apply dedups\n",
    "print(\"dedup_non_pair\")\n",
    "dataset = dedup_non_pair(dataset)\n",
    "print(\"dedup_repetition\")\n",
    "dataset = dedup_repetition(dataset)\n",
    "print(\"dedup_math\")\n",
    "dataset = dedup_math(dataset)\n",
    "print(\"dedup_too_much_token\")\n",
    "dataset = dedup_too_much_token(dataset)\n",
    "# print(\"dedup_short\")\n",
    "# dataset = dedup_short(dataset)\n",
    "# do similarity dedup if needed\n",
    "# do code refine if needed\n",
    "\n",
    "dataset_to_json(dataset, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ffd00-8cac-47d1-ab3a-f8a109cfe104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for path in sftv5_list:\n",
    "    dst = os.path.join(\"/data/llm_datasets/sftv5/deduped/\", os.path.basename(path))\n",
    "    dataset = load_raw_dataset(dst)\n",
    "    \n",
    "    print(dst, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157fecd-56f8-4657-ae17-e3ca2ffc8071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refine\n",
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json\",#\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",#\n",
    "    \"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/refined/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "# correction_list = [\n",
    "#     \"/data/llm_datasets/custom/deduped2/KoreaSpellingCorrection-10000.json\",\n",
    "# ]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_tech_dedup-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_book-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_law-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/naver-news-summarization-ko-vicuna_dedup-5000.json\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + summary_list + translation_list\n",
    "\n",
    "import os\n",
    "for path in dataset_list:\n",
    "    dataset = load_raw_dataset(path)\n",
    "    \n",
    "    print(path, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2faedae1-a177-454e-a462-42ef36d52003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_raw_dataset(\"/data/llm_datasets/aihub_summary_data/도서/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a3283d-a994-4adf-9bbf-6b925b7f50fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniform_select(dataset, division_factor):\n",
    "    return dataset.select(list(range(0, len(dataset), division_factor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a601f5-ad7f-4246-8a49-7516ef78c9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/llm_datasets/sftv5/deduped/kocommercial_rear_infer.json',\n",
       " '/data/llm_datasets/sftv5/deduped/mr_tydi_akward_correction.json',\n",
       " '/data/llm_datasets/sftv5/deduped/aihub_interface_spelling_correction.json',\n",
       " '/data/llm_datasets/sftv5/deduped/korea_spelling_correction.json',\n",
       " '/data/llm_datasets/sftv5/deduped/wiki_qa_mid_key_infer.json',\n",
       " '/data/llm_datasets/sftv5/deduped/kocommercial_mid_word_infer.json',\n",
       " '/data/llm_datasets/sftv5/deduped/kocommercial_rearrange.json',\n",
       " '/data/llm_datasets/sftv5/deduped/kocommercial_rearrange_sen.json',\n",
       " '/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json',\n",
       " '/data/llm_datasets/sftv5/deduped/aihub_tech_sci_summary_infer.json']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"/data/llm_datasets/sftv5/deduped/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1dc743f-c7e9-4d81-a4b0-cd2522b677ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sftv5\n",
    "sftv5_list = [\n",
    "    '/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/wiki_qa_mid_key_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/aihub_tech_sci_summary_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/korea_spelling_correction.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/aihub_interface_spelling_correction.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d18e85f-b08d-4877-859a-aebf9ffe7ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /data/llm_datasets/sftv5/rebal/kocommercial_rear_infer.json\n",
      "saved at /data/llm_datasets/sftv5/rebal/kocommercial_mid_word_infer.json\n",
      "saved at /data/llm_datasets/sftv5/rebal/kocommercial_rearrange_sen.json\n",
      "saved at /data/llm_datasets/sftv5/rebal/kocommercial_rearrange.json\n",
      "saved at /data/llm_datasets/sftv5/rebal/mr_tydi_akward_correction.json\n"
     ]
    }
   ],
   "source": [
    "rebal_list = [\n",
    "    '/data/llm_datasets/sftv5/deduped/kocommercial_rear_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/kocommercial_mid_word_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/kocommercial_rearrange_sen.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/kocommercial_rearrange.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/mr_tydi_akward_correction.json',\n",
    "]\n",
    "rebal_factor = [2,4,2,5,2]\n",
    "\n",
    "for i in range(5):\n",
    "    src = rebal_list[i]\n",
    "    dataset = load_raw_dataset(src)\n",
    "    dataset = uniform_select(dataset, rebal_factor[i])\n",
    "    dataset_to_json(dataset, src.replace('deduped', 'rebal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "396cc699-aec3-4f06-b189-a585e19cae68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 67350 examples [00:02, 26472.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json Dataset({\n",
      "    features: ['task', 'system', 'conversations', 'id'],\n",
      "    num_rows: 67350\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 59333 examples [00:01, 53497.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json Dataset({\n",
      "    features: ['task', 'system', 'conversations', 'id'],\n",
      "    num_rows: 59333\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 63341 examples [00:03, 18816.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json Dataset({\n",
      "    features: ['task', 'system', 'conversations', 'id'],\n",
      "    num_rows: 63341\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 50759 examples [00:01, 48422.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json Dataset({\n",
      "    features: ['task', 'system', 'conversations', 'id'],\n",
      "    num_rows: 50759\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 59872 examples [00:02, 25484.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json Dataset({\n",
      "    features: ['task', 'conversations', 'id'],\n",
      "    num_rows: 59872\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sftv5_list = [\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rear_infer.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_mid_word_infer.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rearrange_sen.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rearrange.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/mr_tydi_akward_correction.json',\n",
    "]\n",
    "for path in sftv5_list:\n",
    "    dataset = load_raw_dataset(path)\n",
    "    print(path, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a89e3a14-454f-40f6-a25b-8562f1c343df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /data/llm_datasets/sftv5/rebal/aihub_book_summary.json\n",
      "saved at /data/llm_datasets/sftv5/rebal/aihub_law_summary.json\n",
      "saved at /data/llm_datasets/sftv5/rebal/naver-news-summarization.json\n"
     ]
    }
   ],
   "source": [
    "summary_list = [\n",
    "    \"/data/llm_datasets/aihub_summary_data/도서/train.json\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/법률/train.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/naver-news-summarization-ko-vicuna_dedup/train.json\"\n",
    "]\n",
    "rebal_factor = [16, 5, 5]\n",
    "\n",
    "for i in range(3):\n",
    "    path = summary_list[i]\n",
    "    if i == 0:\n",
    "        dst = \"/data/llm_datasets/sftv5/rebal/aihub_book_summary.json\"\n",
    "    elif i == 1:\n",
    "        dst = \"/data/llm_datasets/sftv5/rebal/aihub_law_summary.json\"\n",
    "    else:\n",
    "        dst = \"/data/llm_datasets/sftv5/rebal/naver-news-summarization.json\"\n",
    "    dataset = load_raw_dataset(path)\n",
    "    dataset = uniform_select(dataset, rebal_factor[i])\n",
    "    dataset_to_json(dataset, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8be05bc-5c74-4cb8-a5f0-3239d14ee83b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10001 examples [00:00, 40382.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/rebal/aihub_book_summary.json Dataset({\n",
      "    features: ['task', 'conversations', 'id'],\n",
      "    num_rows: 10001\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4866 examples [00:00, 28377.56 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/rebal/aihub_law_summary.json Dataset({\n",
      "    features: ['task', 'conversations', 'id'],\n",
      "    num_rows: 4866\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4357 examples [00:00, 22618.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/rebal/naver-news-summarization.json Dataset({\n",
      "    features: ['task', 'conversations', 'id'],\n",
      "    num_rows: 4357\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summary_list = [\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_book_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_law_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/naver-news-summarization.json\",\n",
    "]\n",
    "for path in summary_list:\n",
    "    dataset = load_raw_dataset(path)\n",
    "    print(path, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "733af047-b774-43d2-954c-ce5a0d718f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\", # daily\n",
    "\"/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json\", # daily\n",
    "\"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\", # daily\n",
    "\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\", # daily\n",
    "\n",
    "v4_list = [\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_book_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_law_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/naver-news-summarization.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/alpaca-gpt4-korean_dedup2.json\", \n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\", \n",
    "    \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "task_list = [\n",
    "    '/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/wiki_qa_mid_key_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/aihub_tech_sci_summary_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/korea_spelling_correction.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/aihub_interface_spelling_correction.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rear_infer.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_mid_word_infer.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rearrange_sen.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rearrange.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/mr_tydi_akward_correction.json',\n",
    "    \n",
    "]\n",
    "\n",
    "# random.shuffle(task_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "760d6760-a77b-409e-be7e-b0eb2297d308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/v4_repeat/aihub_book_summary.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/aihub_book_summary.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/aihub_law_summary.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/aihub_law_summary.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/naver-news-summarization.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/naver-news-summarization.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/koalpaca_v1.1-vicuna.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/koalpaca_v1.1-vicuna.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/alpaca-gpt4-korean_dedup2.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/alpaca-gpt4-korean_dedup2.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/korquad-chat-vicuna.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/korquad-chat-vicuna.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/wizardlm_orca_vicuna_dedup2.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/wizardlm_orca_vicuna_dedup2.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(enko)-10000.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(enko)-10000.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(koen)-10000.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(koen)-10000.json\n"
     ]
    }
   ],
   "source": [
    "# 이미 학습된 데이터는 10분의 1만 사용\n",
    "for i in range(len(v4_list)):\n",
    "    path = v4_list[i]\n",
    "    dst = os.path.join(\"/data/llm_datasets/sftv5/v4_repeat/\", os.path.basename(path))\n",
    "    print(dst)\n",
    "    dataset = load_raw_dataset(path)\n",
    "    dataset = uniform_select(dataset, 10)\n",
    "    dataset_to_json(dataset, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37054d79-74db-43bf-9d7f-681ac9b18f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v4_repeat_list = [\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/aihub_book_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/aihub_law_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/naver-news-summarization.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "v5_list = task_list + v4_repeat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b675558a-2fb6-49fe-9b10-1a7dfddbd20e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'Front Inference', 'id': 'wiki_qa_front_infer_0', 'system': None, 'conversations': [{'from': 'human', 'value': \"다음 텍스트에서 제공된 문맥을 정확히 이해하고, 앞에 올 문장을 자연스럽고 문맥에 맞게 작성하세요. 문장은 이후 내용과 논리적으로 연결되어야 합니다.\\n#텍스트: 이들 식물은 사과와 같은 향이 나기 때문에 '캐모마일'이라고 불립니다. 캐모마일은 노란색 꽃을 가진 식물이며, 6월 중순에서 7월 하순까지 꽃을 피워요. 추위에 강한 편이며, 양지에서 잘 자라요. 꽃은 낮에는 피고 밤에는 오므라져 있으며, 일주일 정도 꽃이 핀다고 합니다. 씨받이는 매우 쉬우며, 싹도 잘 나기 때문에 직접 씨를 받을 수 있습니다. 캐모마일은 양지 바르고 배수가 좋은 사질토에서 잘 피워요. 수확한 꽃은 통풍이 잘 되는 응달에서 말려 저장해요. 캐모마일의 방향유는 긴장 완화, 두통 등의 통증 완화에 효과가 있으며, 몸을 따뜻하게 해주는 효과도 있습니다. 차로 마시면 위장장애의 완화에 도움을 줄 수 있습니다. 캐모마일은 카페인 성분이 없는 허브이며, 좀벌레를 없애는 데 효과적인 허브식물입니다.\"}, {'from': 'gpt', 'value': \"캐모마일은 국화과에 속하는 식물로, 저먼캐모마일, 캐모마일, 카미레 등이 포함됩니다. 이들 식물은 사과와 같은 향이 나기 때문에 '캐모마일'이라고 불립니다. 캐모마일은 노란색 꽃을 가진 식물이며, 6월 중순에서 7월 하순까지 꽃을 피워요. 추위에 강한 편이며, 양지에서 잘 자라요. 꽃은 낮에는 피고 밤에는 오므라져 있으며, 일주일 정도 꽃이 핀다고 합니다. 씨받이는 매우 쉬우며, 싹도 잘 나기 때문에 직접 씨를 받을 수 있습니다. 캐모마일은 양지 바르고 배수가 좋은 사질토에서 잘 피워요. 수확한 꽃은 통풍이 잘 되는 응달에서 말려 저장해요. 캐모마일의 방향유는 긴장 완화, 두통 등의 통증 완화에 효과가 있으며, 몸을 따뜻하게 해주는 효과도 있습니다. 차로 마시면 위장장애의 완화에 도움을 줄 수 있습니다. 캐모마일은 카페인 성분이 없는 허브이며, 좀벌레를 없애는 데 효과적인 허브식물입니다.\"}]}\n",
      "\n",
      "{'task': 'Middle Keyword Inference', 'id': 'wiki_qa_mid_key_infer_0', 'conversations': [{'from': 'human', 'value': \"이 문제에서는 주어진 텍스트 내의 <MASK>로 표시된 부분에 들어갈 적절한 문장을 예측해야 합니다. <MASK>가 위치한 문장의 전체 문맥 및 주어진 단어들을 분석하여, 문장의 나머지 내용과 일관되게 <MASK>에 들어갈 가장 적합한 문장을 답하세요.\\n#텍스트: 밀로시 벨코비치(1995년 9월 26일)는 세르비아의 축구 선수로, 현재 SV 베르더 브레멘에서 활약하고 있습니다. 그는 수비형 미드필더와 수비수 포지션에서 능숙하게 플레이합니다. 스위스 출신이지만 세르비아 부모님에게서 태어나 두 개의 국적을 가지고 있습니다. 처음에는 스위스 대표팀에서 활동했으나, 2012년 이후에는 세르비아 대표팀에서 경기를 진행하고 있습니다. <MASK> 또한, 바젤슈타트주 출신이며 미들즈브러 FC와 찰턴 애슬레틱 FC에도 소속되어 있습니다. 2018년 FIFA 월드컵에도 참가했습니다.\\n#단어: ['2015년 FIFA U-20 월드컵 우승', '세르비아 U-20 팀', '그의 참가']\"}, {'from': 'gpt', 'value': '그는 2015년 FIFA U-20 월드컵에서 세르비아 U-20 팀과 함께 우승을 차지했습니다.'}]}\n",
      "\n",
      "{'task': 'Summary Inference', 'id': 'aihub_tech_sci_summary_infer_14', 'conversations': [{'from': 'human', 'value': '이 문제에서는 어떤 논문의 요약본이 주어집니다. 참조 문장을 참고하여 요약본을 구체화하여 논문의 본문을 작성하세요.\\n#요약본: 안개가 낀 악조건의 날씨에서는 가시성이 저하되어 카메라로 포착한 성보들을 성확허 인식하기 어렵다. 따라서 본 논문에서는 카메라 기반의 기기들이 성상 동작할 수 있도록 안개제거 연구를 진행하였다.\\n#참조 문장: 안개가 낀 악조건의 날씨에서는 가시성이 저하되어 카메라로 포착한 성보들을 성확허 인식하기 어렵다. 본논문에서는 안개 영상에서 밝기와 채도의 차이가 영상의 깊이에 따라 비선형적으로 증가한다는 분석을 통해 깊이 맵 추성을 위한 비선형 모델을 제시한다.'}, {'from': 'gpt', 'value': '<h1>요 약</h1><p>안개가 낀 악조건의 날씨에서는 가시성이 저하되어 카메라로 포착한 성보들을 성확허 인식하기 어렵다. 안개 낀 날씨에서도 사물인식, 차선 인식 등 카메라 기반의 기기들이 성상 동작할 수 있도록 안개제거 연구가 활발히 진행되고 있다. 본논문에서는 안개 영상에서 밝기와 채도의 차이가 영상의 깊이에 따라 비선형적으로 증가한다는 분석을 통해 깊이 맵 추성을 위한 비선형 모델을 제시한다. 비선형 모델의 안개 제거 방법은 어러 가지 안개제거 방법과의 성량직 수치평가(MSE, SSIM, TMQI)를 통해 동등 이상의 결과를 보여줌으로써 우수한 성능을 자랑한다.</p>'}]}\n",
      "\n",
      "{'task': 'Spelling Correction', 'id': 'korea_spelling_correction_0', 'conversations': [{'from': 'human', 'value': '국냇에서 유일하게 법정 커버드본드를 발행할 수 있는 주택금융공상는 지난해 7월 아시아 최촗로 5억 달러 규모의 커버드본드를 발행했다'}, {'from': 'gpt', 'value': '국내에서 유일하게 법정 커버드본드를 발행할 수 있는 주택금융공사는 지난해 7월 아시아 최초로 5억 달러 규모의 커버드본드를 발행했다'}]}\n",
      "\n",
      "{'task': 'Spelling Correction', 'id': 'aihub_interface_spelling_correction_0', 'conversations': [{'from': 'human', 'value': '이 문제에서는 주어진 텍스트 내의 문법 오류를 찾아 수정해야 합니다. 주어진 텍스트를 분석하여 문법 오류가 있는 부분을 나누어 설명하고 올바르게 교정된 문장을 제공하세요.\\n#텍스트: 근데이러한실언은 그냥 나온말이아니라 평소 선생님의생각이 반영되어 입 밖으로 표출된것이라는생각이들었습니다.'}, {'from': 'gpt', 'value': '주어진 텍스트에는 문법 오류가 있습니다.\\n오류 부분:\\n- \"근데이러한실언은\" - 이 문구에는 맞춤법, 띄어쓰기 오류가 있습니다.\\n- \"나온말이아니라\" - 이 문구에는 띄어쓰기 오류가 있습니다.\\n- \"선생님의생각이\" - 이 문구에는 띄어쓰기 오류가 있습니다.\\n- \"표출된것이라는생각이들었습니다.\" - 이 문구에는 띄어쓰기 오류가 있습니다.\\n\\n수정된 문장:\\n그런데 이러한 실언은 그냥 나온 말이 아니라 평소 선생님의 생각이 반영되어 입 밖으로 표출된 것이라는 생각이 들었습니다.'}]}\n",
      "\n",
      "{'task': 'Rear Inference', 'system': '당신은 인공지능 비서입니다. 주어진 원문을 바탕으로 주어진 질문에 가장 적절한 답변을 생성하세요.', 'conversations': [{'from': 'human', 'value': '다음 텍스트에서 제공된 문맥을 정확히 이해하고, 마지막 문장을 자연스럽고 문맥에 맞게 완성하세요. 문장은 이전 내용과 논리적으로 연결되어야 합니다.\\n#텍스트: 주식시장에서 주식 가격 급등락을 완화하여 가능한 한 가격 형성의 안정성을 유지하기 위해 하루에 변동할 수 있는 가격의 등락폭을 상하 일정률로 제한하는 제도적 장치로 가격제한폭 제도를 시행하고 있다. 가격제한폭 제도의 긍정적인 측면은 가격제한폭은 과도한 투기로부터 비롯되는 합리적이지 못한 가격의 형성을 방지할 수 있고, 단기적으로 주가 급등락을 막아 시장안정성에 도움을 준다는 것이다. 부정적인 측면으로 가격제한폭은 주가조정과정을 지연시켜 시장효율성을 감소시킨다는 것이다. 가격제한폭에 대한 우리 나라 기존의 연구들은 거래소시장(KSE)에 대해서 연구가 이루어져 왔는데 본 연구에서는 코스닥시장의 경우를 분석해 보았다.'}, {'from': 'gpt', 'value': '그리고 기존의 연구들은 시장에서 관찰되는 변동성, 즉 관측변동성(observed volatility)을 대상으로 분석이 이루어져 왔는데, 이 관측변동성에는 순수하게 정보에 의해서 발생되는 부분만이 있는 것이 아니라 비합리적 거래자들의 잡음거래(noise trading)로부터 발생되는 부분도 있다는 것이다 그리하여 본 연구에서는 관측변동성을 두 부분으로 분해하여 이 들 각각에 어떠한 영향을 주었는지를 분석하려고 한다.'}], 'id': 'kocommercial_rear_infer_0'}\n",
      "\n",
      "{'task': 'Middle Word Inference', 'system': '당신은 인공지능 비서입니다. 주어진 질문에 가장 적절한 답변을 제공하세요.', 'conversations': [{'from': 'human', 'value': '이 문제에서는 주어진 텍스트 내의 <MASK>로 표시된 부분에 들어갈 적절한 단어를 예측해야 합니다. <MASK>가 위치한 문장의 전체 문맥을 분석하여, 문장의 나머지 내용과 일관되게 <MASK>에 들어갈 가장 적합한 단어를 답하세요.\\n#텍스트: 정신보건법의 내용은 정신질환자의 불가피한 타의입원(involuntary admission) 때문에 일어날 수 있는 환자의 인권침해를 방지할 수 있는 최소한의 규정으로 충분하다는 입장이 있는가 하면, 정신질환의 예방, 치료 및 재활에 필요한 모든 내용을 포함하여 전 국민의 정신건강의 증진에 기여하도록 해야 한다는 주장도 있다. 후자의 내용이 들어간 복지법적인 성격의 법안은 정신보건 증진시책, 시설에 대한 정부보조, 특별시설의 설치에 대한 규정을 두고 있는 일본과 대만의 경우에서 보인다. 우리 나라의 정신보건법도 이런 복지법적인 성격을 띄고 있다고 하겠다. 두 가지 주장의 경우 모두 핵심내용은 정신질환자의 치료와 인권보호이며 정신질환자의 인권침해를 방지하고자 하는 노력은 입원 절차와 환자의 권리에 관한 규정에 반영되어 있다. 우리 나라의 경우, 입원절차에 관한 규정은 1985년 복지부안에서 처음으로 정비되기 시작하여 1992년 국회상정안부터 구체적으로 이루어 졌고, 환자의 권리에 관한 사항들은 1988년 정신의학회안부터 환자가 자신의 입원이 부당함을 주장할 수 있는 <MASK>, 부당한 대우를 받지 않을 <MASK>, 그리고 이러한 문제들을 심의할 수 있는 심의기구, 등의 3가지 사항이 포함되는 내용들이 구체적으로 나타나기 시작하였다.'}, {'from': 'gpt', 'value': '장치'}], 'id': 'kocommercial_mid_word_infer_0'}\n",
      "\n",
      "{'task': 'Rearrange Sentence', 'system': '당신은 인공지능 비서입니다. 주어진 문장 리스트를 모두 활용하여 가장 정확한 단락을 생성하세요.', 'conversations': [{'from': 'human', 'value': \"이 문장들은 임의의 순서로 섞여 있습니다. 모든 문장을 활용하여 원본 단락의 순서와 내용을 올바른 순서로 재구성하세요.\\n#문장 리스트: ['일본인 상업자본가의 토지매수는 초기에는 비옥한 삼남지방의 평야지대에 집중되었으나, 토지확보경쟁이 가열되면서 그 범위는 점차 서남해안의 도서지역으로까지 확대되었다.', '이에 대해 조선농민은 ‘토지조사사업’ 이전의 사적 토지소유권의 확대·강화를 근거로 ‘토지소유권 확인소송’이나 ‘부당이익 반환소송’을 제기하는 등 일본인 지주의 비합법적인 토지매수에 맞서 갖가지 반대운동을 전개하였다.', '일본의 조선침략 이후 조선의 각 개항장에 진출한 일본인 상업자본가는 소작제 농장경영을 통한 고율의 토지수익률을 확인하고, 미곡반출로 상업이윤을 극대화하는 동시에 차츰 자기자본의 투자대상으로서 토지에 눈을 돌리게 되었다.', '그러나 조선통치의 사회적·경제적 기반으로서 식민지 지주제의 확립을 옹호한 식민지 권력은 ‘토지가옥증명규칙’(1906년)과 ‘토지가옥소유권증명규칙’(1908년)을 공포하는 등 일련의 토지법제 개정을 통해 일본인 지주의 토지매수를 합법화하고, 최종적으로는 ‘토지조사사업’을 실시하여 토지소유권을 법률적으로 확정지었다.']\"}, {'from': 'gpt', 'value': '일본의 조선침략 이후 조선의 각 개항장에 진출한 일본인 상업자본가는 소작제 농장경영을 통한 고율의 토지수익률을 확인하고, 미곡반출로 상업이윤을 극대화하는 동시에 차츰 자기자본의 투자대상으로서 토지에 눈을 돌리게 되었다. 일본인 상업자본가의 토지매수는 초기에는 비옥한 삼남지방의 평야지대에 집중되었으나, 토지확보경쟁이 가열되면서 그 범위는 점차 서남해안의 도서지역으로까지 확대되었다. 이에 대해 조선농민은 ‘토지조사사업’ 이전의 사적 토지소유권의 확대·강화를 근거로 ‘토지소유권 확인소송’이나 ‘부당이익 반환소송’을 제기하는 등 일본인 지주의 비합법적인 토지매수에 맞서 갖가지 반대운동을 전개하였다. 그러나 조선통치의 사회적·경제적 기반으로서 식민지 지주제의 확립을 옹호한 식민지 권력은 ‘토지가옥증명규칙’(1906년)과 ‘토지가옥소유권증명규칙’(1908년)을 공포하는 등 일련의 토지법제 개정을 통해 일본인 지주의 토지매수를 합법화하고, 최종적으로는 ‘토지조사사업’을 실시하여 토지소유권을 법률적으로 확정지었다.'}], 'id': 'kocommercial_rearrange_sen_0'}\n",
      "\n",
      "{'task': 'Rearrange Word', 'system': '당신은 인공지능 비서입니다. 주어진 지시사항에 따라 가장 적절한 문장을 생성하세요.', 'conversations': [{'from': 'human', 'value': \"이 문제에는 문장에서 공백을 기준으로 나누고, 구두점을 제거한 무작위로 섞인 단어들이 담긴 리스트가 제공됩니다. 이 리스트의 단어를 모두 활용하여 가장 문맥상 적절한 문장을 생성하세요.\\n#단어리스트: ['평형이', '보충되는', '심전도를', '양전하와', '음전하가', '차이가', '조석', '음전하를', '향어의', '이', '내측은', '현상을', '활동은', '활동을', '환경요인인', '심장세포에', '상태별로', '전기적', '세포막', '달라질', '구분하여', '있는', '상태를', '일주기적', '생리생태는', '혈액량좌우', '나타나는', '밝히고자', '이때', '띠고', '것을', '한다', '이루고', '심장의', '야간에', 'Na', '유지되고', '따라', '나며', '체장에', '혈액을', '말하는데', '포함되어', '이온의', '있다본', '주간과', '외측은', '생체전위', '하고', '깨지면서', '유출되는', '관상동맥으로', 'Ca이온이', '양전하를', '필요로', '유입', '있어서', '부터', '외측의', '많은', '있는데', '구명하기', '혈액', '중에', '보이는지를', '삽입하여', '분극', '세포내외의', '심장은', '卞5E位가', '정지', '말한다', '활동에', '균형을', '따라서도', '도출되는', '어떠한', '위하여어체내에', '의해서', '내측의', '간을', '심근', '도출하였는데이들', '상태전기적', '야', '시간', '파형은', '알려져', '등', '전극을', '행동은', '광음전기', '24', '및', '세포내로', '연구는', '속개하기', '것으로', '따른', '주기로', '있고하루', '어류의', '외부', '심박수와', '상태는', '하는', '등에', '심전도', '말하며', '조도수온염분먹이일사량', '전기적으로', '수', '과', '위해서는', '분석하여', 'Ka', '자극인', '변화를', '난다고']\"}, {'from': 'gpt', 'value': \"심전도 파형은 심장의 전기적 활동에 의해서 도출되는 것을 말하는데， 심장의 전기적 활동은 심장세포에 있어서 외측은 전기적으로 양전하를 내측은 음전하를 띠고 있는데， 이때 심장의 정지 상태는 세포막 외측의 양전하와 내측의 음전하가 균형을 이루고 있는 상태（전기적 분극 상태）를 말하며， 심장은 활동을 속개하기 위해서는 많은 혈액량（좌우 관상동맥으로 보충되는 혈액）을 필요로 하고， 이 혈액 중에 포함되어 있는 Ka\\\\ Na： Ca+이온이 심근 세포내로 유입 및 유출되는 것으로 부터 세포내외의 이온의 평형이 유지되고 깨지면서 나타나는 전기적 현상을 말한다. 어류의 생리생태는 외부 자극인 광，음，전기 등 과 환경요인인 조도，수온，염분，먹이，일사량, 조석 등에 따라 달라질 수 있고，하루 24 시간 주기로 하는 일주기적 어류의 행동은 주간과 야간에 차이가 나며， 체장에 따라서도 차이가 난다고 알려져 있다.본 연구는 향어의 체장에 따른 심전도를 구명하기 위하여，어체내에 전극을 삽입하여 주간과 야 간을 구분하여 심전도를 도출하였는데，이들 심전도를 분석하여， 체장에 따른 심박수와 생체전위 （卞§5'『E位）가 상태별로 어떠한 변화를 보이는지를 밝히고자 한다.\"}], 'id': 'kocommercial_rearrange_0'}\n",
      "\n",
      "{'task': 'Akward Correction', 'conversations': [{'from': 'human', 'value': \"주어진 문장 리스트에서 문맥 상 어울리지 않는 문장을 찾으세요.\\n#문장 리스트: ['수학에서 상수란 그 값이 변하지 않는 불변량으로, 변수의 반대말이다. 물리 상수와는 달리, 수학 상수는 물리적 측정과는 상관없이 정의된다.', '수학의 각 분야들은 상업에 필요한 계산을 하기 위해, 숫자들의 관계를 이해하기 위해, 토지를 측량하기 위해, 그리고 천문학적 사건들을 예견하기 위해 발전되어왔다. 이 네 가지 목적은 대략적으로 수학이 다루는 대상인 양, 구조, 공간 및 변화에 대응되며, 이들을 다루는 수학의 분야를 각각 산술, 대수학, 기하학, 해석학이라 한다. 또한 이 밖에도 근대 이후에 나타난 수학기초론과 이산수학 및 응용수학 등이 있다.', '특정 수학 상수, 예를 들면 골롬-딕맨 상수, 프랑세즈-로빈슨 상수, formula_1, 레비 상수같은 상수는 다른 수학상수 또는 함수와 약한 상관관계 또는 강한 상관관계를 갖는다.']\"}, {'from': 'gpt', 'value': '주어진 문장 리스트에서 문맥상 어울리지 않는 문장은 다음과 같습니다.\\n수학의 각 분야들은 상업에 필요한 계산을 하기 위해, 숫자들의 관계를 이해하기 위해, 토지를 측량하기 위해, 그리고 천문학적 사건들을 예견하기 위해 발전되어왔다. 이 네 가지 목적은 대략적으로 수학이 다루는 대상인 양, 구조, 공간 및 변화에 대응되며, 이들을 다루는 수학의 분야를 각각 산술, 대수학, 기하학, 해석학이라 한다. 또한 이 밖에도 근대 이후에 나타난 수학기초론과 이산수학 및 응용수학 등이 있다.\\n\\n나머지 문장들은 모두 이와는 전혀 관련이 없는 문장입니다.'}], 'id': 'mr_tydi_akward_correction_2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in task_list:\n",
    "    dataset = load_raw_dataset(path)\n",
    "    print(dataset[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670e52d-9c94-4204-ab9a-49eb1c583e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
