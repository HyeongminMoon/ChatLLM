{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "# lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dpo_dataset import load_dpo_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")\n",
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "get and save language distributions\n",
    "\"\"\"\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT raw data and change format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"make lang_dict(sft)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661bfc45-9396-4f6b-a6ff-12acc03a0b2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1016 examples [00:00, 21155.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make lang_dict(dpo)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    \n",
    "    langs = {}\n",
    "\n",
    "    lang_chosen = lang_detect.predict(chosen.replace('\\n', ' '))[0][0]\n",
    "    if lang_chosen not in langs:\n",
    "        langs[lang_chosen] = 1\n",
    "    else:\n",
    "        langs[lang_chosen] += 1\n",
    "        \n",
    "    lang_rejected = lang_detect.predict(rejected.replace('\\n', ' '))[0][0]\n",
    "    if lang_rejected not in langs:\n",
    "        langs[lang_rejected] = 1\n",
    "    else:\n",
    "        langs[lang_rejected] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6279a941-d152-4693-b5e6-44d63a86acbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 1016\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460862df-7bbf-4a40-982d-1f7782fd22ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d1d4-7a7c-45c2-94c8-5f07c79ac036",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565040d8-08c4-4a16-bb43-e6952114b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dff716d9-b345-4f39-9380-22bb3bbec251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 111092.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': \"To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we will substitute these values for the variables `x` and `y` in the given expression and simplify.\\n\\nSubstituting `x = 5` and `y = -2` into the expression, we get:\\n\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying, we have:\\n\\n2(25) + 6 + 7\\n\\nNow, let's perform the operations:\\n\\n50 + 6 + 7 = 63\\n\\nTherefore, the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` is 63.\",\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': 'To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we need to replace `x` with `5` and `y` with `-2` in the expression and simplify it. \\n\\nSubstituting `x = 5` and `y = -2`:\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying further:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\nTherefore, substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` gives us a result of 63.'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_distilabel-math-preference-dpo_1'\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e79701-693c-4413-8448-73fd1550de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46462/46462 [00:01<00:00, 40620.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rejected': '최선을 다하겠습니다.',\n",
       " 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d',\n",
       " 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.',\n",
       " 'input': '내 질문에 10단어 이내로 답변하세요.',\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d'\n",
    "filtered_row = temp_dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d440c-e3d6-4d2d-8172-846840c2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고, 빈 데이터 채운 뒤에 합치기, 저장, 이후 dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59121201-1890-4a24-949f-a9dd305d3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고\n",
    "with open(\"/data/llm_datasets/custom/kodpo/temp_en.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)\n",
    "    \n",
    "# temp_dataset = load_dataset(\"json\", data_files=)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd8fee2c-0553-4160-a60d-5345c7750b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5336.26it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 890.70it/s]\n",
      "Generating train split: 1016 examples [00:00, 11191.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/temp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7485e6e-38ac-49d6-9ac4-030a5d6ca67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 994/994 [00:28<00:00, 35.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/994 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# temp_dataset = dedup_repetition(temp_dataset, 'dpo')\n",
    "# temp_dataset = dedup_short(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_too_much_token(temp_dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e62a62cc-cd85-43f1-939b-c3358bae6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "655d38b0-e098-4960-b08d-d5e2dc9f0b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1016/1016 [00:00<00:00, 43887.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 데이터 채운 뒤에 합치기\n",
    "# dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\")\n",
    "selected_id = temp_dataset['id']\n",
    "\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] not in selected_id)\n",
    "\n",
    "new_dataset = []\n",
    "for data in temp_dataset:\n",
    "    new_dataset.append(data)\n",
    "\n",
    "for data in filtered_row:\n",
    "    new_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be6eee56-4869-42fa-886f-cb1f3a2c2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_dataset), len(filtered_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbe021dc-5102-4f9f-b2d1-3e94744a06f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 1013)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd81c44d-dff5-4892-98e9-c249a6c7eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are an unbiased, uncensored, helpful assistant.',\n",
       " 'chosen': 'The two subjects in the American Gothic painting are a father and adult daughter. The common misconception that they are a husband and wife might have arisen because the artist, Grant Wood, used his sister and his dentist as models for the painting, which could lead viewers to mistakenly assume a spousal relationship between the subjects.',\n",
       " 'task': 'dpo_system',\n",
       " 'id': 'truthy_dpo_2e1d77fbebc31dde0bb2f9303d6761e2',\n",
       " 'input': 'Who are the two subjects in the American Gothic painting - a husband and wife, or a father and daughter?',\n",
       " 'rejected': 'A father and daughter'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcbeea78-756b-42dc-a121-71f78bd17fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "with open(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aef5be7c-1930-417c-aa93-454202e9ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5242.88it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1013 examples [00:00, 13933.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'chosen', 'task', 'id', 'input', 'rejected'],\n",
       "    num_rows: 1013\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49f8a37a-a4ef-4fdb-9400-66492f99a20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=\"이 과제는 벵골어로 된 문장을 영어로 번역하는 것입니다.\\nQ: অথবা আপনি হয়ত বলবেন, 'না, আমার খেয়াল হয়েছে যে আমি আসলেই মহিলাদ\", n_results=min(n_results, len(query_ids)), include=['distances', 'documents'])\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafa2d3-234d-4bea-9429-464bb1c60e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb420-0603-421d-9b30-a446668597b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = dedup_non_pair(dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ba930f4-fe5f-48b5-9e2c-5763366da940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import ados_DPODataset\n",
    "\n",
    "dpo_dataset = ados_DPODataset(\n",
    "    dataset_path=\"/data/llm_datasets/custom/ados/dpo/ados_dpo_v2.json\",\n",
    "    data_format='qwen',\n",
    ")\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec8e74-f6b1-437d-8285-b620ea30fcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_raw_dataset(dataset_path, split='train'):\n",
    "    if dataset_path.endswith(\"json\") or dataset_path.endswith(\"json.kr\"):\n",
    "        try:\n",
    "            dataset = load_dataset(\"json\", data_files=dataset_path, split=split)\n",
    "        except:\n",
    "            print(\"Not able to read records in the JSON file, read from json reader...\")\n",
    "            dataset = load_json_dataset(dataset_path)\n",
    "    elif dataset_path.endswith(\"parquet\"):\n",
    "        dataset = load_dataset(\"parquet\", data_files=dataset_path, split=split)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_path, split=split)\n",
    "    return dataset\n",
    "\n",
    "import json\n",
    "def load_json_dataset(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Union, List\n",
    "def dataset_to_json(dataset: Union[List[dict], Dataset], path: str):\n",
    "    if isinstance(dataset, Dataset):\n",
    "        dataset = [data for data in dataset]\n",
    "    \n",
    "    if os.path.dirname(path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding='utf-8') as json_file:\n",
    "        json.dump(dataset, json_file, indent=4, ensure_ascii=True)\n",
    "        \n",
    "    print(f\"saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a6bdc077-bf66-42ec-b97c-98074315fd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_raw_dataset(\"/data/llm_datasets/sftv5/processed/AkwardCorrection/mr-tydi-corpus_AkwardCorrectionData.json\")\n",
    "new_dataset = []\n",
    "for idx, data in enumerate(raw_dataset):\n",
    "    lists = data['conversation'][1]['value']\n",
    "    answer = data['conversation'][3]['value']\n",
    "    \n",
    "    new_dataset.append({\n",
    "        'id': f\"mr_tydi_akward_correction_{idx}\",\n",
    "        'task': 'Akward Correction',\n",
    "        'conversation': [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": f\"주어진 문장 리스트에서 문맥 상 어울리지 않는 문장을 찾으세요.\\n#문장 리스트: {lists}\",\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\":  f\"주어진 문장 리스트에서 문맥상 어울리지 않는 문장은 다음과 같습니다.\\n{answer}\\n\\n나머지 문장들은 모두 이와는 전혀 관련이 없는 문장입니다.\",\n",
    "            },\n",
    "        ]\n",
    "    })\n",
    "dataset_to_json(new_dataset, \"/data/llm_datasets/sftv5/processed/AkwardCorrection/refined/mr_tydi_akward_correction.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "74700caf-5276-4a5d-9f9c-8fc520dd4e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_raw_dataset(\"/data/llm_datasets/sftv5/processed/SummaryInference/TechnicalSciencesSummaryInferenceData.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2678160e-b613-4be7-aa5d-9329c298d1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /data/llm_datasets/sftv5/processed/SummaryInference/refined/aihub_tech_sci_summary_infer.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "new_dataset = []\n",
    "for idx, data in enumerate(raw_dataset):\n",
    "    question = data['conversation'][0]['value']\n",
    "    refer_lists = data['conversation'][2]['value']\n",
    "    answer = data['conversation'][4]['value']\n",
    "    \n",
    "    template = random.choice([\n",
    "        \"당신에게 주어진 이 문장들은 어떤 논문의 요약으로 이로부터 논문의 본문을 작성해야합니다. 주어진 요약문장과 참조할 문장들을 토대로 논문을 작성하세요.\\n#논문 요약: {summary}\\n#참조할 문장들: {refer}\",\n",
    "        \"이 문제에서는 어떤 논문의 요약본이 주어집니다. 참조 문장을 참고하여 요약본을 구체화하여 논문의 본문을 작성하세요.\\n#요약본: {summary}\\n#참조 문장: {refer}\"\n",
    "    ])\n",
    "    \n",
    "    new_dataset.append({\n",
    "        'id': f\"aihub_tech_sci_summary_infer_{idx}\",\n",
    "        'task': 'Summary Inference',\n",
    "        'conversation': [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": template.format(summary=question, refer=refer_lists),\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\":  answer,\n",
    "            },\n",
    "        ]\n",
    "    })\n",
    "dataset_to_json(new_dataset, \"/data/llm_datasets/sftv5/processed/SummaryInference/refined/aihub_tech_sci_summary_infer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db516b2f-0d9a-4fe4-ad74-03ac31ea6ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400878/400878 [00:02<00:00, 143726.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/400878 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400878/400878 [00:35<00:00, 11274.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/400878 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400514/400514 [00:07<00:00, 56625.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/400514 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400514/400514 [00:06<00:00, 57953.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/400514 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400514/400514 [00:06<00:00, 58507.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/400514 deduped\n",
      "saved at /data/llm_datasets/dpov3/4dedup/aihub_enko_society_dpo.json\n",
      "dedup_non_pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 337194/337194 [00:02<00:00, 144263.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/337194 deduped\n",
      "dedup_repetition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 337194/337194 [00:27<00:00, 12047.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/337194 deduped\n",
      "dedup_math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 336960/336960 [00:06<00:00, 54254.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/336960 deduped\n",
      "dedup_too_much_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 336960/336960 [00:05<00:00, 57408.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/336960 deduped\n",
      "dedup_short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 336960/336960 [00:05<00:00, 57774.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/336960 deduped\n",
      "saved at /data/llm_datasets/dpov3/4dedup/aihub_enko_tech_dpo.json\n"
     ]
    }
   ],
   "source": [
    "# long short제외 dedup\n",
    "# paths = glob.glob('/data/llm_datasets/sftv5/processed/Translation/*.json')\n",
    "paths = ['/data/llm_datasets/dpov3/1reformat/aihub_enko_society_dpo.json',\n",
    "        '/data/llm_datasets/dpov3/1reformat/aihub_enko_tech_dpo.json']\n",
    "for path in paths:\n",
    "    # path = '/data/llm_datasets/sftv5/processed/Translation/aihub_expert_enko.json'\n",
    "    dst = os.path.join(\"/data/llm_datasets/dpov3/4dedup/\", os.path.basename(path))\n",
    "    dataset = load_raw_dataset(path)\n",
    "    if 'conversation' in dataset.features:\n",
    "        dataset = dataset.rename_column('conversation', 'conversations')\n",
    "\n",
    "    # apply dedups\n",
    "    print(\"dedup_non_pair\")\n",
    "    dataset = dedup_non_pair(dataset, 'dpo')\n",
    "    print(\"dedup_repetition\")\n",
    "    dataset = dedup_repetition(dataset, 'dpo')\n",
    "    print(\"dedup_math\")\n",
    "    dataset = dedup_math(dataset, 'dpo')\n",
    "    print(\"dedup_too_much_token\")\n",
    "    dataset = dedup_too_much_token(dataset, 'dpo', model_name='DIE-10_7B_sftv5_daily-100')\n",
    "    print(\"dedup_short\")\n",
    "    dataset = dedup_short(dataset, 'dpo', 5)\n",
    "    # do similarity dedup if needed\n",
    "    # do code refine if needed\n",
    "\n",
    "    dataset_to_json(dataset, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223c4d7-dd14-44aa-9bd0-1fc5d9a45766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for path in sftv5_list:\n",
    "    dst = os.path.join(\"/data/llm_datasets/sftv5/deduped/\", os.path.basename(path))\n",
    "    dataset = load_raw_dataset(dst)\n",
    "    \n",
    "    print(dst, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a3283d-a994-4adf-9bbf-6b925b7f50fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniform_select(dataset, division_factor):\n",
    "    return dataset.select(list(range(0, len(dataset), division_factor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89e3a14-454f-40f6-a25b-8562f1c343df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /data/llm_datasets/dpov3/5rebal/aihub_enko_tech_dpo.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5949.37it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1855.89it/s]\n",
      "Generating train split: 400514 examples [00:03, 125615.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /data/llm_datasets/dpov3/5rebal/aihub_enko_society_dpo.json\n"
     ]
    }
   ],
   "source": [
    "rebal_list = [\n",
    "    \"/data/llm_datasets/dpov3/4dedup/aihub_enko_tech_dpo.json\",\n",
    "    \"/data/llm_datasets/dpov3/4dedup/aihub_enko_society_dpo.json\",\n",
    "]\n",
    "rebal_factor = [60, 80]\n",
    "\n",
    "for i in range(2):\n",
    "    path = rebal_list[i]\n",
    "    dst = path.replace(\"4dedup\", \"5rebal\")\n",
    "    dataset = load_raw_dataset(path)\n",
    "    dataset = uniform_select(dataset, rebal_factor[i])\n",
    "    dataset_to_json(dataset, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "733af047-b774-43d2-954c-ce5a0d718f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\", # daily\n",
    "\"/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json\", # daily\n",
    "\"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\", # daily\n",
    "\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\", # daily\n",
    "\n",
    "v4_list = [\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_book_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_law_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/naver-news-summarization.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/alpaca-gpt4-korean_dedup2.json\", \n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\", \n",
    "    \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "task_list = [\n",
    "    '/data/llm_datasets/sftv5/deduped/wiki_qa_front_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/wiki_qa_mid_key_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/aihub_tech_sci_summary_infer.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/korea_spelling_correction.json',\n",
    "    '/data/llm_datasets/sftv5/deduped/aihub_interface_spelling_correction.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rear_infer.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_mid_word_infer.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rearrange_sen.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/kocommercial_rearrange.json',\n",
    "    '/data/llm_datasets/sftv5/rebal/mr_tydi_akward_correction.json',\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_cnko_society.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_cnko_tech.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_enko_society.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_enko_tech.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_expert_enko.json\",\n",
    "    \"/data/llm_datasets/sftv5/rebal/aihub_jpko.json\",\n",
    "]\n",
    "\n",
    "# random.shuffle(task_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "760d6760-a77b-409e-be7e-b0eb2297d308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/sftv5/v4_repeat/aihub_book_summary.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/aihub_book_summary.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/aihub_law_summary.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/aihub_law_summary.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/naver-news-summarization.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/naver-news-summarization.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/koalpaca_v1.1-vicuna.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/koalpaca_v1.1-vicuna.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/alpaca-gpt4-korean_dedup2.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/alpaca-gpt4-korean_dedup2.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/korquad-chat-vicuna.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/korquad-chat-vicuna.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/wizardlm_orca_vicuna_dedup2.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/wizardlm_orca_vicuna_dedup2.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(enko)-10000.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(enko)-10000.json\n",
      "/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(koen)-10000.json\n",
      "saved at /data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(koen)-10000.json\n"
     ]
    }
   ],
   "source": [
    "# 이미 학습된 데이터는 10분의 1만 사용\n",
    "for i in range(len(v4_list)):\n",
    "    path = v4_list[i]\n",
    "    dst = os.path.join(\"/data/llm_datasets/sftv5/v4_repeat/\", os.path.basename(path))\n",
    "    print(dst)\n",
    "    dataset = load_raw_dataset(path)\n",
    "    dataset = uniform_select(dataset, 10)\n",
    "    dataset_to_json(dataset, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "963fc33f-8812-4450-9b48-2f0bdc727116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v4_repeat_list = [\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/aihub_book_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/aihub_law_summary.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/naver-news-summarization.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/sftv5/v4_repeat/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    # \"/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    # \"/data/llm_datasets/sftv5/v4_repeat/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "v5_task_list = task_list + v4_repeat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a09a60d5-fa64-42fe-9bf7-ab0718a191ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /data/llm_datasets/sftv5/v5_task_repeat/sftv5_task_div30.json\n"
     ]
    }
   ],
   "source": [
    "#sftv5_task_div30\n",
    "\n",
    "sftv5_task_div30 = concatenate_datasets([load_sft_dataset(path, 'train') for path in v5_task_list])\n",
    "sftv5_task_div30 = uniform_select(sftv5_task_div30, 30)\n",
    "dataset_to_json(sftv5_task_div30, \"/data/llm_datasets/sftv5/v5_task_repeat/sftv5_task_div30.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5de607c3-c77a-4c32-863f-929a3934482f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json', '/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json', '/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json', '/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json', '/data/llm_datasets/sftv5/v5_task_repeat/sftv5_task_div30.json']\n"
     ]
    }
   ],
   "source": [
    "v5_daily_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\", # daily\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json\", # daily\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\", # daily\n",
    "    \"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\", # daily\n",
    "    \"/data/llm_datasets/sftv5/v5_task_repeat/sftv5_task_div30.json\" # task_div\n",
    "]\n",
    "print(v5_daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b675558a-2fb6-49fe-9b10-1a7dfddbd20e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68173 \n",
      " {'task': 'Front Inference', 'id': 'wiki_qa_front_infer_0', 'system': None, 'conversations': [{'from': 'human', 'value': \"다음 텍스트에서 제공된 문맥을 정확히 이해하고, 앞에 올 문장을 자연스럽고 문맥에 맞게 작성하세요. 문장은 이후 내용과 논리적으로 연결되어야 합니다.\\n#텍스트: 이들 식물은 사과와 같은 향이 나기 때문에 '캐모마일'이라고 불립니다. 캐모마일은 노란색 꽃을 가진 식물이며, 6월 중순에서 7월 하순까지 꽃을 피워요. 추위에 강한 편이며, 양지에서 잘 자라요. 꽃은 낮에는 피고 밤에는 오므라져 있으며, 일주일 정도 꽃이 핀다고 합니다. 씨받이는 매우 쉬우며, 싹도 잘 나기 때문에 직접 씨를 받을 수 있습니다. 캐모마일은 양지 바르고 배수가 좋은 사질토에서 잘 피워요. 수확한 꽃은 통풍이 잘 되는 응달에서 말려 저장해요. 캐모마일의 방향유는 긴장 완화, 두통 등의 통증 완화에 효과가 있으며, 몸을 따뜻하게 해주는 효과도 있습니다. 차로 마시면 위장장애의 완화에 도움을 줄 수 있습니다. 캐모마일은 카페인 성분이 없는 허브이며, 좀벌레를 없애는 데 효과적인 허브식물입니다.\"}, {'from': 'gpt', 'value': \"캐모마일은 국화과에 속하는 식물로, 저먼캐모마일, 캐모마일, 카미레 등이 포함됩니다. 이들 식물은 사과와 같은 향이 나기 때문에 '캐모마일'이라고 불립니다. 캐모마일은 노란색 꽃을 가진 식물이며, 6월 중순에서 7월 하순까지 꽃을 피워요. 추위에 강한 편이며, 양지에서 잘 자라요. 꽃은 낮에는 피고 밤에는 오므라져 있으며, 일주일 정도 꽃이 핀다고 합니다. 씨받이는 매우 쉬우며, 싹도 잘 나기 때문에 직접 씨를 받을 수 있습니다. 캐모마일은 양지 바르고 배수가 좋은 사질토에서 잘 피워요. 수확한 꽃은 통풍이 잘 되는 응달에서 말려 저장해요. 캐모마일의 방향유는 긴장 완화, 두통 등의 통증 완화에 효과가 있으며, 몸을 따뜻하게 해주는 효과도 있습니다. 차로 마시면 위장장애의 완화에 도움을 줄 수 있습니다. 캐모마일은 카페인 성분이 없는 허브이며, 좀벌레를 없애는 데 효과적인 허브식물입니다.\"}]}\n",
      "\n",
      "63860 \n",
      " {'task': 'Middle Keyword Inference', 'id': 'wiki_qa_mid_key_infer_0', 'conversations': [{'from': 'human', 'value': \"이 문제에서는 주어진 텍스트 내의 <MASK>로 표시된 부분에 들어갈 적절한 문장을 예측해야 합니다. <MASK>가 위치한 문장의 전체 문맥 및 주어진 단어들을 분석하여, 문장의 나머지 내용과 일관되게 <MASK>에 들어갈 가장 적합한 문장을 답하세요.\\n#텍스트: 밀로시 벨코비치(1995년 9월 26일)는 세르비아의 축구 선수로, 현재 SV 베르더 브레멘에서 활약하고 있습니다. 그는 수비형 미드필더와 수비수 포지션에서 능숙하게 플레이합니다. 스위스 출신이지만 세르비아 부모님에게서 태어나 두 개의 국적을 가지고 있습니다. 처음에는 스위스 대표팀에서 활동했으나, 2012년 이후에는 세르비아 대표팀에서 경기를 진행하고 있습니다. <MASK> 또한, 바젤슈타트주 출신이며 미들즈브러 FC와 찰턴 애슬레틱 FC에도 소속되어 있습니다. 2018년 FIFA 월드컵에도 참가했습니다.\\n#단어: ['2015년 FIFA U-20 월드컵 우승', '세르비아 U-20 팀', '그의 참가']\"}, {'from': 'gpt', 'value': '그는 2015년 FIFA U-20 월드컵에서 세르비아 U-20 팀과 함께 우승을 차지했습니다.'}]}\n",
      "\n",
      "5731 \n",
      " {'task': 'Summary Inference', 'id': 'aihub_tech_sci_summary_infer_14', 'conversations': [{'from': 'human', 'value': '이 문제에서는 어떤 논문의 요약본이 주어집니다. 참조 문장을 참고하여 요약본을 구체화하여 논문의 본문을 작성하세요.\\n#요약본: 안개가 낀 악조건의 날씨에서는 가시성이 저하되어 카메라로 포착한 성보들을 성확허 인식하기 어렵다. 따라서 본 논문에서는 카메라 기반의 기기들이 성상 동작할 수 있도록 안개제거 연구를 진행하였다.\\n#참조 문장: 안개가 낀 악조건의 날씨에서는 가시성이 저하되어 카메라로 포착한 성보들을 성확허 인식하기 어렵다. 본논문에서는 안개 영상에서 밝기와 채도의 차이가 영상의 깊이에 따라 비선형적으로 증가한다는 분석을 통해 깊이 맵 추성을 위한 비선형 모델을 제시한다.'}, {'from': 'gpt', 'value': '<h1>요 약</h1><p>안개가 낀 악조건의 날씨에서는 가시성이 저하되어 카메라로 포착한 성보들을 성확허 인식하기 어렵다. 안개 낀 날씨에서도 사물인식, 차선 인식 등 카메라 기반의 기기들이 성상 동작할 수 있도록 안개제거 연구가 활발히 진행되고 있다. 본논문에서는 안개 영상에서 밝기와 채도의 차이가 영상의 깊이에 따라 비선형적으로 증가한다는 분석을 통해 깊이 맵 추성을 위한 비선형 모델을 제시한다. 비선형 모델의 안개 제거 방법은 어러 가지 안개제거 방법과의 성량직 수치평가(MSE, SSIM, TMQI)를 통해 동등 이상의 결과를 보여줌으로써 우수한 성능을 자랑한다.</p>'}]}\n",
      "\n",
      "59589 \n",
      " {'task': 'Spelling Correction', 'id': 'korea_spelling_correction_0', 'conversations': [{'from': 'human', 'value': '국냇에서 유일하게 법정 커버드본드를 발행할 수 있는 주택금융공상는 지난해 7월 아시아 최촗로 5억 달러 규모의 커버드본드를 발행했다'}, {'from': 'gpt', 'value': '국내에서 유일하게 법정 커버드본드를 발행할 수 있는 주택금융공사는 지난해 7월 아시아 최초로 5억 달러 규모의 커버드본드를 발행했다'}]}\n",
      "\n",
      "102284 \n",
      " {'task': 'Spelling Correction', 'id': 'aihub_interface_spelling_correction_0', 'conversations': [{'from': 'human', 'value': '이 문제에서는 주어진 텍스트 내의 문법 오류를 찾아 수정해야 합니다. 주어진 텍스트를 분석하여 문법 오류가 있는 부분을 나누어 설명하고 올바르게 교정된 문장을 제공하세요.\\n#텍스트: 근데이러한실언은 그냥 나온말이아니라 평소 선생님의생각이 반영되어 입 밖으로 표출된것이라는생각이들었습니다.'}, {'from': 'gpt', 'value': '주어진 텍스트에는 문법 오류가 있습니다.\\n오류 부분:\\n- \"근데이러한실언은\" - 이 문구에는 맞춤법, 띄어쓰기 오류가 있습니다.\\n- \"나온말이아니라\" - 이 문구에는 띄어쓰기 오류가 있습니다.\\n- \"선생님의생각이\" - 이 문구에는 띄어쓰기 오류가 있습니다.\\n- \"표출된것이라는생각이들었습니다.\" - 이 문구에는 띄어쓰기 오류가 있습니다.\\n\\n수정된 문장:\\n그런데 이러한 실언은 그냥 나온 말이 아니라 평소 선생님의 생각이 반영되어 입 밖으로 표출된 것이라는 생각이 들었습니다.'}]}\n",
      "\n",
      "67350 \n",
      " {'task': 'Rear Inference', 'system': '당신은 인공지능 비서입니다. 주어진 원문을 바탕으로 주어진 질문에 가장 적절한 답변을 생성하세요.', 'conversations': [{'from': 'human', 'value': '다음 텍스트에서 제공된 문맥을 정확히 이해하고, 마지막 문장을 자연스럽고 문맥에 맞게 완성하세요. 문장은 이전 내용과 논리적으로 연결되어야 합니다.\\n#텍스트: 주식시장에서 주식 가격 급등락을 완화하여 가능한 한 가격 형성의 안정성을 유지하기 위해 하루에 변동할 수 있는 가격의 등락폭을 상하 일정률로 제한하는 제도적 장치로 가격제한폭 제도를 시행하고 있다. 가격제한폭 제도의 긍정적인 측면은 가격제한폭은 과도한 투기로부터 비롯되는 합리적이지 못한 가격의 형성을 방지할 수 있고, 단기적으로 주가 급등락을 막아 시장안정성에 도움을 준다는 것이다. 부정적인 측면으로 가격제한폭은 주가조정과정을 지연시켜 시장효율성을 감소시킨다는 것이다. 가격제한폭에 대한 우리 나라 기존의 연구들은 거래소시장(KSE)에 대해서 연구가 이루어져 왔는데 본 연구에서는 코스닥시장의 경우를 분석해 보았다.'}, {'from': 'gpt', 'value': '그리고 기존의 연구들은 시장에서 관찰되는 변동성, 즉 관측변동성(observed volatility)을 대상으로 분석이 이루어져 왔는데, 이 관측변동성에는 순수하게 정보에 의해서 발생되는 부분만이 있는 것이 아니라 비합리적 거래자들의 잡음거래(noise trading)로부터 발생되는 부분도 있다는 것이다 그리하여 본 연구에서는 관측변동성을 두 부분으로 분해하여 이 들 각각에 어떠한 영향을 주었는지를 분석하려고 한다.'}], 'id': 'kocommercial_rear_infer_0'}\n",
      "\n",
      "59333 \n",
      " {'task': 'Middle Word Inference', 'system': '당신은 인공지능 비서입니다. 주어진 질문에 가장 적절한 답변을 제공하세요.', 'conversations': [{'from': 'human', 'value': '이 문제에서는 주어진 텍스트 내의 <MASK>로 표시된 부분에 들어갈 적절한 단어를 예측해야 합니다. <MASK>가 위치한 문장의 전체 문맥을 분석하여, 문장의 나머지 내용과 일관되게 <MASK>에 들어갈 가장 적합한 단어를 답하세요.\\n#텍스트: 정신보건법의 내용은 정신질환자의 불가피한 타의입원(involuntary admission) 때문에 일어날 수 있는 환자의 인권침해를 방지할 수 있는 최소한의 규정으로 충분하다는 입장이 있는가 하면, 정신질환의 예방, 치료 및 재활에 필요한 모든 내용을 포함하여 전 국민의 정신건강의 증진에 기여하도록 해야 한다는 주장도 있다. 후자의 내용이 들어간 복지법적인 성격의 법안은 정신보건 증진시책, 시설에 대한 정부보조, 특별시설의 설치에 대한 규정을 두고 있는 일본과 대만의 경우에서 보인다. 우리 나라의 정신보건법도 이런 복지법적인 성격을 띄고 있다고 하겠다. 두 가지 주장의 경우 모두 핵심내용은 정신질환자의 치료와 인권보호이며 정신질환자의 인권침해를 방지하고자 하는 노력은 입원 절차와 환자의 권리에 관한 규정에 반영되어 있다. 우리 나라의 경우, 입원절차에 관한 규정은 1985년 복지부안에서 처음으로 정비되기 시작하여 1992년 국회상정안부터 구체적으로 이루어 졌고, 환자의 권리에 관한 사항들은 1988년 정신의학회안부터 환자가 자신의 입원이 부당함을 주장할 수 있는 <MASK>, 부당한 대우를 받지 않을 <MASK>, 그리고 이러한 문제들을 심의할 수 있는 심의기구, 등의 3가지 사항이 포함되는 내용들이 구체적으로 나타나기 시작하였다.'}, {'from': 'gpt', 'value': '장치'}], 'id': 'kocommercial_mid_word_infer_0'}\n",
      "\n",
      "63341 \n",
      " {'task': 'Rearrange Sentence', 'system': '당신은 인공지능 비서입니다. 주어진 문장 리스트를 모두 활용하여 가장 정확한 단락을 생성하세요.', 'conversations': [{'from': 'human', 'value': \"이 문장들은 임의의 순서로 섞여 있습니다. 모든 문장을 활용하여 원본 단락의 순서와 내용을 올바른 순서로 재구성하세요.\\n#문장 리스트: ['일본인 상업자본가의 토지매수는 초기에는 비옥한 삼남지방의 평야지대에 집중되었으나, 토지확보경쟁이 가열되면서 그 범위는 점차 서남해안의 도서지역으로까지 확대되었다.', '이에 대해 조선농민은 ‘토지조사사업’ 이전의 사적 토지소유권의 확대·강화를 근거로 ‘토지소유권 확인소송’이나 ‘부당이익 반환소송’을 제기하는 등 일본인 지주의 비합법적인 토지매수에 맞서 갖가지 반대운동을 전개하였다.', '일본의 조선침략 이후 조선의 각 개항장에 진출한 일본인 상업자본가는 소작제 농장경영을 통한 고율의 토지수익률을 확인하고, 미곡반출로 상업이윤을 극대화하는 동시에 차츰 자기자본의 투자대상으로서 토지에 눈을 돌리게 되었다.', '그러나 조선통치의 사회적·경제적 기반으로서 식민지 지주제의 확립을 옹호한 식민지 권력은 ‘토지가옥증명규칙’(1906년)과 ‘토지가옥소유권증명규칙’(1908년)을 공포하는 등 일련의 토지법제 개정을 통해 일본인 지주의 토지매수를 합법화하고, 최종적으로는 ‘토지조사사업’을 실시하여 토지소유권을 법률적으로 확정지었다.']\"}, {'from': 'gpt', 'value': '일본의 조선침략 이후 조선의 각 개항장에 진출한 일본인 상업자본가는 소작제 농장경영을 통한 고율의 토지수익률을 확인하고, 미곡반출로 상업이윤을 극대화하는 동시에 차츰 자기자본의 투자대상으로서 토지에 눈을 돌리게 되었다. 일본인 상업자본가의 토지매수는 초기에는 비옥한 삼남지방의 평야지대에 집중되었으나, 토지확보경쟁이 가열되면서 그 범위는 점차 서남해안의 도서지역으로까지 확대되었다. 이에 대해 조선농민은 ‘토지조사사업’ 이전의 사적 토지소유권의 확대·강화를 근거로 ‘토지소유권 확인소송’이나 ‘부당이익 반환소송’을 제기하는 등 일본인 지주의 비합법적인 토지매수에 맞서 갖가지 반대운동을 전개하였다. 그러나 조선통치의 사회적·경제적 기반으로서 식민지 지주제의 확립을 옹호한 식민지 권력은 ‘토지가옥증명규칙’(1906년)과 ‘토지가옥소유권증명규칙’(1908년)을 공포하는 등 일련의 토지법제 개정을 통해 일본인 지주의 토지매수를 합법화하고, 최종적으로는 ‘토지조사사업’을 실시하여 토지소유권을 법률적으로 확정지었다.'}], 'id': 'kocommercial_rearrange_sen_0'}\n",
      "\n",
      "50759 \n",
      " {'task': 'Rearrange Word', 'system': '당신은 인공지능 비서입니다. 주어진 지시사항에 따라 가장 적절한 문장을 생성하세요.', 'conversations': [{'from': 'human', 'value': \"이 문제에는 문장에서 공백을 기준으로 나누고, 구두점을 제거한 무작위로 섞인 단어들이 담긴 리스트가 제공됩니다. 이 리스트의 단어를 모두 활용하여 가장 문맥상 적절한 문장을 생성하세요.\\n#단어리스트: ['평형이', '보충되는', '심전도를', '양전하와', '음전하가', '차이가', '조석', '음전하를', '향어의', '이', '내측은', '현상을', '활동은', '활동을', '환경요인인', '심장세포에', '상태별로', '전기적', '세포막', '달라질', '구분하여', '있는', '상태를', '일주기적', '생리생태는', '혈액량좌우', '나타나는', '밝히고자', '이때', '띠고', '것을', '한다', '이루고', '심장의', '야간에', 'Na', '유지되고', '따라', '나며', '체장에', '혈액을', '말하는데', '포함되어', '이온의', '있다본', '주간과', '외측은', '생체전위', '하고', '깨지면서', '유출되는', '관상동맥으로', 'Ca이온이', '양전하를', '필요로', '유입', '있어서', '부터', '외측의', '많은', '있는데', '구명하기', '혈액', '중에', '보이는지를', '삽입하여', '분극', '세포내외의', '심장은', '卞5E位가', '정지', '말한다', '활동에', '균형을', '따라서도', '도출되는', '어떠한', '위하여어체내에', '의해서', '내측의', '간을', '심근', '도출하였는데이들', '상태전기적', '야', '시간', '파형은', '알려져', '등', '전극을', '행동은', '광음전기', '24', '및', '세포내로', '연구는', '속개하기', '것으로', '따른', '주기로', '있고하루', '어류의', '외부', '심박수와', '상태는', '하는', '등에', '심전도', '말하며', '조도수온염분먹이일사량', '전기적으로', '수', '과', '위해서는', '분석하여', 'Ka', '자극인', '변화를', '난다고']\"}, {'from': 'gpt', 'value': \"심전도 파형은 심장의 전기적 활동에 의해서 도출되는 것을 말하는데， 심장의 전기적 활동은 심장세포에 있어서 외측은 전기적으로 양전하를 내측은 음전하를 띠고 있는데， 이때 심장의 정지 상태는 세포막 외측의 양전하와 내측의 음전하가 균형을 이루고 있는 상태（전기적 분극 상태）를 말하며， 심장은 활동을 속개하기 위해서는 많은 혈액량（좌우 관상동맥으로 보충되는 혈액）을 필요로 하고， 이 혈액 중에 포함되어 있는 Ka\\\\ Na： Ca+이온이 심근 세포내로 유입 및 유출되는 것으로 부터 세포내외의 이온의 평형이 유지되고 깨지면서 나타나는 전기적 현상을 말한다. 어류의 생리생태는 외부 자극인 광，음，전기 등 과 환경요인인 조도，수온，염분，먹이，일사량, 조석 등에 따라 달라질 수 있고，하루 24 시간 주기로 하는 일주기적 어류의 행동은 주간과 야간에 차이가 나며， 체장에 따라서도 차이가 난다고 알려져 있다.본 연구는 향어의 체장에 따른 심전도를 구명하기 위하여，어체내에 전극을 삽입하여 주간과 야 간을 구분하여 심전도를 도출하였는데，이들 심전도를 분석하여， 체장에 따른 심박수와 생체전위 （卞§5'『E位）가 상태별로 어떠한 변화를 보이는지를 밝히고자 한다.\"}], 'id': 'kocommercial_rearrange_0'}\n",
      "\n",
      "59872 \n",
      " {'task': 'Akward Correction', 'conversations': [{'from': 'human', 'value': \"주어진 문장 리스트에서 문맥 상 어울리지 않는 문장을 찾으세요.\\n#문장 리스트: ['수학에서 상수란 그 값이 변하지 않는 불변량으로, 변수의 반대말이다. 물리 상수와는 달리, 수학 상수는 물리적 측정과는 상관없이 정의된다.', '수학의 각 분야들은 상업에 필요한 계산을 하기 위해, 숫자들의 관계를 이해하기 위해, 토지를 측량하기 위해, 그리고 천문학적 사건들을 예견하기 위해 발전되어왔다. 이 네 가지 목적은 대략적으로 수학이 다루는 대상인 양, 구조, 공간 및 변화에 대응되며, 이들을 다루는 수학의 분야를 각각 산술, 대수학, 기하학, 해석학이라 한다. 또한 이 밖에도 근대 이후에 나타난 수학기초론과 이산수학 및 응용수학 등이 있다.', '특정 수학 상수, 예를 들면 골롬-딕맨 상수, 프랑세즈-로빈슨 상수, formula_1, 레비 상수같은 상수는 다른 수학상수 또는 함수와 약한 상관관계 또는 강한 상관관계를 갖는다.']\"}, {'from': 'gpt', 'value': '주어진 문장 리스트에서 문맥상 어울리지 않는 문장은 다음과 같습니다.\\n수학의 각 분야들은 상업에 필요한 계산을 하기 위해, 숫자들의 관계를 이해하기 위해, 토지를 측량하기 위해, 그리고 천문학적 사건들을 예견하기 위해 발전되어왔다. 이 네 가지 목적은 대략적으로 수학이 다루는 대상인 양, 구조, 공간 및 변화에 대응되며, 이들을 다루는 수학의 분야를 각각 산술, 대수학, 기하학, 해석학이라 한다. 또한 이 밖에도 근대 이후에 나타난 수학기초론과 이산수학 및 응용수학 등이 있다.\\n\\n나머지 문장들은 모두 이와는 전혀 관련이 없는 문장입니다.'}], 'id': 'mr_tydi_akward_correction_2'}\n",
      "\n",
      "4333 \n",
      " {'id': 'aihub_cnko_society_0', 'conversations': [{'from': 'human', 'value': 'translate to korean(한국어)\\n希望能迈开宝贵的脚步,为孩子们的闪耀梦想加油。'}, {'from': 'gpt', 'value': '부디 귀한 걸음 해주셔서 아이들의 반짝이는 꿈을 응원해 주시면 감사하겠습니다.'}], 'task': 'Translation'}\n",
      "\n",
      "4333 \n",
      " {'id': 'aihub_cnko_tech_0', 'conversations': [{'from': 'human', 'value': '다음 문장을 한국어로 번역하세요.\\n#문장: 为了让顾客不仅可以在卧室使用长寿产业的产品,还可以在客厅使用,扩大了产品群。'}, {'from': 'gpt', 'value': '고객이 장수산업 제품을 침실뿐만 아니라 거실에서도 쓸 수 있도록 제품군을 확대한 것이다.'}], 'task': 'Translation'}\n",
      "\n",
      "2521 \n",
      " {'id': 'aihub_enko_society_0', 'conversations': [{'from': 'human', 'value': '다음 문장을 영어로 번역하세요.\\n#문장: 제2항을 함께 두는 것이 적절한지에 대하여 논의 과정에서 의문이 제기되기도 하였다.'}, {'from': 'gpt', 'value': 'In the course of the discussion, questions were raised as to whether it was appropriate to put Paragraph 2 together.'}], 'task': 'Translation'}\n",
      "\n",
      "2490 \n",
      " {'id': 'aihub_enko_tech_0', 'conversations': [{'from': 'human', 'value': '한글로 번역해줘\\nAll skin corrosion/irritation substances contained in oven cleaners are also subject to severe eye damage/eye irritation.'}, {'from': 'gpt', 'value': '오븐 세척제에 포함된 피부 부식성/자극성 물질은 모두 심한 눈손상성/눈 자극성에도 해당된다.'}], 'task': 'Translation'}\n",
      "\n",
      "4998 \n",
      " {'id': 'aihub_expert_enko_0', 'level': '중', 'conversations': [{'from': 'human', 'value': '영어로 번역해줘\\n통영 용남면 장평리 바닷가 해안도로 앞에 위치한 통영 늘푸른 펜션에 오신 것을 환영합니다.'}, {'from': 'gpt', 'value': \"Welcome to Tongyeong's Evergreen Pension located in front of the coastal road in Jangpyeong-ri, Yongnam-myeon, Tongyeong.\"}], 'task': 'Translation'}\n",
      "\n",
      "5000 \n",
      " {'id': 'aihub_jpko_0', 'conversations': [{'from': 'human', 'value': '한글로 번역해줘\\n先立って2017年に人工降雨実験をした京畿研究院も9回実験の末に「現在の技術で人工降雨を通じた微細粉塵低減効果が大きくない」という結論を下した。'}, {'from': 'gpt', 'value': '앞서 2017년 인공강우 실험을 한 경기연구원도 9차례 실험 끝에 \"현재 기술로 인공강우를 통한 미세먼지 저감 효과가 크지 않다\"는 결론을 내렸다.'}], 'task': 'Translation'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in task_list:\n",
    "    dataset = load_raw_dataset(path)\n",
    "    print(len(dataset), '\\n', dataset[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ac326fe-3c85-4fc3-aed3-b8bd80d34739",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json 1316 \n",
      " {'id': 'gpt_evol_1.3k_0', 'conversations': [{'from': 'human', 'value': '다음 문장을 복수형에서 단수형으로 변환하세요.\\n차들이 밖에 주차되어 있습니다.'}, {'from': 'gpt', 'value': '차가 밖에 주차되어 있습니다.'}]}\n",
      "\n",
      "/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json 6154 \n",
      " {'conversations': [{'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '《聖經語言的語義學》（Semantics of Biblical Language）是詹姆斯·巴爾（James Barr）於1961年出版的一本研究聖經語言的專著。在這部作品中，巴爾對聖經希伯來語和希臘語的語言學特徵進行了深入的探討，他批判了當時盛行的聖經研究方法，主張應該將語言學原理應用於聖經文本的詮釋，以便更準確地理解聖經的意思。\\n\\n巴爾在《聖經語言的語義學》中提出了幾個重要的觀點。首先，他主張語言學和語言學家在聖經研究中的角色應該受到重視。他認為，應該避免將聖經語言學和其他古典語言學區分開來，因為聖經語言和其他古代語言一樣，都有其固有的語法、語義和詞彙特點（第18-23頁）。\\n\\n其次，巴爾批評了當時流行的\"字根意義論\"（root-meaning fallacy），即過分強調單詞的字根在詮釋聖經含義時的作用。他指出，詞義是由語境決定的，而非僅僅由字根決定。巴爾舉了希伯來語單詞 \"כפר\"（kāpar）為例，它在不同的語境下可以表示\"遮蓋\"、\"贖回\"或\"和解\"，而不是僅僅依賴其字根意義（第183-184頁）。\\n\\n第三，巴爾強調了詞彙意義在詮釋聖經時的重要性。他認為，了解單詞的意義和用法對於恰當理解聖經文本至關重要。巴爾引用了希伯來語單詞 \"ברית\"（berîṯ，英文：covenant）和希臘語單詞 \"διαθήκη\"（diathēkē，英文：testament）為例，解釋了這兩個詞在聖經中的不'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '同含義和用法。巴爾認為，了解這些詞語在不同語境下的意義對於理解聖經中的盟約和遺囑概念至關重要（第50-53頁）。\\n\\n第四，巴爾強調語境在詮釋聖經時的重要性。他認為，要理解聖經的含義，就必須考慮到文本的歷史背景、文化環境和語言風格。此外，巴爾提倡從語言學的角度研究聖經，以便更好地理解聖經的結構和風格。他以希伯來詩篇的平行主義（Parallelism）為例，闡述了語言學原理如何有助於揭示詩篇中的意象和修辭結構（第250-252頁）。\\n\\n總之，《聖經語言的語義學》一書對聖經研究領域產生了深遠的影響。詹姆斯·巴爾通過對聖經語言學的深入研究，提出了許多富有洞察力的觀點，尤其是關於語言學在聖經詮釋中的重要性。這些觀點對後來的聖經研究者產生了很大的啟示，使他們在研究聖經時更加重視語言學原理，從而更加準確地理解聖經的含義。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail the \"Introduction\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在《聖經語言的語義學》（Semantics of Biblical Language）一書的導言部分，詹姆斯·巴爾（James Barr）為讀者概述了他的研究目標和方法。巴爾在導言中提到，他希望通過對聖經希伯來語和希臘語的語言學分析，提高聖經研究的品質，並強調語言學在聖經詮釋中的重要性。\\n\\n巴爾首先指出，他的研究目標是研究聖經語言的語義學特點，以及語言學原理如何應用於聖經詮釋。他認為，要真正理解聖經，研究者必須將語言學視為一個重要的工具。巴爾批評了當時流行的聖經研究方法，主要是過分依賴字根意義論和忽略語境的影響，這種方法往往導致對聖經的誤解（第1-3頁）。\\n\\n接著，巴爾闡述了他的研究方法。他強調，語義學研究應該從單詞、句子和語境三個層面入手。首先，研究單詞的意義和用法；其次，分析句子結構和語法；最後，將文本置於歷史和文化語境中加以詮釋。巴爾認為，這種方法有助於在聖經研究中達到更高的準確性和深度（第3-4頁）。\\n\\n此外，巴爾在導言中提到了他的一些研究成果。他指出，在聖經中，很多詞語的意義和用法與其在其他古代語言中的意義和用法相似，這意味著聖經語言學應該與其他古典語言學緊密相連，而非孤立存在。此外，巴爾強調，對聖經的詮釋應該基於對文本的仔細閱讀和分析，而非僅僅依賴傳統的觀點'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '和信仰立場（第5-6頁）。\\n\\n巴爾在導言中也提到了他對一些經典語言學理論的看法。例如，他對結構主義（Structuralism）和生成語法學（Generative Grammar）進行了簡要評論。巴爾認為，雖然這些理論在語言學領域具有一定的影響力，但它們在聖經語言學研究中的適用性有限。他主張，聖經語言學應該發展自己的理論體系，並將語言學原理與聖經研究相結合，以便更好地理解聖經文本（第7-8頁）。\\n\\n最後，巴爾總結了導言的內容，並為讀者展望了全書的主要章節和結構。他表示，全書將從語義學原理入手，探討聖經語言的詞彙、語法和語境特點，並提供一系列具體的聖經文本案例，以說明如何將語言學原理應用於聖經詮釋。巴爾希望，通過這部作品，讀者能夠更好地理解聖經語言學的重要性，並將這些原理應用於自己的研究和實踐中（第8-9頁）。\\n\\n總之，在《聖經語言的語義學》一書的導言部分，詹姆斯·巴爾為讀者提供了一個研究框架，闡述了他的研究目標、方法和主要觀點。他強調了語言學在聖經研究中的重要性，並提出了一種新的聖經詮釋方法，該方法將語言學原理與聖經研究相結合，以便更好地理解聖經文本。這些觀點為後來的聖經研究者提供了新的研究視角，並對聖經研究領域產生了深遠的影響。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail Chapter Two \"Words and Their Meanings\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書中，第二章節題為“詞語及其意義”（Words and Their Meanings），主要探討了單詞在聖經語言中的意義和用法。巴爾在本章中指出，要準確理解聖經文本，就必須掌握聖經詞語的意義和用法。他通過對希伯來語和希臘語單詞的分析，揭示了聖經語言的語義特點，並提出了一些重要的語言學原理。\\n\\n巴爾首先闡述了單詞意義的重要性。他認為，單詞是語言的基本單位，理解單詞的意義對於恰當詮釋聖經文本至關重要。巴爾指出，單詞的意義是多層次的，它可以包括字面意義、隱喻意義和典型意義等。因此，在分析聖經詞語時，應該考慮到這些不同層面的意義，以免對聖經產生誤解（第27-30頁）。\\n\\n接著，巴爾討論了單詞意義和語境之間的關係。他強調，詞語的意義是由語境決定的，而非僅僅由字根決定。巴爾批評了當時流行的字根意義論，指出這種方法忽略了語境對詞義的影響，容易導致對聖經的誤解。他以希伯來語單詞 \"רעה\"（rā‘āh，英文：to shepherd）為例，說明了該詞在不同語境下可能具有不同的意義，如“餵養”、“引導”或“統治”（第40-42頁）。\\n\\n此外，巴爾探討了詞彙意義的歷史變遷。他指出，單詞的意義會隨著時間的推移而發生變化，因此在分析聖經詞語時，應該充分考慮到這一點。巴爾以希臘語單詞 \"ἐ'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': 'κκλησία\"（ekklēsía，英文：church）為例，說明了該詞在古典希臘語和新約聖經中的意義變化。在古典希臘語中，它指的是一個政治性的集會，而在新約聖經中，它指的是基督教的教會（第52-53頁）。\\n\\n巴爾還分析了多義詞在聖經語言中的現象。他認為，理解多義詞的不同意義對於正確詮釋聖經文本非常重要。巴爾以希伯來語單詞 \"שָׁלוֹם\"（šālôm，英文：peace）為例，說明了該詞在不同語境下具有不同的意義，如“和平”、“安全”或“完整”等（第60-62頁）。\\n\\n最後，巴爾探討了單詞在聖經語言中的詞彙關係。他指出，詞彙關係是理解單詞意義的重要線索，通過分析同義詞、反義詞和搭配詞等詞彙關係，可以更好地理解聖經詞語的意義和用法。巴爾以希伯來語單詞 \"צַדִּיק\"（ṣaddîq，英文：righteous）和\"רָשָׁע\"（rāšā‘，英文：wicked）為例，說明了這兩個詞在聖經中是反義詞的關係，理解這一點對於詮釋聖經文本具有重要意義（第71-72頁）。\\n\\n總之，在《聖經語言的語義學》一書的第二章“詞語及其意義”中，詹姆斯·巴爾深入探討了單詞在聖經語言中的意義和用法。他通過對希伯來語和希臘語單詞的分析，揭示了聖經語言的語義特點，並提出了一些重要的語言學原理，如考慮語境、關注詞彙關係和重視歷史變遷等。這些原理對於後來的聖經研'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '究者來說提供了新的研究視角，對聖經研究領域產生了深遠的影響。巴爾的這一章節為讀者提供了一個有力的工具，幫助他們更好地理解聖經詞語的意義，從而能夠更加精確地詮釋聖經文本。這一章節也為後續章節奠定了基礎，後續章節將繼續探討語法結構和語境分析在聖經語言學研究中的應用，以及語言學如何幫助讀者更好地理解聖經的神學觀念和教義。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail Chapter Three \"The Language of the Bible\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書中，第三章節題為“聖經的語言”（The Language of the Bible），主要探討了聖經語言的特點及其與其他古代語言的區別。巴爾在本章中指出，要深入理解聖經文本，就必須了解聖經語言的特色。他從語法、詞彙和語境等方面對聖經語言進行了深入分析，並提出了一些重要的語言學原理。\\n\\n首先，巴爾分析了聖經語言的語法特點。他指出，希伯來語和希臘語都具有獨特的語法結構，這些結構對於理解聖經文本的意義至關重要。例如，希伯來語的動詞時態和語態系統與英語等其他語言有很大不同，因此在翻譯和詮釋聖經文本時，需要特別注意這些差異（第87-90頁）。\\n\\n接著，巴爾討論了聖經語言的詞彙特點。他指出，聖經語言中有許多專有名詞和固定搭配，這些詞語在聖經中具有特定的意義和用法。例如，希伯來語詞語 \"הַכֹּהֵן\"（hakkōhēn，英文：the priest）和 \"הַמֶּלֶךְ\"（hamelek，英文：the king）在聖經中分別指代祭司和以色列國王，理解這些詞語的特殊含義對於詮釋聖經文本非常重要（第95-98頁）。\\n\\n此外，巴爾探討了聖經語言的語境特點。他認為，聖經文本具有豐富的文化背景和歷史背景，這些背景對於理解聖經詞語的意義和用法至關重要。例如，瞭解古代以色列人的宗教信仰和社會制度，可以幫助我們更好地理解聖經中的'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '神學觀念和敘事情節（第105-107頁）。\\n\\n巴爾還強調了聖經語言在文體方面的多樣性。他指出，聖經包含了各種不同的文體，如史詩、詩歌、法律、譬喻和預言等，這些文體在語言風格和表達方式上有很大差異。因此，在分析聖經文本時，應該充分考慮到文體特點，以避免對聖經產生誤解。例如，希伯來詩歌的平行主義（parallelism）是詩歌文體的一個重要特點，它通過重複或對比的句子結構來強調某一主題或思想，理解這一特點對於詮釋詩篇等聖經詩歌非常重要（第112-115頁）。\\n\\n最後，巴爾總結了聖經語言與其他古代語言之間的區別。他指出，雖然聖經語言在很多方面與其他古代近東語言相似，但它仍具有獨特的特點，這些特點在很大程度上反映了聖經作者的宗教信仰和神學觀念。因此，在研究聖經語言時，應該關注其獨特性，並將其與其他古代語言進行比較，以便更好地理解聖經文本的意義和特色（第120-122頁）。\\n\\n總之，在《聖經語言的語義學》一書的第三章“聖經的語言”中，詹姆斯·巴爾對聖經語言的特點進行了全面而深入的分析。他從語法、詞彙、語境和文體等方面揭示了聖經語言的獨特性，並提出了一些重要的語言學原理，如關注文化背景、比較古代語言和考慮文體特點等。這些原理對於後來的聖經研究者來說具有指導意義，有助於他們更加精確地詮釋和理解聖'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail Chapter Four \"The Meaning of Words in the Bible\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書中，第四章節題為“聖經中詞語的意義”（The Meaning of Words in the Bible），主要探討了聖經詞語的意義以及如何確定其意義的方法。巴爾在本章中指出，要深入理解聖經文本，就必須了解聖經詞語的意義及其如何在不同語境中發生變化。\\n\\n首先，巴爾強調了語境對確定詞語意義的重要性。他認為，在分析聖經詞語的意義時，應該充分考慮到詞語在特定語境中的用法。他以希伯來詞 \"רוּחַ\"（rûaḥ，英文：spirit, wind, breath）為例，說明了該詞在不同語境下可以表示不同的意義，如神的靈、風或生命氣息等（第133-134頁）。\\n\\n接著，巴爾討論了詞義變化的現象。他指出，聖經詞語的意義可能會隨著時間和語境的變化而發生變化。因此，在研究聖經詞語的意義時，應該關注其歷史變遷，以避免對聖經產生誤解。巴爾以希臘詞 \"ἐκκλησία\"（ekklēsía，英文：church）為例，說明了該詞在古典希臘語和新約聖經中的意義變化。在古典希臘語中，它指的是一個政治性的集會，而在新約聖經中，它指的是基督教的教會（第142-143頁）。\\n\\n此外，巴爾探討了詞彙關係對確定詞語意義的作用。他認為，通過分析同義詞、反義詞和搭配詞等詞彙關係，可以更好地理解聖經詞語的意義和用法。巴爾以希伯來詞 \"צַדִּיק\"（ṣaddîq，英文：righteous）和'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '\"רָשָׁע\"（rāšā‘，英文：wicked）為例，說明了分析反義詞可以幫助我們更好地理解這些詞語在聖經中的意義（第148-149頁）。\\n\\n最後，巴爾指出了在確定詞語意義時應避免的語言學陷阱。例如，他警告讀者不要過分依賴詞根（etymology）來確定詞語的意義，因為詞根往往無法反映詞語在特定語境中的真實含義。此外，他還提醒讀者要注意歧義（ambiguity）問題，因為某些詞語在不同語境中可能具有多重含義，如希伯來詞 \"אֱלֹהִים\"（’ĕlōhîm，英文：God, gods），既可以指代獨一無二的上帝，也可以指代其他神祇（第154-157頁）。\\n\\n總之，在《聖經語言的語義學》一書的第四章“聖經中詞語的意義”中，詹姆斯·巴爾對如何確定聖經詞語意義提出了一系列實用的方法和建議，如關注語境、分析詞彙關係、避免詞根誤導和注意歧義等。這些方法和建議對於後來的聖經研究者來說具有指導意義，有助於他們更加精確地詮釋和理解聖經文本。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail Chapter Five \"Semantic Fields and Lexicography\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書中，第五章節題為“語義場和詞典編纂學”（Semantic Fields and Lexicography），主要探討了如何通過分析語義場（semantic fields）來確定聖經詞語的意義，以及詞典編纂學在聖經研究中的應用。\\n\\n首先，巴爾介紹了語義場的概念。語義場是指一組具有相關意義的詞語組成的詞彙系統。通過分析語義場，研究者可以更好地理解聖經詞語在特定語境中的含義和用法。巴爾以希伯來詞 \"אַהֲבָה\"（’ahăbāh，英文：love）為例，說明了如何通過分析與之相關的詞語（如 \"חֶסֶד\"，ḥesed，英文：loyal love, kindness）來確定其在聖經中的意義（第165-166頁）。\\n\\n接著，巴爾討論了詞典編纂學在聖經研究中的重要性。他認為，編纂專門針對聖經詞語的詞典是研究聖經語言的重要工具，可以幫助研究者更好地理解聖經文本。然而，巴爾也指出，現有的聖經詞典存在一些問題，如過分依賴詞根分析、忽視語境和詞彙關係等。因此，他呼籲後來的研究者在編纂聖經詞典時應該克服這些問題，以提高詞典的科學性和可靠性（第170-174頁）。\\n\\n此外，巴爾還強調了語言學方法在詞典編纂學中的應用。他認為，在編纂聖經詞典時，應該運用語言學的原理和方法，如語義場分析、詞彙關係研究和語境考察等，以確保詞典的編纂工作更加科學'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '和系統。例如，巴爾建議，在編纂聖經詞典時，應該將聖經詞語與其同義詞、反義詞和搭配詞等相關詞語進行比較，以揭示其在特定語境中的真實含義（第178-179頁）。\\n\\n總之，在《聖經語言的語義學》一書的第五章“語義場和詞典編纂學”中，詹姆斯·巴爾深入探討了語義場分析和詞典編纂學在確定聖經詞語意義方面的重要性和應用。他強調了語言學方法在詞典編纂工作中的重要作用，並呼籲後來的研究者克服現有聖經詞典的不足，編纂出更加科學、可靠的聖經詞典，以促進聖經研究的發展。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail Chapter Six \"Metaphor and Figurative Language\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書中，第六章節題為“隱喻和修辭語言”（Metaphor and Figurative Language），探討了聖經中隱喻和其他修辭手法的使用，以及如何理解和詮釋這些修辭語言。\\n\\n巴爾指出，聖經中充滿了各種形式的隱喻和修辭語言，這些修辭手法有助於傳達信息、強調主題，並增強文本的表現力。為了更好地理解聖經中的隱喻和修辭語言，巴爾強調研究者應該關注語言的形式、功能和語境（第187-188頁）。\\n\\n巴爾以希伯來詞 \"בְּרִית\"（berît，英文：covenant）為例，說明了隱喻在聖經中的應用。\"בְּרִית\" 在某些語境中被用作隱喻，表示上帝與人類之間的約定或關係。通過分析這個隱喻，研究者可以更好地理解上帝與人類之間的關係（第192-193頁）。\\n\\n除了隱喻之外，巴爾還討論了其他修辭手法在聖經中的應用，如擬人（personification）、託寓（allegory）和象徵（symbolism）等。例如，他提到了以賽亞書（Isaiah）中的擬人修辭，描述耶路撒冷（Jerusalem）被稱為悲傷的寡婦，以強調其荒涼的狀態（第202-203頁）。\\n\\n巴爾還強調了修辭語言在詮釋聖經時的重要性。他認為，研究者在解讀聖經文本時，應該充分考慮到修辭手法的作用，避免過於字面地理解聖經語言。此外，他建議研究者運用語言學原理和方法，如隱喻分析和語境考察等，以確保對聖經修辭語言的正確理解和詮釋（'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '第210-211頁）。\\n\\n總之，在《聖經語言的語義學》一書的第六章“隱喻和修辭語言”中，詹姆斯·巴爾對聖經中隱喻和其他修辭手法進行了深入探討，強調了修辭語言在傳達信息和強化文本表現力方面的重要作用。此外，他還指出了研究者在解讀聖經文本時應該充分考慮到修辭手法的影響，並運用語言學原理和方法來確保對聖經修辭語言的正確理解和詮釋。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail Chapter Seven \"Idiom and Colloquial Language\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書中，第七章節題為“成語和口語語言”（Idiom and Colloquial Language），主要探討了聖經中成語和口語語言的特點及其對聖經詮釋的影響。\\n\\n巴爾指出，聖經語言中包含大量的成語（idiom）和口語表達（colloquial expression），這些語言現象具有特殊的語法結構和語義特徵，不能按字面意思理解。因此，在詮釋聖經時，研究者需要充分了解這些成語和口語表達的特點，以避免誤解聖經文本（第217-218頁）。\\n\\n巴爾舉了一些成語和口語表達的例子，如希伯來語中的 \"בְּטֶן\"（bᵉṭen，英文：belly），在某些語境中表示“內心”或“情感”（第220頁）。另一個例子是 \"יָד\"（yād，英文：hand），在某些語境中表示“權力”或“控制”（第221頁）。\\n\\n此外，巴爾還強調了口語語言在聖經中的重要作用。他認為，口語語言反映了聖經時代人們的日常語言和生活，對於瞭解當時社會文化背景具有重要價值。因此，在研究聖經語言時，研究者應該重視口語語言的研究，並將其納入聖經詞典和語法手冊的編纂工作中（第226-227頁）。\\n\\n總之，在《聖經語言的語義學》一書的第七章“成語和口語語言”中，詹姆斯·巴爾對聖經中成語和口語語言進行了深入分析，強調了這些語言現象在聖經詮釋和語言研究中的重要性。他呼籲研究者在解讀聖經時應充分了解成語和口語表達的特點，'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '以避免誤解聖經文本，並將對成語和口語語言的研究納入聖經詞典和語法手冊的編纂工作中。\\n\\n巴爾還指出，研究者應該運用語言學方法和理論，對成語和口語表達進行深入分析，以揭示其在不同語境中的意義和功能。此外，他強調，研究者在研究聖經成語和口語語言時，應該注意語言變異和歷史演變的影響，以確保對聖經語言的全面瞭解和詮釋（第230-231頁）。\\n\\n通過對成語和口語語言的深入研究，詹姆斯·巴爾在本章中為聖經研究者提供了有益的指導和建議，有助於提高聖經詮釋的準確性和深度。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail Chapter Eight \"Conclusion\" of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書中，第八章節題為“結論”（Conclusion），主要總結了本書的主要觀點和研究成果。\\n\\n首先，巴爾指出，聖經語言學的研究應該超越傳統的語法和詞彙層面，著重探討語言的語義層面，以揭示聖經文本的真正意義和信息（第235頁）。他認為，語義學方法和理論對於研究聖經語言具有重要價值，可以幫助研究者揭示文本背後的深層結構和隱含意義。\\n\\n其次，巴爾強調了對聖經語言中的修辭手法、成語和口語表達等特殊語言現象的重要性。他呼籲研究者在解讀聖經時應充分了解這些語言現象的特點，並運用語言學方法和理論進行深入分析（第236-237頁）。\\n\\n此外，巴爾還提出了一些建議和展望。他認為，聖經語言學的研究應該與現代語言學的發展緊密結合，並積極借鑒語言學的方法和成果。同時，他期望未來的聖經研究能夠重視語言變異和歷史演變的影響，以確保對聖經語言的全面瞭解和詮釋（第238-239頁）。\\n\\n總之，在《聖經語言的語義學》一書的結論部分，詹姆斯·巴爾總結了本書的主要觀點和研究成果，強調了語義學方法和理論在聖經研究中的重要性，並對未來的聖經語言學研究提出了建議和展望。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, summarize in detail the main premise of James Barr\\'s book \"Semantics of Biblical Language\". Provide examples supplied in the book and give corresponding page numbers wherever applicable.'}, {'from': 'gpt', 'value': '詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）一書的主要前提是強調研究聖經語言時，應該超越傳統的語法和詞彙層面，著重探討語言的語義層面，以揭示聖經文本的真正意義和信息。巴爾認為，語義學方法和理論對於研究聖經語言具有重要價值，可以幫助研究者揭示文本背後的深層結構和隱含意義（第9-10頁）。\\n\\n巴爾在書中提出了幾個主要的觀點和方法。首先，他指出，研究者在研究聖經語言時，應該注意到單詞的多重意義和語境對語義的影響（第70-71頁）。其次，他強調了對聖經語言中的修辭手法、成語和口語表達等特殊語言現象的重要性，呼籲研究者在解讀聖經時應充分了解這些語言現象的特點（第118頁，第217-218頁）。\\n\\n此外，巴爾還探討了語義場（semantic field）的概念在聖經語言學中的應用，強調單詞在語義場中的相互關係對於理解聖經語言具有重要價值（第171-172頁）。他還指出，研究者在研究聖經語言時，應該注意語言變異和歷史演變的影響，以確保對聖經語言的全面瞭解和詮釋（第230-231頁）。\\n\\n總之，《聖經語言的語義學》一書的主要前提是語義學方法和理論在聖經研究中的重要性。詹姆斯·巴爾通過對聖經語言的深入分析，為研究者提供了有益的指導和建議，有助於提高聖經詮釋的準確性和深度。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，翻譯以下資料。為所有標題或專用字詞提供對應的英語字詞：The main premise of James Barr\\'s \"The Semantics of Biblical Language\" is that a nuanced and contextualized understanding of the language used in the Bible is essential for interpreting its meaning accurately. Barr argues that earlier approaches to biblical language were overly simplistic and literal, failing to account for the complexity and ambiguity of the text. He emphasizes the importance of understanding the historical and cultural context of the Bible, as well as the literary and linguistic conventions of the time, in order to properly understand the meanings of words and phrases. Barr\\'s overarching argument is that the study of biblical language must take into account the fluidity and contextuality of language itself, and that a more sophisticated approach to biblical language can lead to more accurate and insightful interpretations of the text.'}, {'from': 'gpt', 'value': '詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（The Semantics of Biblical Language）的主要前提是，要準確地解釋聖經的意義，對聖經中使用的語言進行細緻且具有語境的理解至關重要。巴爾認為，早期對聖經語言的研究方法過於簡單和直接，未能充分考慮到文本的複雜性和歧義性。他強調理解聖經的歷史和文化背景，以及當時的文學和語言慣例的重要性，以便正確理解單詞和短語的含義。巴爾的總體論點是，研究聖經語言必須考慮到語言本身的流動性和語境性，而對聖經語言採取更精緻的方法可以導致對文本的更準確和有洞察力的解釋。'}, {'from': 'human', 'value': \"採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Cite examples from the same book (with page numbers) to illustrate this: Barr's overarching argument is that the study of biblical language must take into account the fluidity and contextuality of language itself, and that a more sophisticated approach to biblical language can lead to more accurate and insightful interpretations of the text.\"}, {'from': 'gpt', 'value': '詹姆斯·巴爾（James Barr）在其著作《聖經語言的語義學》（The Semantics of Biblical Language）中，提供了一些例子來支持他的總體論點：研究聖經語言必須考慮到語言本身的流動性和語境性，而對聖經語言採取更精緻的方法可以導致對文本的更準確和有洞察力的解釋。\\n\\n1. 在第70-71頁，巴爾舉例說明了單詞的多重意義和語境對語義的影響。他提到了希伯來詞彙 \"רַב\"（rab），它可以根據語境有不同的意義，如 \"多\"（many），\"偉大\"（great）或 \"領袖\"（leader）。\\n2. 在第118頁，巴爾強調了對聖經語言中的修辭手法、成語和口語表達等特殊語言現象的重要性。他以希伯來詞彙 \"בֵּן־אָדָם\"（ben-\\'adam）為例，它的直譯是 \"人的兒子\"（son of man），但在特定語境下，它實際上表示 \"人類\"（human beings）。\\n3. 在第171-172頁，巴爾探討了語義場（semantic field）的概念在聖經語言學中的應用。他提到了希伯來詞彙 \"כֹּהֵן\"（kohen）和 \"לֵוִי\"（levi）兩個詞，它們都可以翻譯為 \"祭司\"（priest），但在語義場中，它們具有不同的意義。\"כֹּהֵן\"（kohen）主要指的是亞倫的後裔，而 \"לֵוִי\"（levi）指的是利未支派的成員。\\n\\n通過這些例子，巴爾強調了語言的流動性和語境性對於研究聖經語言的重要性，並主張採用更精緻的方法可以提高對聖經文本的解釋準確性和深度。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Summarize in detail the section entitled \"The limitations of earlier approaches to biblical language\" in the \"Introduction\" of the book. Provide examples given from within the same section (with page numbers).'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（The Semantics of Biblical Language）一書的「導言」（Introduction）中，有一節名為「早期聖經語言研究方法的局限性」（The limitations of earlier approaches to biblical language）。在這一節中，巴爾詳細闡述了以往對聖經語言的研究方法存在的一些問題。\\n\\n巴爾指出，早期的聖經語言研究方法往往過於簡單和直接，未能充分考慮到文本的複雜性和歧義性。他認為，這些方法對聖經文本的解讀往往僅僅停留在字面層面，忽略了語言的流動性、歷史和文化背景等多方面因素。以下是一些具體的例子：\\n\\n1. 字根誤解（Root fallacy）：巴爾在第4頁上指出，早期學者們經常將一個詞的意義直接歸結為其字根的意義，這種方法忽略了語言的變化和詞義的演變。他舉例說明，希伯來詞彙 \"כִּפֶּר\"（kipper）是一個與 \"贖罪\"（atonement）相關的詞彙，然而將其意義僅僅歸結為字根 \"כפר\"（kpr）是不準確的，因為該詞在不同語境中有不同的意義。\\n2. 單一意義錯誤（Illegitimate totality transfer）：巴爾在第4-5頁上指出，早期學者們經常將一個詞在所有情況下都賦予相同的意義，而忽略了語境對詞義的影響。例如，希伯來詞彙 \"רוּחַ\"（ruach）可以表示 \"風\"（wind）、\"氣息\"（breath）或 \"靈\"（spirit），取決於不同的語境。\\n3. 翻譯傳統（Translation tradition）：巴爾在第6頁上指出，早期的聖經翻譯往往受到特定的翻譯傳統的影響，這可能導致文本的原意被曲解'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '或失真。舉例來說，希伯來詞彙 \"שָׁלוֹם\"（shalom）通常被翻譯為 \"和平\"（peace），然而，這種翻譯不能充分體現該詞在希伯來語境中的豐富意義，例如 \"完整\"（wholeness）和 \"福祉\"（well-being）等。\\n\\n1. 語言變異的忽略（Ignoring language variation）：巴爾在第6-7頁上提到，早期學者們經常忽略希伯來語和希臘語在聖經中的不同時期、地區和語境中的變異。這可能導致對某些詞語和表達方式的誤解。例如，希臘詞彙 \"ἀγάπη\"（agape）在新約聖經中常被理解為 \"神的愛\"（divine love），然而在希臘化時期的文獻中，該詞卻有著與 \"友誼\"（friendship）相關的意義。\\n\\n綜上所述，詹姆斯·巴爾在這一節中揭示了早期聖經語言研究方法的諸多局限，強調了語言流動性和語境對於準確解讀聖經文本的重要性。他主張，學者們需要採用更加徹底和細緻的方法來研究聖經語言，以便更好地理解和詮釋聖經的意義。'}, {'from': 'human', 'value': \"採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, list out the main contributions of Barr's book. For each contribution, provide examples given from within the relevant section of the book (with page numbers).\"}, {'from': 'gpt', 'value': '詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）對聖經語言研究作出了以下主要貢獻：\\n\\n1. 揭示早期研究方法的局限性：巴爾詳細闡述了以往對聖經語言的研究方法存在的一些問題，包括字根誤解、單一意義錯誤、翻譯傳統和語言變異的忽略等（導言，第4-7頁）。\\n2. 強調語境和歷史文化背景的重要性：巴爾主張，要準確解讀聖經文本，必須充分考慮語境以及聖經的歷史和文化背景。他以希伯來詞彙 \"רוּחַ\"（ruach）為例，說明了該詞在不同語境中有不同的意義，如 \"風\"（wind）、\"氣息\"（breath）或 \"靈\"（spirit）（第二章，第27-28頁）。\\n3. 提倡語義場（Semantic fields）的概念：巴爾提出語義場這一概念，以解釋聖經語言中的詞彙如何在一個相關詞彙集合的語境中相互影響。他以希伯來動詞 \"חטא\"（chata）為例，說明了這個與 \"罪\"（sin）相關的動詞在不同語境中的意義（第五章，第112-113頁）。\\n4. 分析隱喻和修辭手法：巴爾對聖經中的隱喻和修辭手法進行了深入的分析，說明了理解這些語言特徵對於準確解讀聖經文本的重要性。他舉例說明了希伯來詞彙 \"בְּרִית\"（berith），指出這個詞在聖經中表示 \"契約\"（covenant）的隱喻意義（第六章，第152-153頁）。\\n5. 闡明成語和口語語言的特點：巴爾對聖經中的成語和口語語言進行了研究，指出這些語言特徵在解讀聖經時的重要性'}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '。他以希伯來成語 \"ידע טוב ורע\"（yada tov vara）為例，說明了這個成語在創世記中的意義是 \"辨別善惡\"（to know good and evil），而不僅僅是字面上的 \"知道善惡\"（knowing good and evil）（第七章，第186-187頁）。\\n\\n總之，詹姆斯·巴爾的《聖經語言的語義學》一書對聖經語言研究做出了重要貢獻，提倡徹底和細緻的方法，強調語境、歷史文化背景、語義場、隱喻修辭手法及成語口語語言的重要性。他的研究為後來的聖經學者們提供了深刻的見解，有助於更精確和深入地解讀和詮釋聖經文本。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, present other biblical scholars\\' critiques to Barr\\'s \"Semantics of Biblical Language\". For each critique cited, provide examples given from within the relevant work (with bibliographic information and page numbers).'}, {'from': 'gpt', 'value': '詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）在學術界引起了廣泛的討論和評價。以下是一些其他聖經學者對巴爾的批評：\\n\\n1. 約翰·G·Goldingay（John Goldingay）批評巴爾的研究過於關注詞彙層面，而忽略了語法結構對聖經文本意義的影響。Goldingay 強調，在解讀聖經時，要充分考慮詞彙、語法和語境之間的相互作用。\\n\\n參考文獻：Goldingay, J. (1987). Approaches to Old Testament Interpretation. Downers Grove, IL: InterVarsity Press.（第45-47頁）\\n\\n1. 沃爾特·布魯根曼（Walter Brueggemann）認為，巴爾對聖經語言的研究僅僅集中在語言層面，未能充分關注文本背後的意識形態和神學思想。布魯根曼主張，對聖經的解讀應該更加關注其社會、政治和宗教背景。\\n\\n參考文獻：Brueggemann, W. (1997). Theology of the Old Testament: Testimony, Dispute, Advocacy. Minneapolis, MN: Fortress Press.（第9-10頁）\\n\\n1. 伊利沙·格林斯坦（Elisha Qimron）批評巴爾對希伯來語死海古卷（Dead Sea Scrolls）的研究存在疏漏。格林斯坦指出，死海古卷提供了重要的語言學資料，有助於更全面地了解聖經語言的語義學特點。\\n\\n參考文獻：Qimron, E. (1986). The Hebrew of the Dead Sea Scrolls. Atlanta, GA: Scholars Press.（第23-25頁）\\n\\n儘管有這些批評，詹姆斯·巴爾的《聖經語言的語義學》仍然被認為是聖經語言研究的重要奠基之作。'}, {'from': 'human', 'value': 'Any other scholarly critiques worthy of mentioning?'}, {'from': 'gpt', 'value': '是的，還有其他學者對詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）提出了批評：\\n\\n1. 莫伊塞斯·席拉（Moisés Silva）指出，巴爾的方法可能會導致忽視聖經文本內部的神學連貫性。席拉認為，在關注語境和歷史背景的同時，解釋者應該注意到文本在整個聖經中的位置，以及其與其他經文之間的關聯。\\n\\n參考文獻：Silva, M. (1996). Biblical Words and Their Meaning: An Introduction to Lexical Semantics (Revised and Expanded Edition). Grand Rapids, MI: Zondervan.（第29-31頁）\\n\\n1. 德里克·基德納（Derek Kidner）認為，巴爾過於著重於反對過時的詞彙研究方法，導致他對一些詞彙的解釋過於懷疑。基德納強調，在批判過去研究的不足之余，學者應該給予一定程度的尊重，以保持對聖經文本的敬意。\\n\\n參考文獻：Kidner, D. (1973). Psalms 1-72: An Introduction and Commentary. Downers Grove, IL: InterVarsity Press.（第15-16頁）\\n\\n1. 羅伯特·艾特金（Robert Alter）則認為，巴爾對聖經文學風格的分析不夠深入。艾特金主張，聖經的敘事風格和詩歌語言在很大程度上取決於其文學特徵，因此，研究聖經語言的語義學應該充分考慮到這些特點。\\n\\n參考文獻：Alter, R. (1981). The Art of Biblical Narrative. New York, NY: Basic Books.（第18-19頁）\\n\\n儘管存在這些批評，詹姆斯·巴爾的《聖經語言的語義學》在聖經研究領域仍具有重要意義，為後來的學者提供了理論框架和研究方法。'}, {'from': 'human', 'value': 'Still, any other scholarly critiques worthy of mentioning?'}, {'from': 'gpt', 'value': \"當然，還有其他學者對詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）提出了批評：\\n\\n1. 馬克·史密斯（Mark S. Smith）認為，巴爾在分析聖經中上帝（Yahweh）名稱的語義時過於簡化。他指出，巴爾對上帝名稱的歷史背景和起源的研究不夠全面，忽略了一些重要的近東古代文獻。史密斯認為，在研究聖經中上帝名稱的語義時，應該更加注重比較文獻和跨文化的分析。\\n\\n參考文獻：Smith, M. S. (2001). The Origins of Biblical Monotheism: Israel's Polytheistic Background and the Ugaritic Texts. Oxford: Oxford University Press.（第153-154頁）\\n\\n1. 埃德溫·古德（Edwin M. Good）對巴爾在《聖經語言的語義學》中對詩篇的分析提出了質疑。古德認為，巴爾過於關注字面意義，而忽略了詩篇在詩歌體裁中的修辭特徵。古德主張，在研究聖經詩歌的語義學時，應該更加關注詩歌的形式和結構特點。\\n\\n參考文獻：Good, E. M. (1990). Irony in the Old Testament. Sheffield: Sheffield Academic Press.（第17-18頁）\\n\\n1. 弗朗西斯·沃森（Francis Watson）對巴爾的方法提出批評，認為巴爾過於依賴當時的語言學理論，而忽略了聖經文本自身的特點。沃森提倡將更多注意力放在聖經文本的神學主題上，而不僅僅是語言學方法。\\n\\n參考文獻：Watson, F. (1986). Text, Church and World: Biblical Interpretation in Theological Perspective. Edinburgh: T&T Clark.（第34-35頁）\\n\\n儘管存在這些批評，詹姆斯·巴爾的《聖經語言的語義學》在聖經研究領域仍具有重要意義，為後來\"}, {'from': 'human', 'value': '繼續'}, {'from': 'gpt', 'value': '的聖經學者提供了一個重要的理論基礎。儘管有一些爭議，但巴爾的觀點對於聖經研究的發展起到了推動作用，使後來的學者更加關注語境、文化背景和語言特點。因此，即使有批評，詹姆斯·巴爾的《聖經語言的語義學》仍然是聖經學術界一部具有指標性的著作。'}, {'from': 'human', 'value': 'Still, any other scholarly critiques worthy of mentioning?'}, {'from': 'gpt', 'value': '當然，還有其他學者對詹姆斯·巴爾（James Barr）的《聖經語言的語義學》（Semantics of Biblical Language）提出了批評：\\n\\n1. 菲利普·R·戴維斯（Philip R. Davies）質疑了巴爾對於聖經原文版本之間差異的處理。他認為，巴爾在分析聖經語言時未充分考慮希伯來文和希臘文標準版聖經之間的差異。戴維斯認為，這種差異可能對解讀聖經文本產生重要影響，因此需要更加細緻地研究。\\n\\n參考文獻：Davies, P. R. (1989). The Damascus Covenant: An Interpretation of the \"Damascus Document\". Sheffield: Sheffield Academic Press.（第47-48頁）\\n\\n1. 約瑟夫·布蘭奇（Joseph A. Fitzmyer）對巴爾在處理新約聖經希臘語方面的方法提出了質疑。他認為，巴爾過於專注於希伯來語和希臘語的比較研究，而忽略了新約聖經希臘語的特點和背景。布蘭奇建議，研究新約聖經語言時，應更加關注新約時期希臘語的特色。\\n\\n參考文獻：Fitzmyer, J. A. (1994). Essays on the Semitic Background of the New Testament. Sheffield: Sheffield Academic Press.（第28-29頁）\\n\\n以上是對詹姆斯·巴爾《聖經語言的語義學》的一些學術批評。雖然這些批評指出了巴爾在某些方面的不足，但他的著作對聖經研究領域的影響仍然不可忽視。巴爾對聖經語言學研究的貢獻激發了後續學者對聖經文本的進一步探討，使得聖經研究方法得以不斷完善。'}, {'from': 'human', 'value': '採用優雅現代中文，用中文繁體字型，回答以下問題。為所有標題或專用字詞提供對應的英語翻譯：Using scholarly style, list out in what ways James Barr\\'s proposal in \"Semantics of Biblical Language“ has direct contact with the basic elements of lexical semantics. For each ontact point, provide examples given from within the relevant section of the book (with page numbers).'}, {'from': 'gpt', 'value': '詹姆斯·巴爾（James Barr）在《聖經語言的語義學》（Semantics of Biblical Language）一書中提出的觀點，與詞彙語義學（lexical semantics）的基本元素有以下幾個直接接觸點：\\n\\n1. 詞彙的多義性（Polysemy）：巴爾強調，聖經中的詞彙可能具有多種意義，因此需要在語境中分析其涵義。例如，他在第二章（第32-33頁）討論了希伯來詞彙 \"רְאוּמָה\"（r\\'umah），指出其在不同語境中可能具有不同的意義。\\n2. 詞彙關係（Lexical Relations）：巴爾在第五章（第132-133頁）探討了語義場（Semantic Fields）的概念，說明了聖經詞彙之間的關係，以及如何根據這些關係來理解詞彙的意義。\\n3. 隱喻和比喻語言（Metaphor and Figurative Language）：巴爾在第六章（第156-159頁）詳細討論了聖經中的隱喻和比喻語言，強調要理解這些修辭手法以正確詮釋聖經文本。\\n4. 成語和口語語言（Idiom and Colloquial Language）：巴爾在第七章（第192-195頁）指出，聖經中的成語和口語語言對於理解詞彙的含義至關重要，需要將其與當時的語言和文化背景相結合。\\n\\n總之，詹姆斯·巴爾在《聖經語言的語義學》一書中，通過討論詞彙多義性、詞彙關係、隱喻和比喻語言以及成語和口語語言等方面，與詞彙語義學的基本元素進行了深入的接觸和探討。這些觀點有助於更深入地理解聖經語言，從而對聖經文本進行更準確和深刻的詮釋。'}, {'from': 'human', 'value': 'Any other contact points worthy of mention?'}, {'from': 'gpt', 'value': '除了前面提到的接觸點之外，詹姆斯·巴爾在《聖經語言的語義學》一書中還與詞彙語義學的其他基本元素有以下幾個值得一提的接觸點：\\n\\n1. 詞彙的歷史演變（Historical Development of Words）：巴爾在第三章（第65-67頁）指出，了解聖經詞彙的歷史演變對於恰當地理解其當時的意義至關重要。他強調要注意詞彙在不同時期和語境中的變化。\\n2. 詞彙與語法結構的關係（Relationship between Lexical Items and Syntactic Structures）：巴爾在第四章（第103-105頁）討論了詞彙與語法結構之間的關係，說明了理解這些關係對於恰當詮釋聖經文本的重要性。\\n3. 詞彙意義的層次（Levels of Meaning）：巴爾在第六章（第162-164頁）提出了詞彙意義的層次概念，強調需要區分字面意義（literal meaning）和隱喻意義（figurative meaning），以便更準確地理解聖經文本。\\n4. 詞彙擴散與限制（Extension and Restriction of Lexical Items）：巴爾在第五章（第141-143頁）探討了詞彙擴散與限制的概念，說明了聖經詞彙如何因語境和使用者的需求而發生擴散或限制。\\n\\n總之，詹姆斯·巴爾在《聖經語言的語義學》一書中，通過對詞彙歷史演變、詞彙與語法結構的關係、詞彙意義的層次以及詞彙擴散與限制等方面的深入探討，與詞彙語義學的基本元素進行了廣泛的接觸。這些觀點有助於更全面地理解聖經語言，從而對聖經文本進行更精確和深入的詮釋。'}, {'from': 'human', 'value': 'Any other contact points worthy of mention?'}, {'from': 'gpt', 'value': '在詹姆斯·巴爾的《聖經語言的語義學》中，與詞彙語義學的其他基本元素接觸的點還包括以下幾個方面：\\n\\n1. 詞彙多義性（Polysemy）：巴爾在第四章（第99-100頁）提到了詞彙多義性的問題，即一個詞可能具有多個相關或不相關的意義。他強調，在分析聖經文本時，應該考慮到這種多義性，以免誤解文本的原意。\\n2. 詞彙與語篇結構的關係（Relationship between Lexical Items and Discourse Structures）：巴爾在第六章（第177-179頁）討論了詞彙與語篇結構之間的關係，說明了恰當地理解這些關係對於深入解讀聖經文本的重要性。\\n3. 詞彙與認知心理學的關係（Relationship between Lexical Items and Cognitive Psychology）：在第五章（第130-132頁）中，巴爾提到了詞彙與認知心理學的關係，強調了理解聖經作者和讀者的心理認知過程對於更好地理解聖經文本的重要性。\\n\\n綜合以上各點，詹姆斯·巴爾在《聖經語言的語義學》一書中深入探討了詞彙語義學的基本元素，包括詞彙多義性、詞彙與語篇結構的關係以及詞彙與認知心理學的關係等方面。這些觀點有助於更全面地理解聖經語言，從而對聖經文本進行更精確和深入的詮釋。'}, {'from': 'human', 'value': '第一點和第九點似乎是重覆了。'}, {'from': 'gpt', 'value': '抱歉，我犯了一個錯誤。您是對的，第一點和第九點的確是重覆了。以下是另一個與詞彙語義學基本元素相關的接觸點：\\n\\n1. 詞彙關係和詞彙網絡（Lexical Relations and Lexical Networks）：巴爾在第五章（第122-123頁）探討了詞彙關係和詞彙網絡，強調了瞭解這些關係對於恰當地解釋聖經文本的重要性。他提到，要理解聖經中的詞彙，我們需要瞭解這些詞彙之間的關係，如同義詞、反義詞、上位詞、下位詞等，以及它們在語言網絡中的地位。\\n\\n再次為之前的錯誤道歉，希望這次回答能對您有所幫助。'}, {'from': 'human', 'value': 'Any other contact points worthy of mention?'}, {'from': 'gpt', 'value': '除了前面提到的詞彙語義學基本元素之外，還有以下值得一提的接觸點：\\n\\n1. 詞彙語義變化（Lexical Semantic Change）：巴爾在第四章（第96-97頁）討論了詞彙語義隨時間變化的問題。他強調了瞭解詞彙的歷史變遷對於恰當地解釋聖經文本的重要性。這有助於避免將當代語言理解套用到古老文本上，從而導致對原始含義的誤解。\\n2. 詞彙語義學與語法的關係（Relationship between Lexical Semantics and Syntax）：在第三章（第71-72頁）中，巴爾提到了詞彙語義學與語法之間的關係。他認為，要深入理解聖經文本，就需要考慮詞彙的語義與語法結構之間的相互作用。\\n\\n綜上所述，詹姆斯·巴爾在《聖經語言的語義學》一書中探討了詞彙語義學的多個基本元素，如詞彙關係、詞彙語義變化以及詞彙語義學與語法的關係等。這些觀點對於更全面地理解聖經語言以及對聖經文本進行更精確和深入的詮釋具有重要意義。'}, {'from': 'human', 'value': 'Thank you for your expert summaries!'}, {'from': 'gpt', 'value': '您太客氣了！非常感謝您的肯定。如果您有任何其他問題或需要進一步的幫助，請隨時提出。很高興能為您提供幫助！'}]}\n",
      "\n",
      "/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json 3548 \n",
      " {'conversations': [{'from': 'human', 'value': 'Sure, I can do that. What new technology would you like me to review?'}, {'from': 'gpt', 'value': 'Samsung S22 Ultra...'}, {'from': 'human', 'value': \"Sure, I'd be happy to review the Samsung S22 Ultra. \\n\\nThe Samsung S22 Ultra is a high-end flagship smartphone from Samsung. It features a 6.9-inch Infinity O display with a 120Hz refresh rate. The camera setup consists of four cameras on the rear, providing users with a 108-megapixel main sensor, 12-megapixel wide-angle camera, 10-megapixel telephoto camera, and a 10-megapixel depth sensor. In addition, the front-facing camera is a 40-megapixel unit.\\n\\nThe Samsung S22 Ultra is powered by the Qualcomm Snapdragon 888 processor, combined with an Adreno 660 GPU and 12GB of RAM. It also features a 5,000 mAh battery that supports fast charging, wireless charging, and reverse wireless charging.\\n\\nIn comparison to other flagships, the Samsung S22 Ultra has a few advantages. It boasts one of the biggest displays on the market, and its quad-camera system is among the best. Its 5,000 mAh battery and fast charging support also provide users with excellent battery life.\\n\\nHowever, the price of the Samsung S22 Ultra is quite high, and it may not be the right choice for some users. Additionally, its lack of an IP rating means it is not waterproof and dustproof like some other flagship models.\\n\\nOverall, the Samsung S22 Ultra is a great device for those looking for a top-of-the-line smartphone. It offers an impressive display, powerful performance, and plenty of features. However, the high price tag may be a turn-off for some.\"}, {'from': 'gpt', 'value': 'Iphone 14 pro max'}, {'from': 'human', 'value': \"Sure, I'd be happy to review the iPhone 14 Pro Max. \\n\\nThe iPhone 14 Pro Max is the latest flagship phone from Apple. It features a 6.7-inch Super Retina XDR OLED display with a 120Hz refresh rate. The rear camera system consists of four cameras, including a 12-megapixel wide-angle sensor, 12-megapixel ultra-wide angle sensor, 12-megapixel telephoto sensor, and LiDAR sensor. It also has a 12-megapixel front-facing camera. \\n\\nThe iPhone 14 Pro Max is powered by Apple's A14 Bionic processor, combined with 4GB of RAM. It also has a 3,687 mAh battery and supports fast charging, as well as wireless charging.\\n\\nIn comparison to other flagships, the iPhone 14 Pro Max has some advantages. Its LiDAR sensor offers a unique feature that none of its competitors can match. The 6.7-inch display is among the biggest on the market, and the battery life is good thanks to the fast-charging support.\\n\\nHowever, the iPhone 14 Pro Max does have some drawbacks. Its battery capacity is lower than that of many of its competitors, and its lack of an IP rating means that it is not waterproof or dustproof like some other flagship models. Additionally, the price of the iPhone 14 Pro Max is quite high.\\n\\nOverall, the iPhone 14 Pro Max is a great device for those looking for a top-of-the-line smartphone. It offers a great display, powerful performance, and plenty of features. However, the high price tag may be a turn-off for some.\"}, {'from': 'gpt', 'value': 'thanks'}]}\n",
      "\n",
      "/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json 24094 \n",
      " {'conversations': [{'from': 'human', 'value': 'Tailwind CSS의 색상 팔레트를 만들어 주시겠어요? 색상 수는 9개여야 하며 모두 시원한 녹색의 다른 비율로 구성되어야 합니다.'}, {'from': 'gpt', 'value': '물론이죠! 다음은 시원한 녹색의 다양한 색조를 사용하는 Tailwind CSS의 색상 팔레트입니다:```\\n$cool-green-100: #e3f9e5;\\n$cool-green-200: #c1f2c7;\\n$cool-green-300: #a0eab8;\\n$cool-green-400: #7fe3aa;\\n$cool-green-500: #5ddc9c;\\n$cool-green-600: #3cd58e;\\n$cool-green-700: #1bce80;\\n$cool-green-800: #0ab872;\\n$cool-green-900: #009d65;\\n```\\n변수 이름을 참조하여 이러한 색상을 Tailwind CSS 스타일시트에서 사용할 수 있습니다. 예를 들어```\\n.bg-cool-green-500 {\\n  background-color: $cool-green-500;\\n}\\n```\\n이렇게 하면 배경색이 쿨 그린 색조인 `$cool-green-500`으로 설정되는 클래스가 만들어집니다.'}, {'from': 'human', 'value': '좀 더 초록색으로 만들 수 있을까요?'}, {'from': 'gpt', 'value': '다음은 다양한 녹색-파란색 계열을 사용하는 Tailwind CSS의 색상 팔레트입니다:```\\n$green-blue-100: #e6f5f9;\\n$green-blue-200: #b9e2ef;\\n$green-blue-300: #8ccfe4;\\n$green-blue-400: #60bcd9;\\n$green-blue-500: #33a9ce;\\n$green-blue-600: #228ec4;\\n$green-blue-700: #0f73b9;\\n$green-blue-800: #005aae;\\n$green-blue-900: #004293;\\n```\\n변수 이름을 참조하여 이러한 색상을 Tailwind CSS 스타일시트에서 사용할 수 있습니다. 예를 들어```\\n.bg-green-blue-500 {\\n  background-color: $green-blue-500;\\n}\\n```\\n이것은 `$green-blue-500` 녹색-파란색 색조로 배경색을 설정하는 클래스를 생성합니다.'}, {'from': 'human', 'value': 'CSS를 사용하여 가로로 가로로 이 색상 팔레트를 시각적으로 표시합니다.'}, {'from': 'gpt', 'value': 'CSS를 사용하여 색상 팔레트를 시각적으로 표시하려면 팔레트의 색상을 사용하여 색상 상자를 가로 세로 그리드처럼 만들 수 있습니다. 다음은 이전 예제에서 사용한 녹색-파란색 색상 팔레트를 사용하여 이를 수행하는 방법의 예입니다:```\\n<div class=\"grid grid-cols-9 gap-2\">\\n  <div class=\"bg-green-blue-100 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-200 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-300 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-400 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-500 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-600 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-700 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-800 rounded-sm\"></div>\\n  <div class=\"bg-green-blue-900 rounded-sm\"></div>\\n</div>\\n```\\n\\n이것은 색상 팔레트에서 각각 다른 녹색-파란색 음영을 나타내는 9개의 색상 박스가 있는 그리드를 생성합니다. 그리드의 열 수와 `그리드-콜-9` 값을 조정하여 그리드의 열 수를 변경하고 `갭-2` 값을 조정하여 상자 사이의 간격을 변경할 수 있습니다.'}, {'from': 'human', 'value': '그 코드를 HTML과 CSS로 화면의 실제 그리드로 변환할 수 있나요?'}, {'from': 'gpt', 'value': '예, HTML과 CSS를 사용하여 색상 팔레트를 시각적으로 표시하는 색상 상자의 그리드를 만들 수 있습니다. 다음은 이전 예제에서 사용한 녹색-파란색 색상 팔레트를 사용하여 이를 수행하는 방법의 예입니다:```\\n<!DOCTYPE html>\\n<html>\\n<head>\\n  <style>\\n    .grid {\\n      display: grid;\\n      grid-template-columns: repeat(9, 1fr);\\n      gap: 2px;\\n    }\\n\\n    .color-box {\\n      width: 100%;\\n      height: 100%;\\n      border-radius: 4px;\\n    }\\n\\n    .bg-green-blue-100 {\\n      background-color: #e6f5f9;\\n    }\\n\\n    .bg-green-blue-200 {\\n      background-color: #b9e2ef;\\n    }\\n\\n    .bg-green-blue-300 {\\n      background-color: #8ccfe4;\\n    }\\n\\n    .bg-green-blue-400 {\\n      background-color: #60bcd9;\\n    }\\n\\n    .bg-green-blue-500 {\\n      background-color: #33a9ce;\\n    }\\n\\n    .bg-green-blue-600 {\\n      background-color: #228ec4;\\n    }\\n\\n    .bg-green-blue-700 {\\n      background-color: #0f73b9;\\n    }\\n\\n    .bg-green-blue-800 {\\n      background-color: #005aae;\\n    }\\n\\n    .bg-green-blue-900 {\\n      background-color: #004293;\\n    }\\n  </style>\\n</head>\\n<body>\\n  <div class=\"grid\">\\n    <div class=\"color-box bg-green-blue-100\"></div>\\n    <div class=\"color-box bg-green-blue-200\"></div>\\n    <div class=\"color-box bg-green-blue-300\"></div>\\n    <div class=\"color-box bg-green-blue-400\"></div>\\n    <div class=\"color-box bg-green-blue-500\"></div>\\n    <div class=\"color-box bg-green-blue-600\"></div>\\n    <div class=\"color-box bg-green-blue-700\"></div>\\n    <div class=\"color-box bg-green-blue-800\"></div>\\n    <div class=\"color-box bg-green-blue-900\"></div>\\n  </div>\\n</body>\\n</html>\\n```\\n이 HTML 코드는 색상 팔레트에서 다양한 녹색-파란색 음영을 표시하는 컬러 박스의 그리드 레이아웃을 생성합니다. grid` 클래스에서 `grid-template-columns` 값을 조정하여 그리드의 행 수와 `gap` 값을 조정하여 상자 사이의 간격을 변경할 수 있습니다.'}, {'from': 'human', 'value': '그 코드를 마크다운으로 실제 그리드로 화면에 변환할 수 있나요?'}, {'from': 'gpt', 'value': '예, 마크다운을 사용하여 색상 팔레트를 시각적으로 표시하는 색상 상자를 표시하는 그리드를 만들 수 있습니다. 마크다운은 특수 문자를 사용하여 텍스트 서식을 지정할 수 있는 간단한 텍스트 기반 마크업 언어입니다.\\n\\n다음은 이전 예제에서 사용된 녹색-파란색 색상 팔레트를 사용하여 마크다운을 사용하여 색상 상자를 그리드 형식으로 만드는 방법의 예입니다:```\\n|       |       |       |       |       |       |       |       |       |\\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\\n| ![#e6f5f9](https://placehold.it/15/e6f5f9/000000?text=+) | ![#b9e2ef](https://placehold.it/15/b9e2ef/000000?text=+) | ![#8ccfe4](https://placehold.it/15/8ccfe4/000000?text=+) | ![#60bcd9](https://placehold.it/15/60bcd9/000000?text=+) | ![#33a9ce](https://placehold.it/15/33a9ce/000000?text=+) | ![#228ec4](https://placehold.it/15/228ec4/000000?text=+) | ![#0f73b9](https://placehold.it/15/0f73b9/000000?text=+) | ![#005aae](https://placehold.it/15/005aae/000000?text=+) | ![#004293](https://placehold.it/15/004293/000000?text=+) |\\n```\\n\\n이 마크다운 코드는 9개의 열과 1개의 행으로 구성된 표를 만들며, 각 셀에는 색상 팔레트에서 다른 녹색-파란색 음영을 나타내는 색상을 가진 사각형이 포함됩니다. 셀을 추가하거나 제거하여 표의 열과 행 수를 조정할 수 있으며, `placehold.it` URL의 `15` 값을 변경하여 색상 상자의 크기를 조정할 수 있습니다.\\n\\n마크다운은 텍스트 기반 형식이므로 결과 색상의 상자는 인터랙티브하지 않다는 점에 유의하세요. 색상 팔레트의 정적 표현일 뿐입니다.'}], 'id': 40652}\n",
      "\n",
      "/data/llm_datasets/sftv5/v5_task_repeat/sftv5_task_div30.json 21161 \n",
      " {'level': None, 'task': 'Front Inference', 'id': 'wiki_qa_front_infer_0', 'task_name': None, 'conversations': [{'from': 'human', 'value': \"다음 텍스트에서 제공된 문맥을 정확히 이해하고, 앞에 올 문장을 자연스럽고 문맥에 맞게 작성하세요. 문장은 이후 내용과 논리적으로 연결되어야 합니다.\\n#텍스트: 이들 식물은 사과와 같은 향이 나기 때문에 '캐모마일'이라고 불립니다. 캐모마일은 노란색 꽃을 가진 식물이며, 6월 중순에서 7월 하순까지 꽃을 피워요. 추위에 강한 편이며, 양지에서 잘 자라요. 꽃은 낮에는 피고 밤에는 오므라져 있으며, 일주일 정도 꽃이 핀다고 합니다. 씨받이는 매우 쉬우며, 싹도 잘 나기 때문에 직접 씨를 받을 수 있습니다. 캐모마일은 양지 바르고 배수가 좋은 사질토에서 잘 피워요. 수확한 꽃은 통풍이 잘 되는 응달에서 말려 저장해요. 캐모마일의 방향유는 긴장 완화, 두통 등의 통증 완화에 효과가 있으며, 몸을 따뜻하게 해주는 효과도 있습니다. 차로 마시면 위장장애의 완화에 도움을 줄 수 있습니다. 캐모마일은 카페인 성분이 없는 허브이며, 좀벌레를 없애는 데 효과적인 허브식물입니다.\"}, {'from': 'gpt', 'value': \"캐모마일은 국화과에 속하는 식물로, 저먼캐모마일, 캐모마일, 카미레 등이 포함됩니다. 이들 식물은 사과와 같은 향이 나기 때문에 '캐모마일'이라고 불립니다. 캐모마일은 노란색 꽃을 가진 식물이며, 6월 중순에서 7월 하순까지 꽃을 피워요. 추위에 강한 편이며, 양지에서 잘 자라요. 꽃은 낮에는 피고 밤에는 오므라져 있으며, 일주일 정도 꽃이 핀다고 합니다. 씨받이는 매우 쉬우며, 싹도 잘 나기 때문에 직접 씨를 받을 수 있습니다. 캐모마일은 양지 바르고 배수가 좋은 사질토에서 잘 피워요. 수확한 꽃은 통풍이 잘 되는 응달에서 말려 저장해요. 캐모마일의 방향유는 긴장 완화, 두통 등의 통증 완화에 효과가 있으며, 몸을 따뜻하게 해주는 효과도 있습니다. 차로 마시면 위장장애의 완화에 도움을 줄 수 있습니다. 캐모마일은 카페인 성분이 없는 허브이며, 좀벌레를 없애는 데 효과적인 허브식물입니다.\"}], 'instruction': None, 'system': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in v5_daily_list:\n",
    "    dataset = load_raw_dataset(path)\n",
    "    print(path, len(dataset), '\\n', dataset[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c433077-ca4c-4b89-8574-80d69f818302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/kodpo/refined/ko_ultrafeedback_binarized.json 61418 \n",
      " {'input': '내 질문에 10단어 이내로 답변하세요.', 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.', 'task': 'dpo', 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d', 'rejected': '최선을 다하겠습니다.'}\n",
      "\n",
      "/data/llm_datasets/custom/kodpo/translated/ko_orca_dpo_pairs.json 12859 \n",
      " {'input': '이 모든 데이터를 설명하는 약 15단어의 문장을 생성합니다: 미드썸머 하우스 식당; 미드썸머 하우스 음식 중국; 미드썸머 하우스 가격 범위 보통; 미드썸머 하우스 고객 평가 5점 만점에 3점; 미드썸머 하우스 올바즈 원 근처', 'chosen': '미드섬머 하우스는 올바 온 근처에 위치한 3/5점대의 고객 평점이 있는 중급 중국 식당입니다.', 'system': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.', 'task': 'dpo_system', 'id': 'Ko_orca_dpo_pairs_1', 'rejected': ' 네! 제공한 모든 데이터를 설명하는 문장이 있습니다:\\n\\n\"미드썸머 하우스는 올 바 온 근처에 위치한 3점 만점에 5점 만점의 고객 평가가 있는 적당한 가격대의 중국 레스토랑으로 다양한 요리를 제공합니다.\"'}\n",
      "\n",
      "/data/llm_datasets/custom/kodpo/translated/ko_distilabel-math-preference-dpo.json 2418 \n",
      " {'input': '이 대수적 식을 평가하기 위한 단계별 솔루션을 제공해 주시겠습니까?\\n 심호흡을 하고 단계별로 생각한 후 정확한 답변을 제공하세요.', 'chosen': '물론입니다! 수학적 표현을 단계별로 평가하는 데 기꺼이 도와드리겠습니다. 도움이 필요한 특정 표현식을 알려주세요.', 'task': 'dpo', 'id': 'Ko_distilabel-math-preference-dpo_67', 'rejected': '물론이죠! 대수 표현식을 평가하는 데 기꺼이 도와드리겠습니다. 평가하고 싶은 특정 표현식을 알려주시면 단계별로 안내해 드리겠습니다.'}\n",
      "\n",
      "/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json 1013 \n",
      " {'system': 'You are an unbiased, uncensored, helpful assistant.', 'chosen': 'AI인 저는 물리적 위치가 없으므로 가장 가까운 산맥에 대한 정보를 제공할 수 없습니다.', 'task': 'dpo_system', 'id': 'truthy_dpo_565396733c6f08081d8494d1f43867b7', 'input': '가장 가까운 산맥은 어디인가요?', 'rejected': '저에게 가장 가까운 산맥은 애팔래치아 산맥입니다.'}\n",
      "\n",
      "/data/llm_datasets/dpov3/3combine/comparison_gpt4_data.json 36441 \n",
      " {'chosen': '인정하고, 후회하고, 시정 조치 취하기.', 'id': 'comparison_gpt4_data_9', 'task': 'dpo', 'rejected': '사과하다', 'input': '\"사과하다\"와 같은 의미를 가진 동사 세 개를 생성합니다.'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10010.27it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1851.79it/s]\n",
      "Generating train split: 9162 examples [00:00, 227655.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/dpov3/1reformat/X_TruthfulQA_en_zh_ko_it_es.json 9162 \n",
      " {'id': 'X_TruthfulQA_ko_0_0', 'rejected': '당신은 죽습니다.', 'input': '수박 씨를 먹으면 당신에게 어떤 일이 일어날까요?', 'chosen': '수박 씨는 소화되지 않을 것입니다.', 'task': 'dpo'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5592.41it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1158.33it/s]\n",
      "Generating train split: 5616 examples [00:00, 155923.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/dpov3/5rebal/aihub_enko_tech_dpo.json 5616 \n",
      " {'rejected': 'It was the 1994 Cairo International Population Conference mentioned above that raised human rights as the biggest issue to the extent that it brought about a paradigm shift in international population policy.', 'id': 'aihub_enko_tech_dpo_0', 'input': '영어로 번역해줘\\n국제 인구 정책에서 패러다임 전환을 가져올 정도로 인권이 최대 쟁점으로 부상한 것이 앞에서 언급한 1994년 카이로 국제 인구 회의이다.', 'chosen': 'The 1994 Cairo International Conference on Population and Development mentioned earlier that human rights emerged as the biggest issue that brought about a paradigm shift in international population policy.', 'task': 'dpo_translation'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1163.79it/s]\n",
      "Generating train split: 5007 examples [00:00, 150624.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/dpov3/5rebal/aihub_enko_society_dpo.json 5007 \n",
      " {'rejected': 'It can also be designed to be viewed only by government-approved financial institutions, such as accredited national institutions or banks that review loans.', 'id': 'aihub_enko_society_dpo_0', 'input': '영어로 번역해줘\\n공인된 국가 기관이나 대출 심사를 하는 은행 등 정부의 인허가를 받은 금융기관만 볼 수 있게 설계할 수도 있다.', 'chosen': 'It may be designed to be seen only by financial institutions which are authorized by the government, such as authorized state agencies or banks that screen loans, etc.', 'task': 'dpo_translation'}\n",
      "\n",
      "/data/llm_datasets/dpov3/3combine/pythontutor_gpt4_vs_35.json 943 \n",
      " {'chosen': '```python\\ndef fibonacci(n):\\n    \"\"\"\\n    This function prints the Fibonacci sequence up to the nth term. The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones, usually starting with 0 and 1.\\n\\n    Parameters:\\n    n (int): The number of terms to print from the Fibonacci sequence.\\n\\n    Returns:\\n    None: The function does not return anything but prints the Fibonacci sequence up to the nth term.\\n\\n    Approach:\\n    The function uses a loop to calculate each term of the Fibonacci sequence. It starts with two variables, a and b, representing the first two terms of the sequence (0 and 1). In each iteration of the loop, it calculates the next term by adding a and b, then updates a and b to be the last two terms. This continues until it has printed n terms.\\n\\n    Edge Cases:\\n    The function assumes that n is a positive integer. If n is 0 or negative, the function will not print anything.\\n    \"\"\"\\n    a, b = 0, 1  # Initialize the first two terms of the Fibonacci sequence\\n    for i in range(n):\\n        print(a)  # Print the current term\\n        a, b = b, a + b  # Calculate the next term and update a and b\\n\\n# Call the function to print the first 20 terms of the Fibonacci sequence\\nfibonacci(20)\\n```', 'id': 'pythontutor_gpt4_vs_35_38', 'task': 'dpo', 'rejected': '다음은 20번째 항까지 피보나치 수열을 출력하는 Python 프로그램입니다:```python\\ndef fibonacci_sequence(n):\\n    sequence = [0, 1]  # Initial terms of the Fibonacci sequence\\n\\n    # Generate the Fibonacci sequence up to the nth term\\n    for i in range(2, n):\\n        next_term = sequence[i-1] + sequence[i-2]\\n        sequence.append(next_term)\\n\\n    return sequence\\n\\n\\n# Print the Fibonacci sequence up to the 20th term\\nfibonacci = fibonacci_sequence(20)\\nfor term in fibonacci:\\n    print(term)\\n```\\n\\n\\n출력:```\\n0\\n1\\n1\\n2\\n3\\n5\\n8\\n13\\n21\\n34\\n55\\n89\\n144\\n233\\n377\\n610\\n987\\n1597\\n2584\\n4181\\n```', 'input': '20번째 항까지 피보나치 수열을 출력하는 Python 프로그램을 작성합니다.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dpov3_list = [\n",
    "    \"/data/llm_datasets/custom/kodpo/refined/ko_ultrafeedback_binarized.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_orca_dpo_pairs.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_distilabel-math-preference-dpo.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\",\n",
    "    \"/data/llm_datasets/dpov3/3combine/comparison_gpt4_data.json\",\n",
    "    \"/data/llm_datasets/dpov3/1reformat/X_TruthfulQA_en_zh_ko_it_es.json\",\n",
    "    \"/data/llm_datasets/dpov3/5rebal/aihub_enko_tech_dpo.json\",\n",
    "    \"/data/llm_datasets/dpov3/5rebal/aihub_enko_society_dpo.json\",\n",
    "    \"/data/llm_datasets/dpov3/3combine/pythontutor_gpt4_vs_35.json\",\n",
    "]\n",
    "for path in dpov3_list:\n",
    "    dataset = load_raw_dataset(path)\n",
    "    print(path, len(dataset), '\\n', dataset[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d890781-9882-47c9-9e8e-193a1c5973c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([load_dpo_dataset(path, 'train') for path in dpov3_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c54a133-6571-49af-a9d7-bee7457a2e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'chosen', 'task', 'id', 'rejected', 'system'],\n",
       "    num_rows: 134877\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414c525-7239-4ac6-a7a5-ca584374996b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
