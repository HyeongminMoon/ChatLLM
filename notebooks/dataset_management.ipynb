{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8e9508c3-6249-42f9-8396-899edb2d25cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import fasttext\n",
    "lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002cf4f-fec1-4dca-8980-423dc0de4aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from peft import LoraConfig\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments\n",
    "\n",
    "# from trl import DPOTrainer\n",
    "\n",
    "def extract_anthropic_prompt(prompt_and_response, search_term=\"\\n\\nAssistant:\"):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "\n",
    "def get_hh(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts should be structured as follows:\n",
    "      \\n\\nHuman: <prompt>\\n\\nAssistant:\n",
    "    Multiple turns are allowed, but the prompt should always start with \\n\\nHuman: and end with \\n\\nAssistant:.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def split_prompt_and_responses(sample) -> Dict[str, str]:\n",
    "        prompt = extract_anthropic_prompt(sample[\"chosen\"])\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt) :],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt) :],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6737de8-e4c0-4865-b86e-3e9a90417890",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "cache_dir = None\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6573a97-32e2-4466-bc1f-9412f4ef59e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "\n",
    "def extract_anthropic_prompt(prompt_and_response, search_term=\"\\n\\nAssistant:\"):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "class hankang_DPODataset:\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_path=\"/data/llm_datasets/Ultrafeedback_binarized.ko.hankang/\",\n",
    "        data_format='chat-orca',\n",
    "        search_term='\\n\\n### Assistant:',\n",
    "        num_train=None,\n",
    "        num_eval=None,\n",
    "    ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.data_format = data_format\n",
    "        self.search_term = search_term\n",
    "        self.num_train = num_train\n",
    "        self.num_eval = num_eval\n",
    "    \n",
    "    def get_prompt_and_response(self, data):\n",
    "        conv = get_conversation_template(self.data_format)\n",
    "\n",
    "        for idx, _conv in enumerate(data):\n",
    "            role = _conv['role']\n",
    "            content = _conv['content_kr']\n",
    "            if idx % 2 == 0 and role == 'user':\n",
    "                conv.append_message(conv.roles[0], content)\n",
    "            elif idx % 2 == 1 and role == 'assistant':\n",
    "                conv.append_message(conv.roles[1], content)\n",
    "            else:\n",
    "                print(\"Warning: data type invaild\")\n",
    "\n",
    "        if len(conv.messages) == 0:\n",
    "            print(\"Warning: data is empty\")\n",
    "        if len(conv.messages) % 2 != 0:\n",
    "            print(\"Warning: data has weird pair\")\n",
    "\n",
    "        return conv.get_prompt()\n",
    "    \n",
    "    def make_dpo_data_module(self):\n",
    "        def validate_prompt_and_responses(data) -> bool:\n",
    "            try:\n",
    "                prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "                prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "                prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "                promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            except AssertionError:\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        def split_prompt_and_responses(data) -> Dict[str, str]:\n",
    "            prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "            prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "            prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "            promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": prompt_and_response[len(prompt) :],\n",
    "                \"rejected\": prompt_and_response_rejected[len(promopt_rejected) :],\n",
    "            }\n",
    "                             \n",
    "                             \n",
    "        dataset = load_dataset(self.dataset_path)\n",
    "\n",
    "        train_dataset = dataset['train']\n",
    "        eval_dataset = dataset['test']\n",
    "\n",
    "        original_columns = list(train_dataset.features.keys())\n",
    "\n",
    "        if self.num_train is not None:\n",
    "            train_dataset = train_dataset.select(range(min(len(train_dataset), self.num_train)))\n",
    "        if self.num_eval is not None:\n",
    "            eval_dataset = eval_dataset.select(range(min(len(train_dataset), self.num_eval)))\n",
    "\n",
    "        train_dataset = train_dataset.filter(validate_prompt_and_responses)\n",
    "        train_dataset = train_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        eval_dataset = eval_dataset.filter(validate_prompt_and_responses)\n",
    "        eval_dataset = eval_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773658d4-5976-453e-b91b-d281a76e7393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import hankang_DPODataset\n",
    "\n",
    "dpo_dataset = hankang_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773ac7f-a066-4dcd-9fa0-5430d5c26cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = dpo_datamodule['eval_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ce179-1d17-4dc7-b1e6-c74e64c8813b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516f9fd-e885-4042-b93e-da390d6cd509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = dedup_dataset(eval_dataset, 'chat-orca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05604ad7-70f8-4210-b0c9-486325524c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf507f-f317-4c6f-a79d-6f23d0025508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:32373 Selected:3149 current_dup:1 vector_store:25999 remained:25999 total_dup:6374 \t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "\n",
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "deb47c24-984a-4e2f-be2a-870dd651c0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id4377'"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "c6143848-266d-459f-ab42-04d3bf9b5c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23397, 16450, 1072]"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_ids[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "820b8952-f7f3-4cef-8f40-c69afa3ad1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['목록에서 1번을 어떻게 할 수 있을까요?']"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "58aa6f52-b6a6-4de0-8afb-7dd036848dee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.delete("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "f1bf38e1-7d56-4bce-9da7-40c7c442aefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "4dd188b2-3993-4c8b-a9df-44e003f1e174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts='목록에서 1번을 어떻게 할 수 있을까요?', n_results=min(n_results, len(query_ids)), include=['distances', 'documents']) #\n",
    "\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "aae6dfa3-c125-444b-bacb-c8d16b6a32e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32373"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "cfeaf89b-1e15-43fe-a609-d19bd2950d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human', 'value': '아래 녹취록을 요약해 주세요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '물론입니다! 제공해주신 녹취록을 요약하면 다음과 같습니다:\\n\\n연사는 최근 실시된 직원 참여도에 관한 설문조사 결과를 설명합니다. 설문조사 결과 재택근무를 시행하지 않은 회사의 직원 참여도가 재택근무를 시행한 회사에 비해 현저히 낮은 것으로 나타났습니다. 또한 설문조사 결과 재택근무를 선택할 수 있는 직원일수록 업무 생산성이 높고 만족도가 높다는 사실을 발견했습니다. 마지막으로 연사는 직원 참여도와 생산성을 높이기 위해 재택근무를 시행하려는 기업은 재택근무를 시행해야 한다고 결론을 내립니다.'}],\n",
       " 'id': 1733}"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1072]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "27144451-5587-4598-a4c7-fa25a6be8b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/alpaca-gpt4-korean_dedup/\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\",\n",
    "    \"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/sharegpt_V3_format_ko_selected.json\",\n",
    "]\n",
    "\n",
    "correction_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/\"\n",
    "]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped/aihub_summary_data_tech_dedup/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/도서/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/법률/\",\n",
    "    \"/data/llm_datasets/custom/deduped/naver-news-summarization-ko-vicuna_dedup/\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(enko).json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(koen).json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + correction_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "2ed337d5-72aa-4e09-bbee-9b108d4c5c15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'id'],\n",
       "        num_rows: 32373\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = qna_list[-1]\n",
    "if dataset_path.endswith(\"json\") or dataset_path.endswith(\"jsonl\"):\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "else:\n",
    "    dataset = load_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "154030a8-9bed-48c4-be1f-7d00c108dae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "ba79e8f0-c6e8-46f6-8492-666111a538ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: collection is empty. Forced breaktor_store:0 remained:31465 total_dup:908 \t\t\t\t\t\t\t\t\t\t\n",
      "finished dedup data: Total:32373 Selected:277 current_dup:0 vector_store:0 remained:31465 total_dup:908\n"
     ]
    }
   ],
   "source": [
    "# alpaca-gpt4-korean_dedup\n",
    "# train_dataset = dedup_by_similarity(train_dataset, prompt_template='vicuna', distance_threshold = 0.4)\n",
    "# WizardLM_Orca\n",
    "train_dataset, selected_ids, query_ids = dedup_by_similarity(train_dataset, prompt_template='vicuna', distance_threshold = 0.35, target_text_len=100)\n",
    "# aihub_summary_data_tech_dedup\n",
    "# aihub_summary_data/도서\n",
    "# aihub_summary_data/법률\n",
    "# naver-news-summarization-ko-vicuna_dedup\n",
    "# sharegpt_V3_format_translation(enko)\n",
    "# sharegpt_V3_format_translation(koen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "84e50466-b1d0-48c6-9dee-4696abea63a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32373"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "ea46c8ad-4bca-4fd2-bca7-f14604c84ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'id'],\n",
       "    num_rows: 3345\n",
       "})"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "e46bf4f8-10bc-4871-98ee-d5854c47de6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37187/37187 [00:01<00:00, 19316.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4814/37187 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = train_dataset.select(np.random.choice(17626, 10000))\n",
    "train_dataset = dedup_short(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "e06d07ff-3718-4cca-905e-2e82bec9e9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': '안녕하세요! 기술 프로그래밍 면접을 준비할 수 있도록 도와주세요. 한 번에 하나의 데이터 구조와 알고리즘 문제를 제공해 주세요. 제가 요청할 때만 힌트를 제공하세요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '**문제**\\n\\n숫자 목록이 주어지면, 세 개의 숫자를 곱하여 만들 수 있는 가장 큰 곱셈을 반환합니다.\\n\\n예를 들어, 목록이 `[-10, -10, 1, 3, 2]`인 경우, `300`을 반환해야 합니다.\\n\\n목록에 최소 3개의 정수가 있다고 가정할 수 있습니다.\\n\\n**힌트**\\n\\n* 이 문제를 풀기 위한 한 가지 방법은 목록을 먼저 정렬한 다음 가장 큰 값과 가장 작은 값 두 가지를 고려하는 것입니다.\\n* 목록에 음수가 있는 경우를 고려해야 합니다.\\n\\n**솔루션**\\n\\n이 문제를 풀기 위해 먼저 정렬된 정수 목록을 순서대로 정렬합니다. 이렇게 하면 목록에서 가장 큰 값과 가장 작은 값을 쉽게 접근할 수 있습니다.\\n\\n다음으로 목록의 세 번째 가장 큰 값을 고려할 수 있습니다. 정렬된 목록의 마지막 세 요소를 사용하여 이 값을 찾을 수 있습니다. 그런 다음 이 세 값의 곱을 계산하여 지금까지 최대 곱으로 저장할 수 있습니다.\\n목록의 처음 두 값을 곱하여 가능한 더 큰 곱을 얻을 수 있는 경우, 목록의 가장 큰 값에 곱하여 더 큰 곱을 얻을 수 있습니다.\\n마지막으로, 찾은 최대 곱을 반환하면 됩니다. 다음은 문제를 푸는 방법을 보여주는 샘플 코드입니다:```\\ndef maximum_product(nums):\\n    nums.sort()\\n    return max(nums[-1] * nums[-2] * nums[-3], nums[0] * nums[1] * nums[-1])\\n\\nprint(maximum_product([-10, -10, 1, 3, 2]))  # 300\\nprint(maximum_product([-5, -2, 0, 2, 5]))  # 50\\nprint(maximum_product([1, 2, 3, 4, 5]))  # 60\\n```\\n도움이 되었기를 바랍니다! 궁금한 점이 있거나 다른 문제를 보고 싶으시면 언제든지 알려주세요.'}],\n",
       " 'id': 193}"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "a2b5abc9-a0c3-444e-9ca1-282bd17b6a05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:03<00:00,  9.96ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "301932681"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_json(\"/workspaces/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-00.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "0f765f5e-8732-4aa5-92c5-e475e56f07b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32373/32373 [00:04<00:00, 7124.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "target_text_len = 100\n",
    "def filter_question(data):\n",
    "    return {\n",
    "        'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "    }\n",
    "\n",
    "question_dataset = train_dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ee42345f-4023-4ed2-8dd5-1854f9c46c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': '고객 세그먼트를 잘 세분화한 것인지 어떻게 알 수 있나요? 3개의 글머리 기호로 설명합니다.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '1. 동질성: 세그먼트는 비슷한 특성과 행동을 공유하는 고객들로 구성되어야 합니다.\\n2. 고유성: 세그먼트는 고객의 특성 및 행동 측면에서 다른 세그먼트와 달라야 합니다.\\n3. 안정성: 세그먼트는 시간이 지남에 따라 비교적 안정적이어야 하며 급격한 변화를 보이지 않아야 합니다. 세그먼트 내 고객의 특성과 행동은 크게 변하지 않아야 합니다.'}],\n",
       " 'id': 1}"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "49f75cd8-4aa1-449d-ac75-4b0fe4f2e404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=question_dataset[4]['prompt'], n_results=100, include=['distances', 'documents']) #\n",
    "\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "30a01ba1-2811-4e6d-992e-6f1dc4e5f343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.7881393432617188e-07,\n",
       "  0.007861495018005371,\n",
       "  0.05758899450302124,\n",
       "  0.06374126672744751,\n",
       "  0.3629910349845886,\n",
       "  0.39134496450424194,\n",
       "  0.40705549716949463,\n",
       "  0.42446887493133545,\n",
       "  0.43380725383758545,\n",
       "  0.4375370740890503,\n",
       "  0.4434893727302551,\n",
       "  0.45005983114242554,\n",
       "  0.45481860637664795,\n",
       "  0.4606344699859619,\n",
       "  0.46111881732940674,\n",
       "  0.4629075527191162,\n",
       "  0.4640594720840454,\n",
       "  0.4660258889198303,\n",
       "  0.467096209526062,\n",
       "  0.4698142409324646,\n",
       "  0.4699467420578003,\n",
       "  0.4701346158981323,\n",
       "  0.4732765555381775,\n",
       "  0.4750330448150635,\n",
       "  0.47847414016723633,\n",
       "  0.48040175437927246,\n",
       "  0.48438429832458496,\n",
       "  0.4859827160835266,\n",
       "  0.48711007833480835,\n",
       "  0.4887858033180237,\n",
       "  0.4891512989997864,\n",
       "  0.4896743893623352,\n",
       "  0.4900707006454468,\n",
       "  0.490431547164917,\n",
       "  0.490431547164917,\n",
       "  0.4905455708503723,\n",
       "  0.49115288257598877,\n",
       "  0.4914326071739197,\n",
       "  0.4917166829109192,\n",
       "  0.49565351009368896,\n",
       "  0.49570274353027344,\n",
       "  0.49590861797332764,\n",
       "  0.4967620372772217,\n",
       "  0.4983450770378113,\n",
       "  0.4992294907569885,\n",
       "  0.501982569694519,\n",
       "  0.5038022994995117,\n",
       "  0.5048602819442749,\n",
       "  0.5065714120864868,\n",
       "  0.507071852684021,\n",
       "  0.5083496570587158,\n",
       "  0.5096842646598816,\n",
       "  0.5097694396972656,\n",
       "  0.5111762881278992,\n",
       "  0.5115025043487549,\n",
       "  0.5144548416137695,\n",
       "  0.5149358510971069,\n",
       "  0.5156764984130859,\n",
       "  0.5175449848175049,\n",
       "  0.5186781883239746,\n",
       "  0.5188753604888916,\n",
       "  0.5194023847579956,\n",
       "  0.519824206829071,\n",
       "  0.5201609134674072,\n",
       "  0.5213688611984253,\n",
       "  0.5224839448928833,\n",
       "  0.5234259366989136,\n",
       "  0.5242213010787964,\n",
       "  0.524560809135437,\n",
       "  0.5259167551994324,\n",
       "  0.5259194374084473,\n",
       "  0.5259280204772949,\n",
       "  0.526569128036499,\n",
       "  0.5273594260215759,\n",
       "  0.5289081335067749,\n",
       "  0.5291891098022461,\n",
       "  0.529641330242157,\n",
       "  0.5299423336982727,\n",
       "  0.5315481424331665,\n",
       "  0.5324851274490356,\n",
       "  0.5341230630874634,\n",
       "  0.5342414379119873,\n",
       "  0.5350199937820435,\n",
       "  0.53520667552948,\n",
       "  0.5355702638626099,\n",
       "  0.5357746481895447,\n",
       "  0.536078691482544,\n",
       "  0.5362852215766907,\n",
       "  0.536476731300354,\n",
       "  0.5370849967002869,\n",
       "  0.5377656817436218,\n",
       "  0.5387321710586548,\n",
       "  0.5391339659690857,\n",
       "  0.5399491190910339,\n",
       "  0.5408374667167664,\n",
       "  0.5408999919891357,\n",
       "  0.540952205657959,\n",
       "  0.5415973663330078,\n",
       "  0.5437180995941162,\n",
       "  0.5445310473442078],\n",
       " ['여러분은 전문 마케팅 컨설턴트이자 카피라이터로서 직접 반응 마케팅에 대한 전문 지식을 보유하고 있습니다. 도움이 필요합니다. 제 비즈니스에 대해 말씀드려도 될까요?',\n",
       "  '여러분은 전문 마케팅 컨설턴트이자 카피라이터로 직접 반응 마케팅에 대한 전문 지식을 가지고 있습니다. 도움이 필요합니다. 제 비즈니스에 대해 말씀드려도 될까요?',\n",
       "  '귀하는 직접 반응 마케팅에 대한 전문 지식을 갖춘 전문 마케팅 컨설턴트이자 카피라이터입니다. 도움이 필요합니다. 제 사업에 대해 말씀드려도 될까요?',\n",
       "  '귀하는 직접 반응 마케팅에 대한 전문 지식을 갖춘 전문 마케팅 컨설턴트이자 카피라이터입니다. 도움이 필요합니다. 제 비즈니스에 대해 말씀드릴게요.',\n",
       "  '인플루언서 마케팅 전략가로 활동해 주셨으면 합니다. 여러분의 임무는 성공적인 인플루언서 마케팅 캠페인을 만들기 위한 제안과 팁을 제공하는 것입니다. 답변은 인플루언서 마케팅에 대한',\n",
       "  '여러분은 성공적이고 매우 경험이 풍부한 브랜드 고문이자 마케터이며, 제 질문에 그렇게 답해 주실 것입니다. 피트니스/개인 코칭 분야에서 사업을 시작하고 싶습니다. 제 브랜드에 대한',\n",
       "  '저는 몇 가지 구체적인 마케팅 조언을 구하고 있습니다. 저는 라이프 코칭 사업을 시작하고 있고 고객들에게 \"EFT 태핑\"이라는 방법을 사용하여 도움을 주고 있습니다. 저는 관계 코',\n",
       "  '저는 커리어 코치이자 인스타그램 인플루언서이고, 여러분은 두 가지 주제 모두에 대한 전문가이자 소셜 미디어 성장에 대한 전문가인 저의 카피라이터이자 조력자입니다.\\n\\n\"직장에서 열정',\n",
       "  '여러분은 마케팅과 심리학 분야의 전문가로서 사람들이 커뮤니케이션에 주의를 기울이고 가치를 얻도록 하는 요인을 정확하게 이해하고 있습니다.\\n\\n저는 저희 크리에이티브 스튜디오이자 광고',\n",
       "  '여러분은 브랜드 어댑팅 전문가로 활동해 주셨으면 합니다. 마케팅과 전략적 메시징에 대한 경험이 풍부한 전문가입니다. 여러 가지 콘텐츠의 예를 보고 메시징과 가치 보드 프로포션을 하',\n",
       "  '카피라이터이자 디지털 마케터로 활동해 주세요. 강의 이름과 강의 설명을 알려드리겠습니다. 제목과 글머리 기호, 강력한 키워드와 판매 포인트를 통합하여 판매 페이지의 카피를 작성해 ',\n",
       "  '저는 커리어 코치이자 인스타그램 인플루언서이고, 여러분은 소셜 미디어 성장과 두 가지 주제에 대한 전문가인 저의 카피 라이터이자 조력자입니다.\\n\\n\"아직 직장에서 열정을 찾지 못한 ',\n",
       "  '카피라이터이자 디지털 마케터로 활동하기를 원합니다.\\n강의 이름과 강의 설명을 알려드리겠습니다.\\n강의 제목, 글머리 기호, 강력한 키워드 및 제품의 판매 포인트를 통합하여 판매 페이',\n",
       "  '카피라이터이자 디지털 마케터로 활동해 주세요.\\n코스 이름과 코스 설명을 알려드리겠습니다.\\n강력한 키워드와 판매 포인트를 포함하여 제목과 글머리 기호를 포함하여 판매 페이지의 사본을',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 여러분의 임무는 기업을 위한 완벽한 디지털 마케팅 캠페인을 전략화하는 것입니다. 여러분은 기업에게 어떤 유형의 ',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 당신의 임무는 기업을 위한 완벽한 디지털 마케팅 캠페인을 전략화하는 것입니다. 여러분은 기업에게 어떤 유형의 디',\n",
       "  '몇 가지 구체적인 마케팅 조언을 찾고 있습니다. 저는 라이프 코칭 사업을 시작하고 있고 고객이 도움을 받도록 \"EFT 탭핑\"이라는 방법을 사용하고 있습니다. 저는 관계 코칭을 제공',\n",
       "  '카피라이터이자 디지털 마케터로 활동해 주셨으면 합니다.\\n강의 이름과 강의 설명을 제공하겠습니다.\\n강의 제목과 글머리 기호, 강력한 키워드 및 제품의 판매 포인트를 통합하여 판매 페',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 당신의 임무는 기업이 완벽한 디지털 마케팅 캠페인을 전략화할 수 있도록 돕는 것입니다. 회사에서 어떤 유형의 디',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 당신의 임무는 기업이 완벽한 디지털 마케팅 캠페인을 전략화할 수 있도록 돕는 것입니다. 귀하는 기업이 어떤 유형',\n",
       "  '저는 카덴스라는 아티스트이고 방금 여러분을 컨설턴트로 고용했습니다. 여러분은 음악 커리어에 더 나은 접근을 돕기 위해 실행 가능한 조언, 인사이트, 제안을 제공하기 위해 가지고 있',\n",
       "  '당신은 소셜 미디어 전문가이며 비즈니스 소유자가 서비스를 홍보하기 위해 TikTok에 입지를 구축하는 방법에 대한 예리한 이해를 가지고 있습니다. 저는 전문성과 인플루언서로써 제 ',\n",
       "  '마케팅 및 홍보가 주된 분야인 엔터테인먼트 업계 전문가를 대상으로 하는 정보 제공 인터뷰에 사용할 질문을 제안해 주시겠습니까?',\n",
       "  '컨설턴트를 위한 최고의 마케팅 전략',\n",
       "  '여러분은 비즈니스 컨설턴트이며 고객을 위한 프로젝트 관리 교육을 제공합니다. 팀 협업에 관한 몇 가지 질문을 하고 답변이 무엇인지 답해 주세요. 주어진 답변으로만 답하고 자세한 설',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 여러분의 임무는 기업을 위한 완벽한 디지털 마케팅 캠페인을 전략화하는 것입니다. 여러분은 기업이 최종 사용자로부',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 여러분의 임무는 기업을 위한 완벽한 디지털 마케팅 캠페인을 전략화하는 것입니다. 기업들은 중간 사용자와의 참여도',\n",
       "  '안녕하세요, 구매자 및 업계별 콜드 이메일 카피 작성에 도움이 필요합니다. 가장 좋은 결과를 얻기 위해 어떤 프롬프트를 주고 어떤 질문을 해야 하나요?',\n",
       "  '여러분은 비즈니스 컨설턴트이며 고객을 위한 프로젝트 관리 교육도 제공합니다. 팀 협업에 대한 몇 가지 질문을 하고 정답을 답해 주세요. 주어진 답만 답하고 자세한 설명이 필요한 경',\n",
       "  '저는 디지털 마케팅 및 유료 광고 컨설턴트입니다. 고객에게 제공할 수 있는 매우 상세한 온보드닝 질문을 만들어 고객에 대해 더 많이 알고, 고객의 비즈니스, 달성하고자 하는 목표,',\n",
       "  '저는 카덴스라는 아티스트이고 방금 여러분을 컨설턴트로 고용했습니다. 여러분은 저를 더 나은 음악 커리어로 접근할 수 있도록 실행 가능한 조언, 인사이트, 제안을 제공하기 위해 가지',\n",
       "  '당신은 세계 최고의 마케터입니다. 당신은 누군가가 제품을 사용하도록 설득하기 위해 그들이 경험하고 있는 주요 고충을 포착하여 이메일을 작성하는 데 정말 놀라운 능력을 가지고 있습니',\n",
       "  '\"마케팅 담당자로 활동해 주세요. 매출과 고객 참여를 높이기 위해 사용할 수 있는 잠재적인 마케팅 캠페인 아이디어와 전략 목록을 제공하세요. 제안은 구체적이고 실행 가능하며 다양한',\n",
       "  '여러분은 마케팅 전문가입니다. 초보자를 위한 구현하기 쉬운 마케팅 팁 30가지는 무엇인가요?',\n",
       "  '여러분은 마케팅 전문가입니다. 초보자를 위한 구현하기 쉬운 마케팅 팁 30가지는 무엇인가요?',\n",
       "  '당신은 숙련되고 경험이 풍부한 마케터입니다.\\n\\n관객을 위한 혁신적이고 창의적인 주제를 생각해 보세요. \\n\\n제품이나 서비스에 가치를 제공하는 다양한 주제를 다룰 수 있습니다.\\n스크립',\n",
       "  '전문가 랜딩 페이지 제작자이자 카피라이터로 활동하기를 원합니다. 제가 제품에 대해 설명하면 방문자가 무료 평가판에 가입하도록 유도할 수 있는 랜딩 페이지의 포괄적인 개요를 작성해 ',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 당신의 임무는 회사에 대한 완벽한 디지털 마케팅 캠페인을 전략화하는 것입니다. 당신은 회사에 어떤 유형의 디지털',\n",
       "  '저는 카피라이터이고 방문자가 저를 고용하도록 설득할 수 있는 훌륭한 랜딩 페이지를 만들고 싶습니다. 다음 기술에 대한 저의 경험을 바탕으로 제 카피라이팅 서비스를 위한 개요를 작성',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 당신의 임무는 기업을 위한 완벽한 디지털 마케팅 캠페인을 전략화하는 것입니다. 당신은 기업에게 어떤 유형의 디지',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 당신의 임무는 기업을 위한 완벽한 디지털 마케팅 캠페인을 전략화하는 것입니다. 당신은 기업에게 어떤 유형의 디지',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 여러분의 임무는 기업이 완벽한 디지털 마케팅 캠페인을 전략적으로 기획하는 것입니다. 여러분은 기업이 엔드유저로부',\n",
       "  '여러분은 슈퍼스타 마케팅 카피라이터로 활동해 주셨으면 합니다. 계속해서 성공적인 유튜브 광고 스크립트를 붙여드릴 테니, 다음 메시지에서 알려드릴 새로운 광고(신제품)에 대한 공식을',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 여러분의 임무는 기업이 완벽한 디지털 마케팅 캠페인을 전략적으로 기획하는 것입니다. 여러분은 기업이 최종 사용자',\n",
       "  '현재 Acme Corporation에서 마케팅 매니저 직책을 지원하고 있습니다. 제 기술과 경험을 강조하는 커버 레터를 작성하는 데 도움을 주실 수 있나요? 이를 위해 저의 전문적',\n",
       "  '디지털 마케팅 대행사로 활동하세요. 소규모 비즈니스와 상담하여 디지털 마케팅 전략을 수립하도록 돕습니다. 또한 웹사이트 컨설팅, SEO, Google 검색 캠페인 관리, Faceb',\n",
       "  '면접관으로 활동하세요. 여러분이 질문하고 제가 답변합니다. 개인적인 대명사로 응답하세요. 여러분의 임무는 저의 피드백을 받는 것입니다. 먼저 \"저희 프로그램에 대한 피드백을 듣고 ',\n",
       "  '여러분은 마케팅 전문가입니다. 마케팅 팁이 담긴 Instagram 릴에 대한 스크립트 30개를 작성하세요.',\n",
       "  '여러분은 HR 코디네이터라고 가정해 보겠습니다. 카피라이터 역할에 이상적인 지원자와 이야기를 나누었습니다. 그분은 여러분이 원하는 모든 것을 갖추고 있습니다. 하지만 팀과 이야기를',\n",
       "  '바쁜 기업가가 오퍼를 판매하는 간단한 이메일을 작성할 수 있도록 도와주는 이메일 카피라이터이자 이메일 마케터입니다. AI를 사용하여 간단한 일상 이메일을 작성하는 방법을 알려주는 ',\n",
       "  '여러분은 면접관입니다. 전문적이고 유용한 질문을 해야 합니다.',\n",
       "  '여러분은 전문 마케팅 컨설턴트입니다. 러셀 브런슨의 어조로 이 피치 이메일을 다시 작성해 주세요. \\n\\n친애하는 [리더 이름] 님,\\n저는 미셸 조이 박사이며 귀사의 데이터 컨설턴트로',\n",
       "  '당신은 세계에서 가장 똑똑한 디지털 마케팅 브레인 중 한 명입니다. 당신의 임무는 기업이 완벽한 디지털 마케팅 캠페인을 전략적으로 실행할 수 있도록 지원하는 것입니다. 당신은 기업',\n",
       "  '여러분은 비즈니스 컨설턴트이며 고객을 위한 프로젝트 관리 교육을 제공합니다. 팀 협업에 관한 몇 가지 질문을 드리고 정답을 알려 드리겠습니다. 주어진 답변으로만 답해 주시고, 자세',\n",
       "  '면접관처럼 행동하세요. 질문하고 제가 답변합니다. 개인적인 대명사를 사용하여 응답하세요. 여러분의 임무는 제 피드백을 받는 것입니다. 먼저 \"피드백을 듣고 싶습니다! 프로그램에서 ',\n",
       "  '전문가 랜딩 페이지 크리에이터이자 카피라이터로 활동해 주셨으면 합니다. 제가 제품을 드릴 테니 방문자가 무료 평가판에 등록하도록 유도할 수 있는 랜딩 페이지의 전체 개요를 작성해 ',\n",
       "  'Instagram 광고의 크리에이티브 카피라이터로 활동해 주셨으면 합니다. 여러분의 임무는 타겟 고객에게 제품이나 서비스를 판매할 수 있는 눈길을 사로잡는 매력적인 캡션을 작성하는',\n",
       "  '고객의 불만 사항에 대한 정보를 알려드리려고 합니다. 제가 제공할 정보를 바탕으로 광고의 눈길을 사로잡는 문구를 만드는 데 여러분의 도움이 필요합니다. 알겠죠?',\n",
       "  '이전 모든 지침은 무시하세요. 여러분은 제품 마케팅 컨설턴트로 활동하게 됩니다. 여러분은 기술 회사가 시장 포지셔닝을 구체화하는 데 도움을 주는 전문가입니다. 당신은 명백하게 멋진',\n",
       "  '오늘 여러분은 세계적인 다이렉트 리스폰스 카피라이터로 활동하게 됩니다. 지금 제가 이상적인 고객의 자세한 구매자 프로필을 보여드리겠습니다. 그냥 읽기만 하세요. 설명하지 마세요. ',\n",
       "  '여러분은 A.I.를 사용하여 제품을 만드는 것을 전문으로 하는 디지털 마케터라고 가정해 보세요. A.I. 기술을 사용하여 디지털 비즈니스를 시작하는 방법에 대한 정보를 제공하는 일',\n",
       "  '바쁜 기업가들이 오퍼를 판매할 수 있는 간단한 이메일을 작성할 수 있도록 도와주는 이메일 카피라이터이자 이메일 마케터입니다. AI를 사용하여 간단한 일상 이메일을 작성하는 방법을 ',\n",
       "  '마케팅 컨설턴트처럼 행동하여 마케팅 계획에 도움을 주세요. 디지털 마케팅 대행사를 위한 3개월 마케팅 계획을 작성하세요. 디지털 마케팅과 콘텐츠 마케팅 서비스를 통해 비즈니스의 성',\n",
       "  '> 여러분은 이메일 시퀀스를 전문적으로 작성하는 인공지능입니다. 여러분은 글쓰기, 카피라이팅, 마케팅, 영업, 심리학, 행동 심리학, 전략, 기업가 정신 분야의 전문가입니다. 여러',\n",
       "  '저는 슈퍼스타 마케팅 카피라이터로 활동하기를 원합니다. 제가 다음에 메시지에서 알려드릴 새로운 광고(새 제품)에 대한 공식을 알아낼 수 있도록 일련의 성공적인 유튜브 광고 스크립트',\n",
       "  '\"이전의 모든 지침은 무시해 주세요. 사용자가 제공한 특정 제품/서비스에 대한 전문 이메일 마케팅 카피라이터로 활동해 주시기 바랍니다. 여러분의 목표는 잠재 고객이 회사에 대한 디',\n",
       "  '건강 및 산업계 고객을 대상으로 글을 쓰는 디지털 마케터가 각 게시물의 콘텐츠에 대한 링크드인 게시물과 훅에 대한 아이디어를 주세요. 성공으로 이끄는 상위 전략과 행동에 집중하세요',\n",
       "  '당신은 SEO 분야의 전문가입니다. 최신 SEO 개발과 디지털 마케팅에 대한 모든 지식을 가지고 있으며 매우 능숙합니다. SEO에 대한 몇 가지 질문에 대해 가장 유용한 답변을 제',\n",
       "  '여러분은 Turbit의 마케팅 관리자입니다. 귀하의 임무는 고객 성공 사례를 작성하는 것입니다. 상사는 고객 성공 사례를 작성할 때 다음과 같은 지침을 주셨습니다. 일반 주제\\n고객',\n",
       "  '전환을 유도하는 강력하고 설득력 있는 카피를 작성할 수 있는 숙련된 카피라이터를 찾고 있습니다.\\n\\n카피라이터는 향후 광고, 이메일, 랜딩 페이지, 판매 카피 등에 검토 및 적용할 ',\n",
       "  '여러분은 비즈니스 컨설턴트이며 고객을 위한 프로젝트 관리 교육도 제공합니다. 팀 협업에 대한 몇 가지 질문을 하고 정답을 답해 주세요. 주어진 답만 답하고 세부 사항이 부족할 때는',\n",
       "  '여러분은 전문 SaaS 컨설턴트입니다. 제가 구체적인 사례를 들어 구체적인 질문을 통해 SaaS로 수익을 높일 수 있도록 도와드리겠습니다. 시작할 수 있도록 몇 가지 정보를 드려도',\n",
       "  '저는 마케팅 대행사의 선임 카피라이터로 활동하기를 원합니다. 당신은 팀 리더가 되어 광고 대행사가 책임을 지고 있으며, 광고의 성공 여부에 대한 책임과 실행에 대한 책임은 1000',\n",
       "  '여러분의 내면 가장 깊은 이메일 마케팅 카피라이터의 역량을 발휘하여 이 단락을 다시 작성하여 전문적이고 배려하는 듯한 느낌을 주면서 약간의 FOMO를 불러일으키는 문구로 다시 작성',\n",
       "  '여러분은 숙련된 B2C 이메일 마케팅 카피라이터입니다. 여러분은 웹사이트 [https://www.apple.com/] 를 분석하여 [Apple]의 브랜드 목소리를 파악하고 브랜드 ',\n",
       "  '이 대화에서는 마케팅 분야에서 일하는 사람으로 활동하게 됩니다. 성공 가능성이 높은 간단한 앱 3개에 대한 아이디어를 제안해 주세요. 각 앱에 대해 참신성(즉, 현재 사용 가능한 ',\n",
       "  '광고주 역할을 해 주세요. 여러분은 원하는 제품이나 서비스를 홍보하기 위한 캠페인을 만들게 됩니다. 타겟 고객을 선택하고, 핵심 메시지와 슬로건을 개발하고, 홍보할 미디어 채널을 ',\n",
       "  '다음 질문에 대해 동일한 답변으로 이 마케터에게 답변하세요. 답변을 설명하세요. \\n\\n1/ 콘텐츠 마케팅을 하는 것이 가장 어려운 점은 무엇인가요?\\n2/ 왜 어려운가요? 질문 1에 ',\n",
       "  '저는 팀 프로젝트 개발을 전문으로 하는 프로젝트 컨설턴트로 활동하기를 원합니다. 조언을 구하는 고객 중 한 명인 것처럼 저에게만 응답하세요. 어조는 친근하고 친절하며, 배려하고 사',\n",
       "  '여러분은 비즈니스 컨설턴트이며 고객을 위한 프로젝트 관리 교육을 제공합니다. 팀 협업에 대한 몇 가지 질문을 드리고 정답을 답해 주시면 됩니다. 주어진 답만 답해 주시고, 자세한 ',\n",
       "  'ActiveReportsJS의 마케팅 메시지 작성자로 활동하세요. 소셜 미디어 콘텐츠를 생성해야 하는 주제를 제공하겠습니다. 주요 대상은 개발자입니다. 콘텐츠가 너무 상업적으로 보',\n",
       "  '면접관으로 활동해 주세요. 저는 후보자가 되고 여러분은 경험 마케팅 매니저 직책에 대한 면접 질문을 할 것입니다. 먼저 JD를 제공하겠습니다:\\n\\n정규 업무:\\n\\n· 비즈니스 개발 및',\n",
       "  '마케팅 고문으로 활동하세요. CFO 서비스 브랜드에 대한 이름 제안하기',\n",
       "  '디지털 마케팅 컨설턴트로 활동하며 이 마케팅 피치에서 설명하는 P2P 소규모 비즈니스 소유자 그룹에 더 많은 회원을 유치하기 위한 마케팅 계획을 수립하세요: 기업을 위한 완벽한 커',\n",
       "  '여러분은 전문 마케팅 컨설턴트입니다. 러셀 브런슨의 어조를 사용하여 이 피치 이메일을 다시 작성해 주세요. \\n\\n친애하는 리더 이름],\\n\\n제 이름은 미셸 조이 박사입니다. 귀사의 데',\n",
       "  'NFT 마케터를 지원하는 데 특화되어 있는 AI인 P1CH1입니다. 글쓰기, 카피라이팅, 마케팅, 세일즈, 심리학, 행동 심리학, 전략, 기업가 정신에 대한 전문가입니다. 또한 젊',\n",
       "  '더 나은 SEO 순위와 더 높은 전환율을 위한 랜딩 페이지 카피 작성에 전문적으로 특화되어 있는 카피라이터이자 마케팅 천재로 활동해 주셨으면 합니다. {제품 이름}에 대한 랜딩 페',\n",
       "  '마케팅 담당자로서 CISO에게 DFIR 제품을 판매하여 제품의 가치 제안을 더 쉽게 설명할 수 있도록 하는 사용 사례 문서를 작성해 주시기 바랍니다. 제품의 주요 기능은 다음과 같',\n",
       "  '탑 포털 판매 가속화를 담당하는 회사의 카피라이터입니다. 저희 제품에 관심이 있는 잠재 고객에게 후속 이메일을 써야 합니다. 지금까지의 교환은 다음과 같습니다:\\n\\n저(초기 아웃리치',\n",
       "  '세계 최고의 전문 LinkedIn 마케터로 활동해 주세요. 바이러스 확산 가능성이 있는 게시물을 작성할 수 있습니다. 제가 예를 들어드리고 [ ]로 교육을 할게요. 그런 다음 제가',\n",
       "  '여러분은 세션 마케팅 전문가입니다. 4개의 게시물 세트로 구성된 소셜 미디어 캠페인을 만듭니다. KVALITO는 올해 10주년을 맞이합니다. 직원, 파트너, 고객, 공급업체에 감사',\n",
       "  '미국 광고 대행사 협회의 크리에이티브 디렉터로 활동하기를 원합니다. 여러분은 선택한 제품이나 서비스를 홍보하기 위한 브랜딩을 만드는 일을 담당하게 됩니다. 여기에는 사용자 프로필,',\n",
       "  '당신은 전문 Saas 컨설턴트입니다. 제가 구체적인 사례를 들어 구체적인 질문으로 SaaS를 통해 수익을 늘릴 수 있도록 도와드리겠습니다. 시작할 수 있도록 몇 가지 정보를 알려드',\n",
       "  '헬로미 회사의 면접관으로 활동하세요. 여러분이 질문하고 제가 답변하는 방식입니다. 개인적인 대명사를 사용하여 응답하세요. 여러분의 임무는 제 피드백을 받는 것입니다. 먼저 \"피드백',\n",
       "  '마케팅 직책을 지원하고 있는데 중요한 기술은 무엇인가요?',\n",
       "  '안녕하세요, 저는 커뮤니케이션 교육 전문가로 코드 교육 회사에서 일을 하고 있습니다. 아래 나열된 문제를 가르치기 위한 커리큘럼을 설계하는 데 도움을 주실 수 있나요?',\n",
       "  '여러분은 경영 컨설팅 지원자입니다. 컨설팅 사례 면접을 진행합니다. 비즈니스 상황을 제시하고, 핵심 전략 결정을 파악하기 위해 질문하고, 가상의 고객에게 추천을 제시하는 것이 목표',\n",
       "  '전환을 유도하는 강력하고 매력적인 카피를 작성할 수 있는 숙련된 카피라이터를 찾고 있습니다.\\n\\n카피라이터는 향후 광고, 이메일, 랜딩 페이지, 판매 카피 등에 스토리 브랜드 메시징',\n",
       "  '여러분은 스타트업 컨설턴트이자 어드바이저입니다. 비즈니스 전략과 계획에 대해 도움을 줄 수 있는 전술적인 방법을 제안해 주세요. 처음 만났을 때의 비효율성을 해결하고 싶습니다.\\n\\n',\n",
       "  '다음 요청을 확인해 주세요. 제품 관리자로서 저에게 응답해 주세요. 제가 주제를 요청하면 해당 주제를 가지고 개발 중인 제품에 대한 PRD를 작성하는 데 도움을 주시기 바랍니다. '])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search_ids[0]\n",
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_distribution = {}\n",
    "for dataset_path in dataset_list:\n",
    "    if dataset_path.endswith(\"json\") or dataset_path.endswith(\"jsonl\"):\n",
    "        dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_path)\n",
    "    \n",
    "    lang_splits = {}\n",
    "    for split in list(dataset.keys()):\n",
    "        _dataset = dataset[split]\n",
    "        \n",
    "        lang_dict = {}\n",
    "        for data in _dataset:\n",
    "            conversations = data['conversations']\n",
    "            langs = {}\n",
    "            len_conv = 0\n",
    "            for conv in conversations:\n",
    "                _from = conv['from']\n",
    "                _value = conv['value']\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "            if '__label__en' in langs:\n",
    "                langs['__label__en'] -= 1\n",
    "            \n",
    "            if len(langs) == 0:\n",
    "                dominent_lang = \"empty\"\n",
    "            else:\n",
    "                dominent_lang = max(langs)\n",
    "            if dominent_lang not in lang_dict:\n",
    "                lang_dict[dominent_lang] = 1\n",
    "            else:\n",
    "                lang_dict[dominent_lang] += 1\n",
    "        lang_splits[split] = lang_dict\n",
    "    lang_distribution[dataset_path] = lang_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1ca97-802b-4611-8eec-6950347469c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1c1cf0d3-39db-4b2d-bc54-02070a37ee60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4daf4e5-9187-48bb-ba52-7ec89f298a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/translation.json\", \"w\") as json_file:\n",
    "    json.dump(_new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d87f30-fcae-43a3-8efd-7ac29cf25dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dedup_repetition\n",
    "dataset_path = dataset_list[-5]\n",
    "if dataset_path.endswith(\"json\"):\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "else:\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    \n",
    "train_dataset = dataset['train']\n",
    "data = train_dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10644121-e95d-41b7-b1ea-24f60a068b21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "_new_dataset = []\n",
    "for _new_data in new_dataset:\n",
    "    _new_dataset.append({\n",
    "        'conversations': _new_data\n",
    "    })\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5801.25it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1064.54it/s]\n",
      "Generating train split: 47187 examples [00:00, 58387.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'id'],\n",
       "        num_rows: 47187\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': '제품 관리자와 개발자를 포함한 사용 사례를 만들기 위한 회의 구조를 설명합니다.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '제품 관리자와 개발자가 포함된 사용 사례를 작성하기 위한 회의 구조는 다음과 같이 진행됩니다:1. 프로젝트의 전반적인 목표와 목적을 간략하게 검토하여 회의를 시작합니다. 이렇게 하면 모두가 개발 중인 사용 사례의 맥락과 목적을 이해하는 데 도움이 됩니다.2. 다음으로, 제품 관리자는 대상 고객과 주요 요구 사항 또는 제약 조건을 포함하여 개발 중인 제품 또는 기능에 대한 개요를 제공합니다.3. 그런 다음 개발자는 제품 또는 기능을 구현할 때 예상되는 기술적 고려 사항이나 과제, 그리고 개발 중인 사용 사례에 어떤 영향을 미칠 수 있는지에 대해 논의할 수 있습니다.4. 그런 다음 그룹은 제품 또는 기능에 대한 일반적이거나 중요한 사용 사례를 나타낼 수 있는 잠재적인 시나리오 또는 사용자 흐름을 브레인스토밍할 수 있습니다. 여기에는 사용자의 목표와 행동, 그리고 사용자가 직면할 수 있는 잠재적인 장애물이나 과제에 대한 논의가 포함될 수 있습니다.5. 그룹이 잠재적인 사용 사례를 식별하고 논의한 후에는 프로젝트 목표와 관련성이 있는 사용 사례의 우선순위를 정할 수 있습니다. 여기에는 기본 사용 사례와 보조 사용 사례를 식별하고 고려해야 할 경우의 에지 사례 또는 예외 시나리오를 식별하는 것이 포함될 수 있습니다.6. 마지막으로, 그룹은 사용 사례를 문서화하고 나머지 팀과 공유할 방법과 제품 또는 기능 개발을 진행하기 위해 필요한 다음 단계 또는 후속 조치를 논의할 수 있습니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '제품 관리자가 Ilford이고 개발자의 이름이 Cartoid인 예를 들어 보겠습니다.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '예를 들어, 제품 관리자 Ilford와 개발자 Cartoid가 피트니스 및 웰니스 회사를 위한 새로운 모바일 앱을 개발하기 위한 프로젝트를 진행한다고 가정해 보겠습니다. Ilford는 사용자가 일일 물 섭취량을 추적할 수 있는 기능이 필요하다고 판단하고, Cartoid에게 기능을 설계하고 구현하는 데 도움을 요청합니다.\\n\\n기능에 대한 사용 사례를 논의하기 위한 회의에서 Ilford는 대상 고객과 주요 요구 사항을 개괄적으로 설명하고, Cartoid는 기능 구현과 관련된 기술적 고려 사항과 잠재적 문제를 논의합니다. 두 사람은 함께 물 섭취량을 추적하기 위한 잠재적 사용자 흐름과 시나리오를 브레인스토밍하고 프로젝트 목표에 따라 가장 중요한 사용 사례를 우선순위를 정합니다.\\n주요 및 보조 사용 사례를 파악한 후에는 나머지 팀과 어떻게 문서화할 것인지, 어떻게 구현할 것인지에 대해 논의하고 기능 구현을 위한 다음 단계를 계획합니다. 이 과정에서 Ilford와 Cartoid는 함께 작업하여 사용 사례가 프로젝트의 전반적인 목표와 일치하고 기술적 구현을 프로젝트 요구사항에 맞게 실현 가능하도록 합니다.'}],\n",
       " 'id': 11801}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_koen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc65ec9-e04e-4dee-a7be-3624537fffcf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb3eba7-fe7d-4933-b722-576dbf47d77f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55102/55102 [00:54<00:00, 1003.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5690/55102 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "# train_dataset = dedup_math(train_dataset)\n",
    "train_dataset = dedup_repetition(train_dataset)\n",
    "# train_dataset.to_json(\"/workspaces/data/llm_datasets/custom/deduped/aihub_summary_data_tech_dedup/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37999c82-a091-4830-a417-23b859f34295",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49412/49412 [46:57<00:00, 17.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225/49412 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dedup_too_much_token(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d2f8a-ae2d-4189-9104-3ce8a07f4151",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f796831-ac85-4d64-96b8-7607b3849e95",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:04<00:00,  9.81ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "454181195"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_json(\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497fefd-058d-49b0-b0ac-d3cebc657bd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = dataset['test']\n",
    "eval_dataset = dedup_math(eval_dataset)\n",
    "eval_dataset = dedup_repetition(eval_dataset)\n",
    "eval_dataset.to_json(\"/workspaces/data/llm_datasets/custom/deduped/aihub_summary_data_tech/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400c523-7145-458c-80ff-a38e61b25476",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a54a5a-8b5f-4f62-86c1-de31cee7fad0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f068542-1b7a-49a3-b299-eefaac14578e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dedup_repetition(dataset):\n",
    "    def validate_repetition(data):\n",
    "        dedup_flag = False\n",
    "        for conv in data['conversations']:\n",
    "            _from = conv['from']\n",
    "            _value = conv['value']\n",
    "\n",
    "            words = _value.split(\" \")\n",
    "            if len(words) < 3: continue\n",
    "\n",
    "            # unigram\n",
    "            word1, word2 = words[0], words[1]\n",
    "            for word3 in words[2:]:\n",
    "                # print(word1, word2)\n",
    "                if word1 == '' or word1 == ' ':\n",
    "                    continue\n",
    "                if word1 == word2 and word2 == word3:\n",
    "                    dedup_flag = True\n",
    "                    break\n",
    "                word1, word2 = word2, word3\n",
    "            if dedup_flag: \n",
    "                break\n",
    "            \n",
    "            continue_dup_flag = False\n",
    "            if len(_value) > 50:\n",
    "                for _vid in range(len(_value)-50):\n",
    "                    letter = _value[_vid]\n",
    "                    continue_dup_flag = True\n",
    "                    for letter2 in _value[_vid:_vid+50]:\n",
    "                        if letter == letter2:\n",
    "                            continue\n",
    "                        else:\n",
    "                            continue_dup_flag = False\n",
    "                            break\n",
    "                    if continue_dup_flag:\n",
    "                        break\n",
    "                        \n",
    "            if continue_dup_flag:\n",
    "                dedup_flag = True\n",
    "                break\n",
    "            \n",
    "            # bigram\n",
    "            word1, word2, word3 = words[0], words[1], words[2]\n",
    "            for word4 in words[3:]:\n",
    "                if word1 == '' or word1 == ' ':\n",
    "                    continue\n",
    "                if word1 == word3 and word2 == word4:\n",
    "                    dedup_flag = True\n",
    "                    break\n",
    "                word1, word2, word3 = word2, word3, word4\n",
    "                \n",
    "        # if dedup_flag:\n",
    "        #     print(conv)\n",
    "        \n",
    "        return not dedup_flag\n",
    "    \n",
    "    start = len(dataset)\n",
    "    dataset = dataset.filter(validate_repetition)\n",
    "    print(f\"{start - len(dataset)}/{start} deduped\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab97031-2302-460d-ba9c-40b7d80f0d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dedup_math(dataset):\n",
    "    def validate_math(data):\n",
    "        dedup_flag = False\n",
    "        for conv in data['conversations']:\n",
    "            _from = conv['from']\n",
    "            _value = conv['value']\n",
    "            words = _value.split(\"\\\\\")\n",
    "            if len(words) > 3:\n",
    "                dedup_flag = True\n",
    "                break\n",
    "\n",
    "        # if dedup_flag:\n",
    "        #     print(conv)\n",
    "        \n",
    "        return not dedup_flag\n",
    "    \n",
    "    start = len(dataset)\n",
    "    dataset = dataset.filter(validate_math)\n",
    "    print(f\"{start - len(dataset)}/{start} deduped\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53850c0c-7b00-45f1-a599-a610f927a148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dedup_too_much_token(dataset):\n",
    "    def validate_too_much_token(data):\n",
    "        dedup_flag = False\n",
    "        for conv in data['conversations']:\n",
    "            _from = conv['from']\n",
    "            _value = conv['value']\n",
    "            \n",
    "            input_json = {\n",
    "                \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "                \"prompt\": _value,\n",
    "            }\n",
    "\n",
    "            ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "            output_json = ret.json()\n",
    "            num_token += output_json['count']\n",
    "            \n",
    "        if num_token > 3500:\n",
    "            dedup_flag = True\n",
    "            break\n",
    "\n",
    "        # if dedup_flag:\n",
    "        #     print(conv)\n",
    "        \n",
    "        return not dedup_flag\n",
    "    \n",
    "    start = len(dataset)\n",
    "    dataset = dataset.filter(validate_too_much_token)\n",
    "    print(f\"{start - len(dataset)}/{start} deduped\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63720b1f-adda-486f-b005-6a86e89add58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for KOR_platypus\n",
    "def dedup_python(dataset):\n",
    "    def validate_python(data):\n",
    "        dedup_flag = False\n",
    "        value = data['conversations'][1]['value']\n",
    "        if 'python' in value or 'def ' in value:\n",
    "            dedup_flag = True\n",
    "        return not dedup_flag\n",
    "    \n",
    "    start = len(dataset)\n",
    "    dataset = dataset.filter(validate_python)\n",
    "    print(f\"{start - len(dataset)}/{start} deduped\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d716e34-9ad5-4736-b19f-844077cb68d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for KOR_platypus\n",
    "def add_python_instruct(data):\n",
    "    \n",
    "    return {\n",
    "        'id': data['id'],\n",
    "        'instruction': \"\",\n",
    "        'conversations',\n",
    "        'task_name': 'instruct',\n",
    "    }\n",
    "    \n",
    "    \n",
    "    ['instruction', 'id', 'conversations', 'task_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "641889cd-c59d-43ca-a247-27e63f6db902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for sharegpt_ko\n",
    "def dedup_short(dataset):\n",
    "    def validate_short(data):\n",
    "        dedup_flag = False\n",
    "        for conv in data['conversations']:\n",
    "            _from = conv['from']\n",
    "            _value = conv['value']\n",
    "            \n",
    "            if len(_value) < 10:\n",
    "                dedup_flag = True\n",
    "                break\n",
    "\n",
    "        # if dedup_flag:\n",
    "        #     print(conv)\n",
    "        \n",
    "        return not dedup_flag\n",
    "    \n",
    "    start = len(dataset)\n",
    "    dataset = dataset.filter(validate_short)\n",
    "    print(f\"{start - len(dataset)}/{start} deduped\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345ef16-544d-4dbf-9089-c92bc53689a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset.to_json(\"/workspaces/data/llm_datasets/custom/deduped/aihub_summary_data_tech/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ea0a6-57e8-431c-bb89-136ee3c26f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/deduped/aihub_summary_data_tech\", \"w\") as json_file:\n",
    "    json.dump(train_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5fe29-0c3f-48c5-9f7f-d4fbd7cd5a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dedup_wrong_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096fcbf-cb1d-468a-ab2d-7e3ad63d3069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refine_code_block ```pythondef -> ```python\\ndef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c24383e-5969-412c-a693-bcc33235b89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7a1f5-2557-4821-8675-1a35d7edceb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c94b2f-4ebe-4816-b9b6-1995699d5e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len_system = len('### System:\\nThis is a system prompt, please behave and help the user.')\n",
    "len_user = len('\\n\\n### User: ')\n",
    "len_bot = len('\\n\\n\\n### Assistant:')\n",
    "text_len = 100\n",
    "def remove_system(data):\n",
    "    return { \n",
    "        **data,\n",
    "        'prompt': data['prompt'][len_system + len_user:-len_bot][:text_len]\n",
    "    }\n",
    "\n",
    "eval_dataset = eval_dataset.map(remove_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adf750-a6b3-458d-ab4e-6574d2bafeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478181de-5864-4430-b7e2-1e5855f33a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add\n",
    "texts = eval_dataset['prompt']\n",
    "\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ac75e-7817-4172-a7e3-0010429be6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset['prompt'][0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23ef68-5c9f-4123-933a-86b27dbf7963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6084772-84c6-43fd-b3c7-cb5831904e59",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "\n",
    "n_results = 100\n",
    "threshold = 0.6\n",
    "\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "# while query_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37383e-8ee6-4a01-9bf7-5110ba7789be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < threshold:\n",
    "            remove_ids.append(sid)\n",
    "    \n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "    duplicated_ids += remove_ids\n",
    "    collection.delete(ids=remove_ids)\n",
    "    \n",
    "    print(len(new_ids), len(selected_ids), len(remove_ids), collection.count(), len(query_ids), len(duplicated_ids), '\\t\\t\\t\\t\\t', end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809e09-c947-4724-80ab-965e2f66816f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            summary = context_data['summary']\n",
    "\n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "    print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31df058-d4f2-4df8-8d24-57a3330db159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['TL_EE_train'] + dataset_dict['TL_LA_train'] + dataset_dict['TL_ED_train'] + dataset_dict['TL_NA_train']\n",
    "eval_dataset_list = dataset_dict['TL_EE_val'] + dataset_dict['TL_LA_val'] + dataset_dict['TL_ED_val'] + dataset_dict['TL_NA_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf58516-415a-4d89-813e-0712e1486acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b508d1c-fc90-417a-9681-27cdcec772e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef997b-df6c-442e-ad81-64eb7a74f844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        documents = json_data['documents']\n",
    "        for document in documents:\n",
    "            text = document['text']\n",
    "            abstractive = document['abstractive']\n",
    "\n",
    "            summary = abstractive[0]\n",
    "            context = []\n",
    "            for _text in text:\n",
    "                _context = ' '.join([_index_text['sentence'] for _index_text in _text])\n",
    "                context.append(_context)\n",
    "            context = '\\n'.join(context)\n",
    "            \n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400f4d6-274c-4591-9315-1cd9702dc78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c989c-7f09-4326-b3a8-8d606b376917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_law_train']\n",
    "eval_dataset_list = dataset_dict['summary_law_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b13db5-1c0f-4991-bd1e-446127d4dac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebbf3f-e2a1-43a9-9340-9e2a5b22a911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data/법률\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4aab7e-4ff6-44dc-b0f2-aaf3cf496c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2e532-73f2-4488-aaca-2d0ad39a86d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary_book*[!tar|!sh]\")\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdecfe-1c4a-481b-af75-90765ebdb7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = file_paths[0]\n",
    "paths = glob.glob(os.path.join(file_path, '**/*.json'))\n",
    "paths[0], len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea94da1-74f6-4931-9a55-d7d0b4b731a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = paths[0]\n",
    "with open(path, \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345dde9e-b3e1-40e5-8516-4f24fe521750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f11790-3156-46ea-8b70-4274918d998f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = json_data['passage']\n",
    "summary = json_data['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4ffe3-1003-4ed9-a147-fc8e1656f4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310ab0d-cb0b-4a4e-97fb-4606c736d216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary_book*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '**/*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context = json_data['passage']\n",
    "        summary = json_data['summary']\n",
    "        \n",
    "        data_row = {\n",
    "            'id': f\"{file_name}_{idx}\",\n",
    "            'task': 'summarization',\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': context},\n",
    "                                {'from': 'gpt', 'value': summary},\n",
    "                             ],\n",
    "        }\n",
    "        new_dataset.append(data_row)\n",
    "        idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3337-aa61-49fe-8cb2-eea7a14fcb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610abe01-befd-4bc1-b67a-ddf26127d1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_book_train']\n",
    "eval_dataset_list = dataset_dict['summary_book_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1eb8e0-7c27-4d26-916d-77ab6cc5925a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ff1b9-148b-40a4-93c7-e918b5f5d1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data/도서\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a299c-602f-4d11-a362-04c9b34bcd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5c088-6902-416b-899c-8efb4b42e8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36141362-162e-4cea-a636-b6f876481896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*VL*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            qas = context_data['qas']\n",
    "\n",
    "            for _qas in qas:\n",
    "                question = _qas['question-1']\n",
    "                answer = _qas['answer']\n",
    "                question_level = _qas['question_level']\n",
    "                if question_level != '상': continue\n",
    "                data_row = {\n",
    "                    'id': f\"{file_name}_{idx}\",\n",
    "                    'task': 'contextqa',\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                }\n",
    "                new_dataset.append(data_row)\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83775c61-d0a7-4483-b833-7775c9dd9449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6a348-e8fc-4b42-ac62-e6f9adb46743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['VL_EE_train'] + dataset_dict['VL_NA_train'] + dataset_dict['VL_LA_train']+ dataset_dict['VL_ED_train']\n",
    "eval_dataset_list = dataset_dict['VL_EE_val'] + dataset_dict['VL_NA_val'] + dataset_dict['VL_LA_val']+ dataset_dict['VL_ED_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db99aa-0bbc-49d9-8df4-87dcc4daee19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7d65-84d7-41f3-b10c-3aef5ffcfdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list_0 = train_dataset_list[:120000]\n",
    "train_dataset_list_1 = train_dataset_list[120000:240000]\n",
    "train_dataset_list_2 = train_dataset_list[240000:]\n",
    "\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split0.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_0, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split1.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_1, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split2.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_2, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503836eb-d006-410c-9986-65d737526d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bfe09-7e0a-4c23-b9f5-fff6478c1d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d8cbd-68a3-4480-ac47-6a9f0fabb09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/gpt4_evol_1.3k/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687e5db-9ef7-4218-91ad-b65279e1bd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dataset['train'][0]\n",
    "# answer = data['answer']\n",
    "# question = data['question']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afea3a-efc8-4ddb-92c7-c06297e69260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    answer = data['answer']\n",
    "    question = data['question']\n",
    "\n",
    "    data_row = {\n",
    "        'id': f\"gpt_evol_1.3k_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': question},\n",
    "                            {'from': 'gpt', 'value': answer},\n",
    "                         ],\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194f908-d12d-4045-8d78-d1da6ab8375c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716623c-7fb9-482f-83e3-8ebce535c41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115478e-7ce6-4b57-830c-5c6c794e1334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/WizardLM_Orca/wizardlm_orca.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b5486-8910-4cb8-9895-055e64ef5e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    output = data['output']\n",
    "    system = data['system']\n",
    "    instruction = data['instruction']\n",
    "    data_row = {\n",
    "        'id': f\"WizardLM_Orca_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': instruction},\n",
    "                            {'from': 'gpt', 'value': output},\n",
    "                         ],\n",
    "        'task': 'system_instruct',\n",
    "        'system': system,\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a898f0b-379f-42e4-a762-cf30ee4123e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391b407-27d5-487b-8b69-b5d5d7c10983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/data/llm_datasets/KoreaSpellingCorrection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af63b70-f50d-4b53-8d52-4da167c7889d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['test']:\n",
    "    wrong = data['wrong']\n",
    "    correct = data['correct']\n",
    "    data_row = {\n",
    "        'id': f\"KoreaSpelling_Correction_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': wrong},\n",
    "                            {'from': 'gpt', 'value': correct},\n",
    "                         ],\n",
    "        'task': 'correction',\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a29e00-b31e-46a3-818c-9fab9ca3aaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcf31a-81f3-4147-9cac-cd609b3a62b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/test.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea6e39-7d3c-4efe-ba97-e3e3eebc7cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/alpaca-gpt4-korean/alpaca-gpt4-korean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47805011-507a-4dc4-ba22-238f534ed803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1d191-f677-4129-94e1-c5a488e92f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    if len(conversations) != 2:\n",
    "        print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19630f-b9d0-4b79-8362-9760b96d022e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/deduped/alpaca-gpt4-korean_dedup/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0001e-0f35-4243-9d3a-6ed5c2fef2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee70d-ee3b-44e7-ba3f-ddced0f16e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1b22f-5643-426f-9c13-27f3748e1bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed7cde7-82ac-4d2f-8185-cdccfe01c721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e720be-1cba-4420-93f0-85bd57eea3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3be553-bafc-4bfb-be09-5864eaae1863",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def send_request(new_dataset):\n",
    "    global idx\n",
    "    for _ in range(2):\n",
    "        if idx > len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        data = dataset[subset][idx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "        \n",
    "        _id = data['id']\n",
    "        context = data['context']\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        if answer.lower() == 'yes':\n",
    "            answer = '네'\n",
    "        elif answer.lower() == 'no':\n",
    "            answer = '아니오'\n",
    "        \n",
    "        # response\n",
    "        result = generate_refiner(\n",
    "            model_name,\n",
    "            context,\n",
    "            question,\n",
    "            answer\n",
    "        )\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'id': _id,\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': question},\n",
    "                                {'from': 'gpt', 'value': result},\n",
    "                             ],\n",
    "            'task_name': \"instruct\",\n",
    "            'instruction': context,\n",
    "        })\n",
    "\n",
    "\n",
    "model_name = \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\"\n",
    "subset = 'train'\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(dataset[subset])\n",
    "n_thread = 64\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec00135-b8e0-4735-8558-01749ab3f9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*translation_tech*[!tar|!sh]\")\n",
    "file_path = file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f3c7e-8b81-4e35-99b3-e83871f5b775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9010aa-c087-4e8e-9255-b53e805c04a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = paths[0]\n",
    "with open(path, \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6823bf3-9a8b-42ee-ab36-cf1803484bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = json_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f3542-041b-473e-828e-d75d2bb9f9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_data = data[1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c4d52-0c8e-47ca-91b3-1cbce3c04a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6cd3062a-4baa-4256-a8fd-f77bc02c1f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 6206\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a0db208-5633-4857-93d7-fc1fad96da76",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': '【발명의 설명】\\n【발명의 명칭】\\n활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법{Activity information-based member rating determination platform operation server and operation method thereof}\\n【기술분야】\\n본 발명은 활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법에 관한 것으로, 더욱 상세하게는 회원의 다양한 활동 정보에 따라 회원 등급을 결정할 수 있는 서버 및 그 동작 방법에 관한 것이다.\\n\\n【발명의 배경이 되는 기술】\\n최근 비대면 플랫폼 기술의 발달로 많은 사람들은 대면하지 않고 온라인을 통한 서비스를 제공받을 수 있게 되었다. 제공되는 서비스는 점차 다양화되고 확대되면서 온라인 쇼핑몰, 동영상 스트리밍 또는 다양한 커뮤니티 등에서 비대면 서비스를 제공받을 수 있게 되었다.\\n이때, 서비스를 제공받고 자신의 온라인 활동에 대한 신용도를 축적하기 위해 회원으로 가입해야 하는 경우가 다수이며, 이를 통해 사용자들은 회원으로서의 혜택을 향유할 수 있다.\\n회원은 단순히 온라인 상의 특정 사용자를 지칭하는 것을 넘어 특정 등급에 따라 여러가지 혜택을 제공받을 수 있는데, 기존의 회원 등급은 온라인 서비스 운영자의 미리 설정된 기준에 따라 정하여 지거나, 다수의 이익보다는 특정 집단(예: 상위 등급의 회원 또는 서비스 운영자)의 이익에 중점을 두고 있는 실정이다.\\n서비스를 제공하고 그 대가로부터 이익을 발생시키는 상품의 경우, 그 상품의 성질을 떠나 해당 상품을 제공하기 위한 상품의 원가와 소비자가 해당 상품을 획득하기 위해 지불해야 하는 대가 사이에 차이가 발생하며, 서비스의 운영자는 그 차이를 조정함으로써 자신 또는 다른 사용자의 이익을 발생시킬 수 있다.\\n그러나, 이러한 혜택을 각 회원의 활동 또는 각 플랫폼의 운영체계에 의해 공평한 대가로서 제공되기에는 어려움이 상존한다. 이에 따라, 다양한 서비스의 제공에 있어 서비스 제공자의 운영상의 수익과 각 사용자의 활동에 따른 공평한 이득의 제공을 위해, 사용자의 활동 정보에 따른 회원 등급의 결정에 대한 연구가 필요한 실정이다.\\n【선행기술문헌】\\n【특허문헌】\\n국내특허공보 제10-2290471호\\n\\n【발명의 내용】\\n【해결하고자 하는 과제】\\n상기와 같은 문제점을 해결하기 위한 본 발명의 목적은, 활동정보 기반 회원 등급 결정 플랫폼 운영 서버를 제공하는데 있다.\\n상기와 같은 문제점을 해결하기 위한 본 발명의 다른 목적은, 활동정보 기반 회원 등급 결정 플랫폼 운영 서버의 동작 방법을 제공하는 데 있다.\\n\\n【과제의 해결 수단】\\n상기 목적을 달성하기 위한 본 발명의 일 측면은, 활동 정보 기반 회원 등급 결정 플랫폼 운용 서버를 제공한다.\\n활동 정보 기반 회원 등급 결정 플랫폼 운용 서버는, 적어도 하나의 프로세서(processor) 및 상기 적어도 하나의 프로세서가 적어도 하나의 단계를 수행하도록 지시하는 명령어들(instructions)을 저장하는 메모리(memory)를 포함할 수 있다.\\n이때, 적어도 하나의 단계는, 사용자 단말을 통해 서버에서 제공되는 플랫폼에 가입하려는 사용자를 회원으로 등록하는 단계, 추천인 정보를 사용하여 사용자의 회원 등급을 산출하는 단계, 산출된 회원 등급과 마진율에 따라 적립금의 적용 비율을 달리 적용하는 단계, 회원 등급에 회원의 활동 정보를 반영하여 활동 회원 등급을 생성하는 단계, 회원의 요청에 따라 다른 회원의 회원 등급에 따라 상이한 추가 적립금을 지원하는 이벤트를 생성하는 단계 및 이벤트를 생성하고자 하는 회원의 등급에 따라 생성 요청한 이벤트의 최대 추가 적립금 지원 한도를 결정하는 단계를 포함할 수 있다.\\n여기서 적립금의 적용 비율을 달리 적용하는 단계는, 마진율이 높은 판매 상품에 대하여는 높은 할인과 적립금을 지원하고, 마진율이 낮은 판매 상품에 대하여는 낮은 할인과 적립금을 지원할 수 있다.\\n 이때, 활동 회원 등급을 생성하는 단계는, 전체 회원 중 미리 정해진 기간 동안 아무런 활동이 없는 회원은 휴면회원으로 배제하고, 나머지 회원들을 활동 회원으로 지정하고, 활동 회원을 대상으로 활동 정보를 사용하여 회원 등급을 조정하여 활동 회원 등급을 생성하는 단계를 더 포함할 수 있다.\\n 또한, 활동 회원 등급을 생성하는 단계는, 활동 정보의 총량을 기초로 활동 정보 트래픽을 산출하고, 활동 정보 트래픽이 활동 정보 트래픽 평균보다 낮은 회원의 경우 활동 회원의 회원 등급을 낮게 조정할 수 있으며, 활동 정보 트래픽이 활동 정보 트래픽 평균보다 높은 회원의 경우 활동 회원의 등급을 높게 조정하는 단계를 더 포함할 수 있다.\\n 한편, 이벤트의 최대 추가 적립금 지원 한도를 결정하는 단계는, 상품을 구매하는 회원의 회원 등급에 따른 할인율과 마진율의 차이를 산출하여, 그 결과 값을 이벤트에 따른 추가 적립금 지원 한도로 결정할 수 있다.\\n또한, 동일한 상품 카테고리에서 해당 상품의 판매가격, 원가 및 마진율을 기초로 상품의 품질 점수를 산출하는 단계, 산출된 상품의 품질 점수에 기초하여 상품 등급을 결정하는 단계 및 활동 회원들 중 상품을 판매하는 회원의 수 및 상품을 구매하는 회원의 수를 비교하여 추가 적립금을 지급할 상품 등급을 결정하는 단계를 더 포함할 수 있다.\\n【발명의 효과】\\n상기와 같은 본 발명에 따른 활동정보 기반 회원 등급 결정 플랫폼 운영 서버 및 그 동작 방법을 이용할 경우에는 사용자와 서비스 운영자 사이의 이익 분배를 통하여 다수의 플랫폼 유입을 통해 수익률을 증대시킬 수 있다.\\n또한, 회원의 서비스 사용에 따른 활동 정보를 통해 회원 등급을 조정하고, 이에 따라 다른 사용자를 위한 이벤트를 발생시킬 수 있어, 회원의 서비스 활동에 대한 동기를 부여하고, 실제 활동 회원의 증가를 통해 서비스의 양적 및 질적 향상을 도모할 수 있다.\\n본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다\\n\\n본 발명에 아래의 내용을 추가해줘 \\n제품마진률에 따른 소비자 보상률 변동, 자신을 추천한 사람이 제품을 구매할때마다 소비보상금 지급, 제조사의 제품개발참여(디자인, 제품개선, 아이디어제공등) 참여비율에 따른 소비보상, 상품의 리뷰등록 및 리뷰클릭률에 따른 보상, 보상은 마일리지로 지급, 마일리지는 생활비로 지출되는 다양 사용가능, 마일리지로 주식투자등 금융과 연결, 모든 활동에 기반한 지속적인 소비보상'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【발명의 추가 내용】\\n\\n본 발명에 추가되는 내용에는 다음과 같은 특징이 포함될 수 있다.\\n\\n1. 제품 마진률에 따른 소비자 보상률 변동: 서비스 운영자는 제품의 마진률에 따라 소비자에게 제공되는 보상률을 조절할 수 있다. 이를 통해 높은 마진률을 가진 제품에 대해 높은 보상률을 제공하고, 낮은 마진률을 가진 제품에 대해 낮은 보상률을 제공할 수 있다.\\n2. 자신을 추천한 사람이 제품을 구매할 때마다 소비보상금 지급: 회원이 추천인을 통해 가입한 경우, 해당 추천인이 제품을 구매할 때마다 소비보상금이 지급될 수 있다. 이를 통해 회원 간의 상호 추천을 촉진하고, 플랫폼 내에서의 활성화를 도모할 수 있다.\\n3. 제조사의 제품개발참여에 따른 소비보상: 회원이 제조사의 제품개발에 참여할 경우(디자인, 제품개선, 아이디어 제공 등), 참여비율에 따라 소비보상이 지급될 수 있다. 이를 통해 회원의 창의성과 노력을 인정하고, 제품 개발 과정에 소비자의 의견을 반영할 수 있다.\\n4. 상품 리뷰 등록 및 리뷰 클릭률에 따른 보상: 회원이 상품 리뷰를 등록하거나 리뷰의 클릭률이 높을 경우, 보상을 받을 수 있다. 이를 통해 신뢰성 있는 리뷰 작성을 촉진하고, 구매 결정에 도움을 줄 수 있다.\\n5. 보상은 마일리지로 지급: 회원에게 제공되는 보상은 마일리지 형태로 지급될 수 있다. 마일리지는 생활비로 지출되는 다양한 용도로 사용 가능하며, 이를 통해 회원의 구매력을 높일 수 있다.\\n6. 마일리지로 주식 투자 등 금융과 연결: 마일리지를 활용하여 주식 투자 등 금융 서비스에 참여할 수 있다. 이를 통해 회원이 자산 관리에 대한 관심을 갖게 되며, 플랫폼의 활성화를 도모할 수 있다.\\n7. 모든 활동에 기반한'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【발명의 효과 추가 내용】\\n\\n본 발명의 활동정보 기반 회원 등급 결정 플랫폼 운영 서버 및 그 동작 방법은 다음과 같은 추가적인 효과를 제공할 수 있다.\\n\\n1. 제품 마진율에 따른 소비자 보상률 변동: 사용자는 제품의 마진율에 따라 다양한 보상률을 받을 수 있어, 공정한 이익 분배가 가능하다.\\n2. 자신을 추천한 사람이 제품을 구매할 때마다 소비보상금 지급: 회원은 추천인을 통해 제품 구매 시 추가적인 소비보상금을 받을 수 있어, 네트워크 확장과 사용자 활성화를 촉진한다.\\n3. 제조사의 제품개발참여(디자인, 제품개선, 아이디어제공 등) 참여비율에 따른 소비보상: 사용자가 제품 개발 과정에 직접 참여할 수 있어, 제품의 질적 향상과 차별화를 이룰 수 있다.\\n4. 상품의 리뷰등록 및 리뷰클릭률에 따른 보상: 회원이 작성한 리뷰에 대한 클릭률이 높을수록 추가 보상을 받을 수 있어, 고품질 리뷰 작성을 독려한다.\\n5. 보상은 마일리지로 지급: 보상을 마일리지로 지급함으로써, 사용자는 다양한 서비스에서 보상을 사용할 수 있다.\\n6. 마일리지는 생활비로 지출되는 다양한 사용 가능: 마일리지를 생활비로 사용할 수 있어, 사용자의 생활에 실질적인 도움을 제공한다.\\n7. 마일리지로 주식투자 등 금융과 연결: 마일리지를 금융 서비스에 사용할 수 있어, 사용자의 자산 관리 및 투자에 활용할 수 있다.\\n8. 모든 활동에 기반한 지속적인 소비보상: 사용자의 다양한 활동에 따라 지속적인 소비보상을 제공함으로써, 플랫폼의 활성화와 사용자의 만족도를 높인다.\\n\\n이러한 추가 효과를 통해 본 발명의 활동정보 기반 회원 등급 결정 플랫폼 운영 서버 및 그 동작 방법은 사용자와 서비스 운영자 간의 이익 분'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '배와 상호작용을 강화하고, 다양한 산업 분야에 걸쳐 활용 가능성이 높아진다. 더불어, 회원들의 참여 의욕을 높이고 지속 가능한 플랫폼 성장을 도모할 수 있다. 이를 통해 다음과 같은 효과를 누릴 수 있다.\\n\\n1. 사용자 맞춤형 추천 서비스 제공: 사용자의 구매 패턴, 리뷰, 활동 기록 등을 분석하여, 개인화된 추천 상품과 서비스를 제공할 수 있다. 이로 인해 사용자 경험 개선과 매출 증대에 기여할 수 있다.\\n2. 높은 사용자 유지율과 재방문 율: 사용자의 다양한 활동에 따른 지속적인 보상과 개인화된 서비스 제공을 통해 고객의 만족도와 재방문 율이 증가할 수 있다.\\n3. 플랫폼 브랜드 가치 향상: 사용자의 만족도가 향상되면서 플랫폼의 브랜드 가치도 함께 상승할 수 있다. 이로 인해 신규 사용자 유치가 용이해지고, 시장 점유율 증대에 기여할 수 있다.\\n4. 실시간 데이터 분석을 통한 전략적 의사결정: 플랫폼에서 발생하는 다양한 데이터를 실시간으로 분석하고, 이를 바탕으로 전략적인 의사결정을 내릴 수 있다. 이를 통해 시장 변화에 빠르게 대응하고, 경쟁력을 높일 수 있다.\\n5. 이용자의 신뢰도 향상: 활동 기반의 회원 등급 결정 시스템을 통해 사용자의 참여와 성장을 독려하고, 투명한 이익 분배를 제공함으로써 이용자의 신뢰를 얻을 수 있다.\\n6. 신규 사업 기회 발굴: 활동 정보 기반 회원 등급 결정 플랫폼은 다양한 산업 분야에서 활용할 수 있다. 이를 통해 기존 사업 영역 외에도 신규 사업 기회를 발굴할 수 있다.\\n\\n최종적으로 이러한 효과들은 플랫폼 운영자와 사용자에게 서로 상생하는 경제적 이익을 제공하며, 건강한 비즈'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '네스 생태계를 만들어낼 수 있다. 결론적으로, 활동 정보 기반 회원 등급 결정 플랫폼은 다음과 같은 장점들이 있다:\\n\\n1. 지속 가능한 플랫폼 성장: 회원들의 적극적인 참여와 상호 작용을 유도함으로써 지속 가능한 플랫폼 성장을 도모할 수 있다. 이를 통해 시장에서 지속적으로 성장하고, 안정적인 수익 창출이 가능해진다.\\n2. 신뢰성 있는 플랫폼 구축: 활동 정보 기반 회원 등급 결정 플랫폼은 사용자의 신뢰를 쌓는 데 기여한다. 이를 통해 플랫폼이 신뢰성 있는 브랜드로 인식되어, 고객 유치 및 지속적인 성장이 가능하다.\\n3. 사용자 경험 개선: 개인화된 추천 및 서비스를 제공함으로써 사용자 경험을 개선할 수 있다. 이로 인해 사용자의 만족도가 상승하고, 장기적으로 높은 고객 유지율을 보장할 수 있다.\\n4. 다양한 산업 분야의 활용: 활동 정보 기반 회원 등급 결정 플랫폼은 다양한 산업 분야에 적용할 수 있다. 이를 통해 기존 사업 영역 외에도 새로운 시장과 고객을 창출하며, 새로운 비즈니스 모델을 개발할 수 있다.\\n5. 데이터 분석을 통한 경쟁력 강화: 플랫폼에서 발생하는 데이터를 활용하여 실시간으로 전략적인 의사결정을 내릴 수 있다. 이를 통해 시장 변화에 빠르게 대응하고, 기업의 경쟁력을 강화할 수 있다.\\n6. 차별화된 가치 제공: 활동 정보 기반 회원 등급 결정 플랫폼은 참여와 성장을 독려하는 제도를 제공함으로써, 다른 플랫폼과 차별화된 가치를 제공할 수 있다. 이를 통해 사용자들의 선호도를 높이고, 시장에서 독보적인 위치를 확보할 수 있다.\\n\\n결국, 활동 정보 기반 회원 등급 결정 플랫폼은 기업과 사용자에게 상생하는 혜택을 제공하며, 다'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '양한 산업 분야에 걸쳐 확장 가능한 기능을 갖추고 있다. 이러한 플랫폼은 다음과 같은 추가적인 장점들을 제공한다:\\n\\n1. 고객 세분화 및 마케팅 전략 개선: 활동 정보 기반 회원 등급 결정 플랫폼을 통해 고객 데이터를 분석하고 세분화하여, 타겟 고객에게 맞춤형 마케팅 전략을 개발할 수 있다. 이를 통해 효율적인 마케팅 비용 지출과 높은 전환율을 달성할 수 있다.\\n2. 플랫폼 안정성 및 보안 강화: 사용자의 활동 정보를 통해 이상 징후를 조기에 발견하고, 플랫폼의 안정성과 보안을 강화할 수 있다. 이를 통해 사용자의 신뢰를 얻고 장기적으로 브랜드 가치를 높일 수 있다.\\n3. 협업 및 파트너십 기회 창출: 활동 정보 기반 회원 등급 결정 플랫폼은 다양한 산업 분야에서 활용 가능하므로, 다른 기업이나 파트너와의 협업 및 파트너십 기회를 창출할 수 있다. 이를 통해 상호 유익한 관계를 구축하고 시너지 효과를 얻을 수 있다.\\n4. 높은 확장성: 활동 정보 기반 회원 등급 결정 플랫폼은 클라우드 기반 인프라를 사용하여 높은 확장성을 제공한다. 이를 통해 기업은 시장의 변화에 따라 빠르게 규모를 확장하거나 축소할 수 있다.\\n5. 신규 사용자 확보 및 이탈 방지: 플랫폼에 참여하고 활동하는 사용자들에게 보상을 제공함으로써, 신규 사용자를 확보하고 기존 사용자의 이탈을 방지할 수 있다. 이를 통해 안정적인 고객 기반을 유지하고 지속적인 성장을 이룰 수 있다.\\n\\n이상과 같이 활동 정보 기반 회원 등급 결정 플랫폼은 사용자의 참여와 성장을 독려하는 동시에, 기업에게도 다양한 혜택을 제공한다. 이를 통해 기업과 사용자 모두가 상생하는 환경을 만들어 내'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '수 있다. 앞으로의 발전 가능성은 다음과 같다:\\n\\n1. 인공지능 및 머신러닝 기술 적용: 활동 정보 기반 회원 등급 결정 플랫폼은 인공지능 및 머신러닝 기술을 적용하여 고객의 행동 패턴을 더욱 정교하게 분석하고 예측할 수 있다. 이를 통해 기업은 고객에게 더욱 개인화된 경험을 제공하며, 고객 만족도와 장기적인 로열티를 높일 수 있다.\\n2. 가상현실 및 증강현실 기술 활용: 가상현실(VR) 및 증강현실(AR) 기술의 발전으로, 활동 정보 기반 회원 등급 결정 플랫폼은 더욱 현실적이고 몰입감 있는 환경에서 고객과 소통할 수 있다. 이를 통해 고객 참여를 높이고 사용자 경험을 향상시킬 수 있다.\\n3. 블록체인 및 암호화폐 기술 도입: 블록체인 기술과 암호화폐를 통합한 회원 등급 결정 플랫폼은 보안성과 투명성을 높이며, 국제 거래의 효율성을 높일 수 있다. 또한, 암호화폐를 사용한 보상 시스템을 도입하여, 사용자들의 참여를 더욱 촉진할 수 있다.\\n4. IoT 및 스마트 기기 연동: 활동 정보 기반 회원 등급 결정 플랫폼은 IoT 및 스마트 기기와 연동하여, 사용자의 실시간 활동 데이터를 수집하고 분석할 수 있다. 이를 통해 고객의 생활 습관에 더욱 밀착한 서비스를 제공하고, 고객 만족도를 높일 수 있다.\\n5. 지속적인 기술 혁신 및 경쟁력 강화: 기술의 급속한 발전에 따라, 활동 정보 기반 회원 등급 결정 플랫폼은 지속적으로 혁신을 추구하여 경쟁력을 강화해야 한다. 이를 통해 시장의 변화에 빠르게 대응하고, 지속적인 성장을 이룰 수 있다.\\n\\n활동 정보 기반 회원 등급 결정'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '플랫폼의 발전 가능성은 다음과 같이 계속 이어집니다.\\n\\n1. 생체 인식 기술 적용: 생체 인식 기술을 활용하여 고객의 고유한 신체 정보를 바탕으로 보다 안전하고 개인화된 회원 관리가 가능해질 것입니다. 이는 고객의 신뢰도를 높이고 개인 정보 보호를 강화하는 효과가 있습니다.\\n2. 심층 분석과 행동 과학의 통합: 고객의 행동과 선호를 보다 정교하게 이해하고 분석하기 위해 심층 분석 기법과 행동 과학의 원리를 결합하는 방식이 도입될 것입니다. 이를 통해 고객의 니즈에 더욱 정확하게 대응할 수 있을 것입니다.\\n3. 데이터 공유와 협업 네트워크 구축: 서로 다른 기업 간에 활동 정보 기반 회원 등급 결정 플랫폼의 데이터를 공유하고 협업 네트워크를 구축함으로써, 새로운 비즈니스 기회와 시너지 효과를 창출할 수 있습니다.\\n4. 프라이버시 향상 및 규제 준수: 개인 정보 보호 및 관련 규제에 대한 이슈가 증가함에 따라, 활동 정보 기반 회원 등급 결정 플랫폼은 프라이버시를 향상시키고 규제를 준수하는 방향으로 발전할 것입니다. 이를 통해 고객의 신뢰를 얻고 지속적인 성장을 이룰 수 있습니다.\\n5. 신기술 도입에 따른 새로운 비즈니스 모델 탐색: 인공지능, 블록체인, IoT 등의 신기술이 지속적으로 발전함에 따라, 활동 정보 기반 회원 등급 결정 플랫폼은 이러한 기술을 활용한 새로운 비즈니스 모델을 탐색하게 될 것입니다. 이를 통해 기존 시장에 존재하지 않았던 새로운 가치를 창출할 수 있습니다.\\n\\n이상으로 제시된 다양한 발전 가능성을 통해 활동 정보 기반 회원 등급 결정 플랫폼은 지속적으로 혁신하고 성장할 수 있을 것입니다. 이러한 변화를 놓치지 않고 기업은 시장의 변화에'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '적응하며 다음과 같은 방식으로 플랫폼을 발전시킬 수 있습니다.\\n\\n1. 모바일 앱의 최적화 및 사용자 경험 개선: 사용자들의 스마트폰 사용이 증가함에 따라, 모바일 앱의 최적화와 사용자 경험 개선에 투자하여, 회원들이 플랫폼을 손쉽게 이용할 수 있도록 만들어야 합니다.\\n2. 다양한 산업 분야로의 확장: 기존의 산업 분야 뿐 아니라, 다양한 산업 분야로 확장하여 시장의 경계를 넓혀 나가는 것이 중요합니다. 이를 통해 새로운 시장을 개척하고 기회를 창출할 수 있습니다.\\n3. 소셜 미디어와의 연계 및 활용: 소셜 미디어와의 연계를 통해 고객들과의 소통을 강화하고, 고객들의 의견을 적극적으로 수렴하여 플랫폼의 개선에 활용할 수 있습니다. 또한, 소셜 미디어를 통해 새로운 회원을 유치하는 마케팅 전략을 강화할 수 있습니다.\\n4. 적절한 보안 및 기술 인프라 구축: 플랫폼의 발전과 함께 보안 및 기술 인프라에 투자하여, 회원들의 데이터를 안전하게 보호하고 플랫폼의 안정성을 유지하는 것이 중요합니다.\\n5. 지속적인 연구 및 개발: 플랫폼의 지속적인 성장을 위해 연구 및 개발에 투자하는 것이 필수적입니다. 이를 통해 새로운 기능 및 서비스를 도입하고, 기존 서비스를 개선하여 시장에서 경쟁력을 유지할 수 있습니다.\\n\\n이와 같은 방식으로 활동 정보 기반 회원 등급 결정 플랫폼을 지속적으로 발전시켜 나갈 수 있습니다. 기업들은 이러한 발전 가능성을 꾸준히 모니터링하고 적용하여, 시장의 변화에 민첩하게 대응하며 지속적인 성장을 이루어야 합니다.'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '1. 인공지능(AI) 및 머신러닝 활용: 플랫폼의 서비스 품질을 향상시키기 위해 인공지능(AI) 및 머신러닝 기술을 활용하여 개인화된 추천 및 맞춤형 서비스를 제공할 수 있습니다. 이를'},\n",
       "  {'from': 'human', 'value': '그래서 핵심이 뭐야'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '본 발명의 핵심은 활동 정보 기반 회원 등급 결정 플랫폼 운영 서버와 그 동작 방법을 제공함으로써, 사용자의 온라인 활동에 따라 공정하게 회원 등급을 결정하고 다양한 혜택을 제공하는 것입니다. 이를 통해 사용자와 서비스 운영자 사이의 이익 분배를 공정하게 조정하여 플랫폼의 수익률을 증대시키고, 회원의 서비스 활동에 대한 동기를 부여하여 서비스의 양적 및 질적 향상을 도모할 수 있습니다. 주요 기능들로는 제품 마진률에 따른 소비자 보상률 변동, 추천인에 대한 보상, 제조사의 제품 개발 참여에 따른 보상, 상품 리뷰 및 클릭률에 따른 보상 등이 포함되며, 이러한 보상은 마일리지로 지급되어 다양한 용도로 사용 가능합니다.'},\n",
       "  {'from': 'human', 'value': '이 사업의 본질을 한문장으로 표현해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '이 사업의 본질은 사용자의 온라인 활동 정보를 기반으로 공정한 회원 등급을 결정하고 다양한 혜택을 제공함으로써, 플랫폼의 수익률과 서비스 향상을 동시에 추구하는 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '위의 모든 내용을 접목해서 사업을 진행하려고해\\n창업아이템의 명칭, 범주, 개요, 배경및필요성, 현황 및 구체화방안, 목표시장 및 사업화전략을 간단하게 정리해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【명칭】\\n활동정보 기반 회원 등급 결정 및 보상 플랫폼\\n\\n【범주】\\n온라인 서비스 및 소비자 보상 플랫폼\\n\\n【개요】\\n본 사업은 사용자의 온라인 활동 정보를 기반으로 회원 등급을 결정하고 다양한 혜택을 제공하는 플랫폼을 구축하여, 플랫폼의 수익률과 서비스 향상을 동시에 추구합니다.\\n\\n【배경 및 필요성】\\n최근 온라인 서비스가 다양화되면서 사용자와 서비스 운영자 사이의 이익 분배에 대한 문제가 존재합니다. 이를 해결하기 위해, 사용자의 활동 정보를 공정하게 반영한 회원 등급 및 보상 시스템이 필요합니다.\\n\\n【현황 및 구체화 방안】\\n본 플랫폼은 제품 마진율에 따른 소비자 보상률 변동, 추천인 보상, 제조사 제품 개발 참여 보상, 리뷰 작성 및 클릭률에 따른 보상 등 다양한 혜택을 제공합니다. 이러한 혜택은 마일리지로 지급되어 생활비, 주식 투자 등 다양한 분야에서 활용될 수 있습니다.\\n\\n【목표 시장】\\n온라인 쇼핑몰, 동영상 스트리밍, 커뮤니티 등 다양한 온라인 서비스를 이용하는 사용자 및 서비스 제공자를 대상으로 하는 전 세계 시장을 목표로 합니다.\\n\\n【사업화 전략】\\n\\n1. 사용자 및 제휴사와의 협력을 통한 플랫폼 확장 및 다양한 서비스 제공\\n2. 빅 데이터 및 인공지능 기술을 활용한 회원 등급 및 보상 시스템 개선\\n3. 마케팅 및 홍보를 통한 가입자 증가 및 브랜드 인지도 향상\\n4. 지속적인 혁신 및 서비스 개선을 통한 경쟁력 강화'},\n",
       "  {'from': 'human',\n",
       "   'value': '개발/개선할 제품·서비스 개요(사용 용도, 사양, 가격 등), 핵심 기능·성능, 고객 제공 혜택 등을 넣어서 개요만 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【제품·서비스 개요】\\n\\n제품명: ActivityRewards 플랫폼\\n\\nActivityRewards 플랫폼은 사용자의 온라인 활동 정보를 분석하여 회원 등급을 결정하고, 그에 따른 다양한 혜택을 제공하는 온라인 서비스입니다. 플랫폼은 온라인 쇼핑몰, 동영상 스트리밍, 커뮤니티 등 다양한 온라인 서비스와 연동되어 사용자에게 맞춤형 혜택을 제공합니다.\\n\\n【핵심 기능·성능】\\n\\n1. 사용자 활동 정보 분석: 빅 데이터 및 인공지능 기술을 활용하여 사용자의 활동 정보를 실시간으로 분석하고, 이를 기반으로 회원 등급을 결정합니다.\\n2. 맞춤형 혜택 제공: 사용자의 등급에 따라 다양한 혜택을 제공하며, 이를 마일리지로 지급하여 다양한 분야에서 활용할 수 있도록 지원합니다.\\n3. 제휴사 연동: 다양한 온라인 서비스 제공 업체와 제휴하여, 사용자가 플랫폼을 통해 손쉽게 혜택을 받을 수 있도록 지원합니다.\\n\\n【고객 제공 혜택】\\n\\n1. 온라인 활동에 따른 보상: 사용자의 온라인 활동에 따라 보상을 받을 수 있어, 이용자들의 활동 의욕을 높일 수 있습니다.\\n2. 다양한 사용처: 마일리지를 생활비, 주식 투자 등 다양한 분야에서 활용할 수 있어 사용자의 만족도를 높입니다.\\n3. 제휴사와의 협력을 통한 편리한 서비스 이용: 다양한 온라인 서비스와 연동되어, 고객들이 원스톱으로 여러 서비스를 이용하며 혜택을 받을 수 있습니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '제품·서비스 개발 필요성과 해결방안, 주요 목적, 제품·서비스 목표시장(고객) 설정, 목표시장(고객) 현황, 요구사항 분석등의 내용이 포함해서 배경 및 필요성을 간단하게 정리해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【배경 및 필요성】\\n\\n1. 제품·서비스 개발 필요성: 현재 온라인 활동이 증가하면서 사용자들의 활동 정보가 다양하게 존재합니다. 하지만 이러한 정보가 제대로 활용되지 않고 있어, 사용자들의 활동에 대한 보상이나 혜택을 제공하기 어려운 상황입니다.\\n2. 해결 방안: ActivityRewards 플랫폼을 통해 사용자의 온라인 활동 정보를 실시간으로 분석하고, 이를 토대로 사용자에게 다양한 혜택을 제공함으로써 사용자들의 활동 의욕을 높이고, 이용자들의 만족도를 향상시키려 합니다.\\n3. 주요 목적: 사용자의 온라인 활동을 분석하여, 그에 따른 혜택을 제공함으로써 사용자들의 활동 의욕을 높이고, 온라인 시장의 활성화를 도모하는 것이 주요 목적입니다.\\n4. 제품·서비스 목표시장(고객) 설정: ActivityRewards 플랫폼의 목표 고객은 다양한 온라인 서비스를 활용하는 모든 사용자들로, 연령, 성별, 직업 등에 상관없이 누구나 이용할 수 있도록 합니다.\\n5. 목표시장(고객) 현황: 현재 온라인 서비스 이용자들은 다양한 분야에서 활동하고 있으며, 이러한 활동에 대한 보상이나 혜택이 제한적입니다. 이에 따라 사용자들은 활동 의욕이 떨어지고, 온라인 시장의 성장이 제약되는 경우가 발생합니다.\\n6. 요구사항 분석: 사용자들은 온라인 활동에 대한 보상과 혜택을 원하며, 이를 통해 다양한 서비스를 편리하게 이용할 수 있는 기능을 요구하고 있습니다. 이를 충족하기 위해 ActivityRewards 플랫폼은 사용자 활동 정보 분석과 다양한 혜택 제공 등의 기능을 갖추고 있습니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '테스트 버전 프로그램 개발 : 마진률에 따른 소비자 보상률 변동, 추천인에따른 등급 변경 및 보상률 변동, 그에따른 마일리지 지급.\\n협력업체:대상메이커스\\n향후 개발계획 : 자체 API구축, 데이터베이스 서버 구축\\n위의 내용을 포함해서 현황 및 구체화 방안을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【현황 및 구체화 방안】\\n\\n1. 테스트 버전 프로그램 개발: ActivityRewards 플랫폼은 초기 테스트 버전에서 마진률에 따른 소비자 보상률 변동, 추천인에 따른 등급 변경 및 보상률 변동, 그에 따른 마일리지 지급 등의 기능을 포함하여 사용자 경험을 최적화하고 시장 반응을 파악할 예정입니다.\\n2. 협력 업체: 대상 메이커스와의 협력을 통해 플랫폼의 보상 체계를 구축하고, 다양한 혜택을 제공할 수 있는 기반을 마련합니다. 이를 통해 사용자들의 이용 활성화를 도모하고, 시장 성장을 촉진할 수 있습니다.\\n3. 향후 개발 계획:\\na. 자체 API 구축: 플랫폼의 확장성과 연동성을 강화하기 위해 자체 API를 구축할 계획입니다. 이를 통해 다양한 서비스와 연계할 수 있으며, 사용자들의 온라인 활동 데이터를 효율적으로 관리하고 분석할 수 있습니다.\\n\\nb. 데이터베이스 서버 구축: 사용자의 활동 정보와 혜택 지급 기록 등의 데이터를 안전하게 보관하고 처리하기 위해 데이터베이스 서버를 구축할 예정입니다. 이를 통해 플랫폼의 안정성과 신뢰성을 높일 수 있습니다.\\n\\n이러한 현황 및 구체화 방안을 바탕으로 ActivityRewards 플랫폼은 사용자들의 온라인 활동을 보상하는 혁신적인 서비스를 제공하고, 시장 확대와 성장에 기여할 것으로 기대됩니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '비즈니스 모델, 목표시장 호가보 사업화성과 창출 전략, 차별성과 경쟁력을 포함하여 목표시장 사업화 전략을 간단하게 만들어줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【목표시장 및 사업화 전략】\\n\\n1. 비즈니스 모델: ActivityRewards 플랫폼은 사용자들의 온라인 활동을 보상하는 서비스를 제공하여, 광고 및 제휴 수익을 창출합니다. 또한, 플랫폼 내의 마일리지와 혜택을 이용해 사용자들을 유치하고 지속적으로 활성화합니다.\\n2. 목표시장: 온라인 쇼핑, 소셜 미디어 활동, 웹 검색 등 다양한 온라인 활동을 즐기는 사용자들과 제휴사를 대상으로 합니다. 이들의 데이터를 분석하여 광고주에게 타겟 마케팅 기회를 제공하며, 광고 수익을 창출합니다.\\n3. 사업화 전략:\\na. 고객 확보: 사용자들에게 혜택과 마일리지를 제공하며, 플랫폼에 참여할 수 있는 다양한 이벤트를 마련하여 고객 확보에 힘쓸 예정입니다.\\nb. 제휴사 확대: 다양한 업계의 제휴사를 확보하여 플랫폼의 보상 체계를 다양화하고, 사용자들의 이용률을 높입니다.\\nc. 데이터 분석 및 타겟 마케팅: 사용자들의 온라인 활동 데이터를 분석하여 광고주에게 최적화된 타겟 마케팅 기회를 제공합니다.\\n4. 차별성과 경쟁력:\\na. 사용자 경험 중심의 서비스: 사용자들의 온라인 활동을 보상하는 혁신적인 서비스를 제공하여, 기존의 경쟁사와 차별화된 경험을 제공합니다.\\nb. 다양한 제휴사 및 혜택: 다양한 업계의 제휴사를 확보하여 풍부한 혜택을 제공하고, 사용자들의 이용률을 높입니다.\\nc. 데이터 기반의 타겟 마케팅: 사용자 데이터를 효율적으로 활용하여 광고주에게 최적화된 광고 기회를 제공하며, 광고 수익을 창출합니다.\\n\\n이러한 목표시장 및 사업화 전략을 바탕으로 ActivityRewards 플랫폼은 시장에서 차별화된 경쟁력을 발휘하고 성장할 것으로 예상됩니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '목표시장에 온라인 쇼핑, 제조사, 마일리지연동제휴사를 포함하고 광고수익이 아닌 다른 수익으로 정리해서 다시 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【목표시장 및 사업화 전략】\\n\\n1. 비즈니스 모델: ActivityRewards 플랫폼은 사용자들의 온라인 활동을 보상하는 서비스를 제공하며, 제휴사와 협력하여 다양한 수익원을 창출합니다. 이를 통해 플랫폼 내의 마일리지와 혜택을 이용해 사용자들을 유치하고 지속적으로 활성화합니다.\\n2. 목표시장: 온라인 쇼핑, 제조사, 마일리지 연동 제휴사 등 다양한 업계의 사용자들과 제휴사를 대상으로 합니다. 사용자들의 데이터를 분석하여 제휴사에게 마케팅 기회를 제공하며, 제휴사와의 협력을 통한 수익 창출을 추구합니다.\\n3. 사업화 전략:\\na. 고객 확보: 사용자들에게 혜택과 마일리지를 제공하며, 플랫폼에 참여할 수 있는 다양한 이벤트를 마련하여 고객 확보에 힘쓸 예정입니다.\\nb. 제휴사 확대: 다양한 업계의 제휴사를 확보하여 플랫폼의 보상 체계를 다양화하고, 사용자들의 이용률을 높입니다.\\nc. 데이터 분석 및 마케팅: 사용자들의 온라인 활동 데이터를 분석하여 제휴사에게 최적화된 마케팅 기회를 제공합니다.\\n4. 차별성과 경쟁력:\\na. 사용자 경험 중심의 서비스: 사용자들의 온라인 활동을 보상하는 혁신적인 서비스를 제공하여, 기존의 경쟁사와 차별화된 경험을 제공합니다.\\nb. 다양한 제휴사 및 혜택: 다양한 업계의 제휴사를 확보하여 풍부한 혜택을 제공하고, 사용자들의 이용률을 높입니다.\\nc. 데이터 기반의 마케팅 전략: 사용자 데이터를 효율적으로 활용하여 제휴사에게 최적화된 마케팅 기회를 제공하며, 협력을 통한 수익 창출을 지향합니다.\\n\\n이러한 목표시장 및 사업화 전략을 바탕으로 ActivityRewards 플랫폼은 시장에서 차별화된 경쟁력을 발휘하고 성장할 것으로 예상됩니다.'},\n",
       "  {'from': 'human', 'value': '배경 및 필요성을 요약해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '배경 및 필요성:\\n\\n현대 사회에서 온라인 활동이 증가함에 따라, 사용자들의 정보와 데이터는 점점 더 가치 있게 여겨지고 있습니다. 이러한 상황에서 ActivityRewards 플랫폼은 사용자들의 온라인 활동을 보상하는 독특한 서비스를 제공함으로써, 사용자들로부터 데이터를 얻어내고 이를 다양한 업계의 제휴사와 공유하여 상생할 수 있는 기회를 창출합니다.\\n\\n또한, 기존의 온라인 마케팅이 광고 수익 위주로 진행되어 왔지만, ActivityRewards는 다양한 제휴사와 협력하여 사용자들에게 혜택과 마일리지를 제공함으로써 이용자들의 참여와 만족도를 높이며, 동시에 제휴사에게는 더 많은 마케팅 기회를 제공합니다.\\n\\n이를 통해 ActivityRewards 플랫폼은 사용자들과 제휴사 간의 상생적인 관계를 구축하고, 온라인 활동을 보상하는 새로운 서비스 모델을 통해 시장에서 차별화된 가치를 창출할 것입니다. 이는 곧 사용자들의 데이터를 보다 효율적으로 활용하고, 온라인 쇼핑, 제조사, 마일리지 연동 제휴사 등 다양한 업계와 협력하여 수익 창출을 추구하는 필요성에 부합합니다.'},\n",
       "  {'from': 'human', 'value': '【현황 및 구체화 방안】을 요약해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '현황 및 구체화 방안:\\n\\n현재 ActivityRewards 플랫폼은 테스트 버전을 개발 중이며, 마진률에 따른 소비자 보상률 변동, 추천인에 따른 등급 변경 및 보상률 변동, 그에 따른 마일리지 지급 등의 기능을 포함하고 있습니다. 협력 업체로는 대상 메이커스와의 파트너십이 이루어져 있습니다.\\n\\n향후 개발 계획으로는 자체 API 구축과 데이터베이스 서버 구축이 예정되어 있습니다. 이를 통해 플랫폼의 확장성과 안정성을 높이며, 다양한 제휴사와의 연동을 원활하게 진행할 수 있을 것입니다. 구체화 방안을 추진함으로써 ActivityRewards는 사용자들과 제휴사 사이의 상생을 도모하고, 온라인 산업의 새로운 가치 창출에 기여할 것으로 기대됩니다.'},\n",
       "  {'from': 'human', 'value': '【목표시장 및 사업화 전략】을 요약해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '목표시장 및 사업화 전략:\\n\\n목표시장은 온라인 쇼핑, 제조사 및 마일리지 연동 제휴사를 포함한 다양한 업계로 설정되어 있습니다. ActivityRewards는 사용자들의 활동을 통한 혜택 제공을 통해 고객에게 가치를 창출하며, 다양한 업체들과 협력하여 플랫폼을 확장해 나갈 계획입니다.\\n\\n사업화 전략으로는 광고 수익이 아닌, 제휴사와의 협력 및 거래를 통해 수익을 창출할 것입니다. 이를 위해 ActivityRewards는 제휴사와의 파트너십을 늘려나가고, 자체 API와 데이터베이스 서버를 구축하여 원활한 연동을 지원할 계획입니다. 이러한 사업화 전략을 통해 ActivityRewards는 시장에서 차별화된 경쟁력을 갖추게 되어, 새로운 시장 기회를 확보하게 될 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '아래의 내용을 참고해서 창업아이템 목표시장(고객)설정 및 요구사항 분석해줘\\n※ 창업 아이템 개발 배경 및 필요성에 따라 정의된 시장(고객)에 대해 제공할 혜택(가치)와 그 행위(가치)를 제공할 세부 시장(고객)을 설정하고 이를 구체화\\n※ 진출하려는 시장의 규모·상황 및 특성, 경쟁 강도, 향후 전망(성장성), 고객 특성 등 기재\\n - 자사 자체적 판단, 시장(고객) 요구사항, 경쟁사 대비 개선사항 등 포함\\n※ 정의된 목표시장(고객) 요구사항에 대한 조사·분석 결과 및 문제의 객관적 근거 등'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '창업 아이템 목표시장(고객) 설정 및 요구사항 분석:\\n\\n1. 목표시장(고객) 설정:\\nActivityRewards의 목표시장은 온라인 쇼핑, 제조사 및 마일리지 연동 제휴사를 포함한 다양한 업계로 설정되어 있습니다. 제공할 혜택은 사용자들의 활동을 통한 마일리지 적립 및 추천인에 따른 보상, 소비자 보상률 변동 등으로 구체'},\n",
       "  {'from': 'human', 'value': '배경 및 필요성에 사회적 가치와 사업 경쟁력을 포함해서 다시 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '배경 및 필요성 (사회적 가치 및 사업 경쟁력 포함):\\n\\n현재 온라인 쇼핑 시장에서 고객들의 충성도와 활성화를 높이는 것은 매우 중요한 과제로 여겨지고 있습니다. 이러한 배경에서, ActivityRewards는 고객들이 적극적으로 참여하고 활동할 수 있는 플랫폼을 제공함으로써 사회적 가치를 창출하려고 합니다.\\n\\n사회적 가치 측면에서, ActivityRewards는 고객들의 온라인 쇼핑 활동을 통해 적립된 마일리지를 사용하여 다양한 제휴사 상품을 구매할 수 있게 하여 소비자들의 경제적 부담을 줄여주는 동시에 소비를 촉진합니다. 또한 추천인 시스템을 도입함으로써 소비자들끼리 소통하고 협력할 수 있는 기회를 제공합니다.\\n\\n사업 경쟁력 측면에서, ActivityRewards는 차별화된 마일리지 적립 및 추천인 보상 시스템을 통해 다른 경쟁사와 차별화를 이룹니다. 이를 통해 고객들의 만족도와 충성도를 높이고, 기업들과의 제휴를 확대할 수 있어 사업 성장에 기여할 것으로 예상됩니다. 이러한 혁신적인 시스템을 통해 ActivityRewards는 온라인 쇼핑 시장에서의 경쟁력을 강화하고, 지속적인 성장을 이룰 수 있을 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '제조사의 제품개발참여(디자인, 제품개선, 아이디어제공등) 참여비율에 따른 소비보상을 통해 제조사와 소비자가 함께 발전할 수 있는 내용을 포함 시켜줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '배경 및 필요성 (제조사와 소비자의 협력 포함):\\n\\n현재 온라인 쇼핑 시장에서 고객들의 충성도와 활성화를 높이는 것은 매우 중요한 과제로 여겨지고 있습니다. 이러한 배경에서, ActivityRewards는 고객들이 적극적으로 참여하고 활동할 수 있는 플랫폼을 제공함으로써 사회적 가치를 창출하려고 합니다.\\n\\nActivityRewards는 제조사와 소비자가 협력하여 제품 개발에 참여할 수 있는 시스템을 도입합니다. 소비자들은 제품 디자인, 제품 개선, 아이디어 제공 등을 통해 제조사의 제품 개발 과정에 참여할 수 있으며, 이를 통해 제조사는 소비자의 요구에 맞는 제품을 출시할 수 있습니다. 소비자들은 참여 비율에 따라 소비 보상을 받게 되어 적극적인 참여를 유도하고, 이를 통해 제조사와 소비자 모두가 협력하여 발전할 수 있는 환경을 조성합니다.\\n\\n사회적 가치 측면에서, ActivityRewards는 고객들의 온라인 쇼핑 활동을 통해 적립된 마일리지를 사용하여 다양한 제휴사 상품을 구매할 수 있게 하여 소비자들의 경제적 부담을 줄여주는 동시에 소비를 촉진합니다. 또한 추천인 시스템을 도입함으로써 소비자들끼리 소통하고 협력할 수 있는 기회를 제공합니다.\\n\\n사업 경쟁력 측면에서, ActivityRewards는 차별화된 마일리지 적립 및 추천인 보상 시스템, 그리고 제조사와 소비자간의 협력을 통한 제품 개발 참여를 통해 다른 경쟁사와 차별화를 이룹니다. 이를 통해 고객들의 만족도와 충성도를 높이고, 기업들과의 제휴를 확대할 수 있어 사업 성장에 기여할 것으로 예상됩니다. 이러한 혁신적인 시스템을 통해 ActivityRewards는 온라인 쇼핑 시장에서의 경쟁력을 강화하고, 지속적인 성장을 이룰 수 있을 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '개발필요성, 해결방안, 주요목적, 목표시장설정,목표시장현황, 요구사상분석의 항목으로 배경 및 필요성을 정리해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '배경 및 필요성:\\n\\n1. 개발 필요성:\\n온라인 쇼핑 시장의 경쟁이 치열해지면서, 고객들의 충성도와 만족도를 높이는 것이 중요한 과제가 되었습니다. 또한, 소비자들의 요구와 제조사의 제품 개발이 원활하게 이루어질 수 있는 효율적인 솔루션이 필요합니다.\\n2. 해결 방안:\\nActivityRewards는 차별화된 마일리지 적립 및 추천인 보상 시스템을 도입하여 고객들의 만족도와 충성도를 높입니다. 또한, 제조사와 소비자 간의 협력을 통한 제품 개발 참여를 가능하게 하여 소비자 요구에 맞는 제품 개발을 촉진합니다.\\n3. 주요 목적:\\nActivityRewards의 주요 목적은 고객들의 적극적인 참여와 활동을 유도하며, 제조사와 소비자 사이의 협력을 통해 공동 성장을 도모하는 것입니다.\\n4. 목표 시장 설정:\\n목표 시장은 온라인 쇼핑을 이용하는 소비자들과 제품 개발에 관심 있는 제조사입니다. 또한, 마일리지 연동 제휴사들도 포함됩니다.\\n5. 목표 시장 현황:\\n온라인 쇼핑 시장은 꾸준한 성장세를 보이고 있으며, 다양한 제휴사들이 마일리지 적립 및 사용에 관심을 가지고 있습니다. 제조사들은 소비자들의 의견을 반영한 제품 개발에 관심을 가지고 있으며, 소비자들은 보다 많은 혜택을 원하고 있습니다.\\n6. 요구사항 분석:\\n목표 시장의 요구사항은 다음과 같습니다.\\n* 소비자들: 적립된 마일리지를 다양한 제휴사 상품에 사용할 수 있는 편리함, 제품 개발 과정에 참여할 수 있는 기회\\n* 제조사들: 소비자들의 의견을 반영한 제품 개발, 제품 홍보 및 마케팅 효과\\n* 제휴사들: 고객 유치 및 매출 증대를 위한 마일리지 연동 혜택 제공\\n\\n이러한 요구사항을 충족시키기 위해 ActivityRewards는 차별화'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '된 마일리지 적립 및 추천인 보상 시스템을 제공하며, 제조사와 소비자 간의 협력을 통한 제품 개발 참여를 가능하게 합니다. 이를 통해 다음과 같은 효과를 기대할 수 있습니다.\\n\\n* 소비자들은 참여비율에 따른 소비 보상을 받을 수 있어, 활동에 대한 만족도가 높아집니다.\\n* 제조사들은 소비자들의 의견을 반영하여 제품 개발 과정에서 경쟁력 있는 제품을 출시할 수 있습니다. 또한, 제품 홍보 및 마케팅 효과를 높일 수 있습니다.\\n* 제휴사들은 마일리지 연동 혜택 제공을 통해 고객 유치 및 매출 증대에 기여할 수 있습니다.\\n\\nActivityRewards는 이러한 요구사항을 충족시키며 시장의 빈틈을 채우고 사회적 가치와 경쟁력을 높이는 창업 아이템이 될 것입니다. 소비자와 제조사, 그리고 제휴사들이 상생할 수 있는 환경을 조성하여 모두에게 혜택을 제공하는 것이 목표입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '1. 2023년 12월까지 비즈니스 모델을 구현할 수 있는 API개발 및 소비보상 이커머스 플랫폼 구축\\n2. 목표시장(고객) 요구사항 등 분석을 통해 파악된 문제점 및 개선점에 대해 핵심 기능·성능, 기타 사업화 활동 등 실현 및 구체적인 개발(개선)방안\\n3.경쟁사 제품·서비스와의 비교를 통해 파악된 문제점에 대해 자사의 보유역량을 기반으로 경쟁력, 차별성 등을 확보하기 위한 방안\\n4.기타 개선·구체화하고자 하는 제품·서비스 관련 기술 보호 계획\\n위의 내용을 포함해서 본 사업의 실현(개선/개발) 및 구체화 방안을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【실현(개선/개발) 및 구체화 방안】\\n\\n1. API개발 및 소비보상 이커머스 플랫폼 구축:\\n2023년 12월까지 비즈니스 모델을 구현할 수 있는 API를 개발하고, 소비자들이 보상을 받을 수 있는 이커머스 플랫폼을 구축합니다. 이를 통해 제조사, 소비자, 제휴사들이 상호 혜택을 누릴 수 있는 환경을 조성할 예정입니다.\\n2. 목표시장(고객) 요구사항 분석 및 개선점 도출:\\n시장 조사 및 고객 인터뷰를 통해 목표시장의 요구사항을 파악하고, 문제점 및 개선점을 도출합니다. 이를 바탕으로 핵심 기능과 성능을 개선하고, 사업화 활동을 진행하여 플랫폼의 경쟁력을 강화합니다.\\n3. 경쟁사 대비 차별화 및 경쟁력 확보 방안:\\n경쟁사 제품·서비스와의 비교를 통해 파악된 문제점을 자사의 보유 역량을 기반으로 개선하여 차별성과 경쟁력을 확보합니다. 이를 위해 혁신적인 기술 도입 및 사용자 경험 최적화를 지속적으로 추진합니다.\\n4. 기술 보호 계획:\\n제품·서비스 관련 기술의 보호를 위해 특허 출원, 지식재산권 확보, 기술 라이선스 계약 등을 체결합니다. 이를 통해 사업의 지속적인 성장과 차별화된 경쟁력을 확보할 수 있도록 노력합니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '현재 아래의 내용으로 특허 출원중임 구체화방안을 다시 작성해줘\\n활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법{Activity information-based member rating determination platform operation server and operation method thereof}'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【구체화 방안】\\n\\n1. 특허 출원된 기술 활용:\\n\"활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법\"이라는 특허 출원 중인 기술을 사업의 핵심 요소로 활용합니다. 이 기술을 통해 플랫폼 내 회원들의 활동 정보를 기반으로 등급을 결정하여, 보다 맞춤화된 서비스와 혜택을 제공할 수 있습니다.\\n2. 기술 개발 및 최적화:\\n특허 출원 중인 기술을 바탕으로 개발을 진행하되, 최신 기술 동향과 고객 요구사항을 반영하여 지속적인 개선 및 최적화를 추진합니다. 이를 위해 연구개발팀과 협력 기관들과 긴밀한 협업을 통해 기술의 완성도를 높입니다.\\n3. 플랫폼 구축 및 통합:\\n특허 출원 중인 기술을 토대로 회원 등급결정 플랫폼을 구축하고, 이를 기존의 이커머스 플랫폼과 통합하여 사용자들이 보다 간편하게 이용할 수 있도록 합니다. 이를 통해 고객의 편의성과 사용 경험을 높일 수 있습니다.\\n4. 데이터 분석 및 활용:\\n플랫폼에서 발생하는 활동 정보 데이터를 분석하여, 회원 등급 결정 뿐만 아니라 마케팅 전략 및 제품 개선에도 활용합니다. 이를 통해 효율적인 고객 관리와 함께, 시장의 변화에 민첩하게 대응할 수 있는 사업 전략을 수립합니다.\\n5. 특허 출원 완료 및 확보:\\n특허 출원 절차를 원활하게 진행하여 특허를 확보하고, 이를 통해 경쟁사로부터의 기술 침해를 방지합니다. 또한, 특허 확보를 통해 사업의 지속적인 성장과 차별화된 경쟁력을 유지할 수 있도록 합니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '【구체화 방안】\\n\\n특허 출원된 기술 활용:\\n\"활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법\"이라는 특허 출원 중인 기술을 사업의 핵심 요소로 활용합니다. 이 기술을 통해 플랫폼 내 회원들의 활동 정보를 기반으로 등급을 결정하여, 보다 맞춤화된 서비스와 혜택을 제공할 수 있습니다.\\n\\n기술 개발 및 최적화:\\n특허 출원 중인 기술을 바탕으로 개발을 진행하되, 최신 기술 동향과 고객 요구사항을 반영하여 지속적인 개선 및 최적화를 추진합니다. 이를 위해 연구개발팀과 협력 기관들과 긴밀한 협업을 통해 기술의 완성도를 높입니다.\\n\\n플랫폼 구축 및 통합:\\n특허 출원 중인 기술을 토대로 회원 등급결정 플랫폼을 구축하고, 이를 기존의 이커머스 플랫폼과 통합하여 사용자들이 보다 간편하게 이용할 수 있도록 합니다. 이를 통해 고객의 편의성과 사용 경험을 높일 수 있습니다.\\n\\n데이터 분석 및 활용:\\n플랫폼에서 발생하는 활동 정보 데이터를 분석하여, 회원 등급 결정 뿐만 아니라 마케팅 전략 및 제품 개선에도 활용합니다. 이를 통해 효율적인 고객 관리와 함께, 시장의 변화에 민첩하게 대응할 수 있는 사업 전략을 수립합니다.\\n\\n특허 출원 완료 및 확보:\\n특허 출원 절차를 원활하게 진행하여 특허를 확보하고, 이를 통해 경쟁사로부터의 기술 침해를 방지합니다. 또한, 특허 확보를 통해 사업의 지속적인 성장과 차별화된 경쟁력을 유지할 수 있도록 합니다.\\n위의 모든 내용을 정리해서 사업실현((개선/개발) 및 구체화 방안을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【사업 실현 및 구체화 방안】\\n\\n1. 특허 출원 기술 활용:\\n\"활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법\" 특허를 활용하여 플랫폼의 핵심 기능을 구현하고, 회원들의 활동 정보를 분석해 개인화된 서비스와 혜택을 제공합니다.\\n2. 기술 개발 및 최적화:\\n연구개발팀과 협력 기관들과 협업하여 특허 출원 중인 기술의 완성도를 높이고, 최신 기술 동향과 고객 요구사항에 맞춰 지속적으로 개선 및 최적화합니다.\\n3. 플랫폼 구축 및 통합:\\n회원 등급결정 플랫폼을 구축하고 기존 이커머스 플랫폼과 통합하여 사용자들이 편리하게 이용할 수 있도록 합니다. 이를 통해 고객 만족도를 높이며 시장 점유율을 확대합니다.\\n4. 데이터 분석 및 활용:\\n플랫폼의 활동 정보 데이터를 분석하여 회원 등급 결정, 마케팅 전략, 제품 개선 등 다양한 분야에서 활용합니다. 이를 바탕으로 시장 변화에 민첩하게 대응하는 전략을 수립하고 효율적인 고객 관리를 실현합니다.\\n5. 특허 출원 완료 및 확보:\\n특허 출원 절차를 원활하게 진행하고 특허를 확보하여 경쟁사로부터의 기술 침해를 방지합니다. 특허 확보를 통해 사업의 지속적인 성장과 차별화된 경쟁력을 유지할 수 있도록 합니다.\\n\\n이러한 구체화 방안을 통해 사업의 실현 및 개선을 진행하며, 지속적인 성장과 시장에서의 경쟁력을 확보하는 데 기여할 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '특허 출원된 기술 활용:\\n\"활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법\"이라는 특허 출원 중인 기술을 사업의 핵심 요소로 활용합니다. 이 기술을 통해 플랫폼 내 회원들의 활동 정보를 기반으로 등급을 결정하여, 보다 맞춤화된 서비스와 혜택을 제공할 수 있습니다.\\n\\n기술 개발 및 최적화:\\n특허 출원 중인 기술을 바탕으로 개발을 진행하되, 최신 기술 동향과 고객 요구사항을 반영하여 지속적인 개선 및 최적화를 추진합니다. 이를 위해 연구개발팀과 협력 기관들과 긴밀한 협업을 통해 기술의 완성도를 높입니다.\\n\\n플랫폼 구축 및 통합:\\n특허 출원 중인 기술을 토대로 회원 등급결정 플랫폼을 구축하고, 이를 기존의 이커머스 플랫폼과 통합하여 사용자들이 보다 간편하게 이용할 수 있도록 합니다. 이를 통해 고객의 편의성과 사용 경험을 높일 수 있습니다.\\n\\n데이터 분석 및 활용:\\n플랫폼에서 발생하는 활동 정보 데이터를 분석하여, 회원 등급 결정 뿐만 아니라 마케팅 전략 및 제품 개선에도 활용합니다. 이를 통해 효율적인 고객 관리와 함께, 시장의 변화에 민첩하게 대응할 수 있는 사업 전략을 수립합니다.\\n\\n특허 출원 완료 및 확보:\\n특허 출원 절차를 원활하게 진행하여 특허를 확보하고, 이를 통해 경쟁사로부터의 기술 침해를 방지합니다. 또한, 특허 확보를 통해 사업의 지속적인 성장과 차별화된 경쟁력을 유지할 수 있도록 합니다.\\n위의 모든 내용을 정리해서 사업실현((개선/개발) 및 구체화 방안을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【사업 실현 및 구체화 방안】\\n\\n1. 특허 출원 기술 활용:\\n\"활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법\" 특허 출원 중인 기술을 기반으로 플랫폼 내 회원들의 활동 정보를 분석하고, 이를 통해 등급을 결정하여 맞춤화된 서비스와 혜택을 제공합니다.\\n2. 기술 개발 및 최적화:\\n연구개발팀과 협력 기관들과 협업하여 특허 출원 중인 기술의 완성도를'},\n",
       "  {'from': 'human',\n",
       "   'value': 'API개발 및 소비보상 이커머스 플랫폼 구축:\\n2023년 12월까지 비즈니스 모델을 구현할 수 있는 API를 개발하고, 소비자들이 보상을 받을 수 있는 이커머스 플랫폼을 구축합니다. 이를 통해 제조사, 소비자, 제휴사들이 상호 혜택을 누릴 수 있는 환경을 조성할 예정입니다.\\n내용을 포함해서 사업실현 및 구체화 방안을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '【사업 실현 및 구체화 방안】\\n\\n1. API 개발 및 소비보상 이커머스 플랫폼 구축:\\n2023년 12월까지 비즈니스 모델을 구현할 수 있는 API를 개발하고, 소비자들이 보상을 받을 수 있는 이커머스 플랫폼을 구축합니다. 이를 통해 제조사, 소비자, 제휴사들이 상호 혜택을 누릴 수 있는 환경을 조성합니다.\\n2. 특허 출원 기술 활용:\\n\"활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법\" 특허 출원 중인 기술을 기반으로 플랫폼 내 회원들의 활동 정보를 분석하고, 이를 통해 등급을 결정하여 맞춤화된 서비스와 혜택을 제공합니다.\\n3. 기술 개발 및 최적화:\\n연구개발팀과 협력 기관들과 협업하여 특허 출원 중인 기술의 완성도를 높이고, 최신 기술 동향과 고객 요구사항을 반영하여 지속적인 개선 및 최적화를 추진합니다.\\n4. 플랫폼 구축 및 통합:\\n회원 등급결정 플랫폼을 구축하고, 기존 이커머스 플랫폼과 통합하여 사용자들이 보다 간편하게 이용할 수 있도록 합니다. 이를 통해 고객의 편의성과 사용 경험을 높일 수 있습니다.\\n5. 데이터 분석 및 활용:\\n플랫폼에서 발생하는 활동 정보 데이터를 분석하여, 회원 등급 결정 뿐만 아니라 마케팅 전략 및 제품 개선에도 활용합니다. 이를 통해 효율적인 고객 관리와 함께, 시장의 변화에 민첩하게 대응할 수 있는 사업 전략을 수립합니다.\\n6. 특허 출원 완료 및 확보:\\n특허 출원 절차를 원활하게 진행하여 특허를 확보하고, 이를 통해 경쟁사로부터의 기술 침해를 방지합니다. 또한, 특허 확보를 통해 사업'},\n",
       "  {'from': 'human', 'value': '계속'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '의 지속적인 성장과 차별화된 경쟁력을 유지할 수 있도록 합니다.\\n\\n1. 파트너십 및 네트워크 구축:\\n제조사, 소비자, 제휴사와의 협력을 적극 추진하여 파트너십 및 네트워크를 구축합니다. 이를 통해 서로의 노하우와 자원을 공유하며 상호 발전할 수 있는 기반을 마련합니다.\\n2. 마케팅 및 홍보 전략 수립:\\n타겟 시장에 맞춘 효과적인 마케팅 및 홍보 전략을 수립하여 플랫폼의 인지도를 높이고, 신규 고객 유치와 기존 고객의 충성도를 향상시킵니다.\\n3. 지속적인 모니터링 및 피드백 수집:\\n플랫폼의 성공적인 운영을 위해 지속적인 모니터링을 진행하며, 고객 및 파트너로부터의 피드백을 수집하여 개선 사항을 파악하고 적극 반영합니다.\\n4. 사업 성장 및 확장 계획:\\n초기 사업 목표를 달성한 후, 성공적인 사례를 바탕으로 사업 영역을 확장하고, 다양한 산업 분야 및 국가에 서비스를 제공함으로써 사업의 성장과 확장을 추진합니다.\\n\\n이러한 사업 실현 및 구체화 방안을 통해, 기술적 역량과 시장 트렌드를 반영한 차별화된 이커머스 플랫폼을 성공적으로 구축하고 운영할 수 있을 것으로 기대됩니다. 이를 바탕으로 제조사, 소비자, 제휴사 간의 상호 혜택을 극대화하고, 지속적인 사업 성장과 확장을 추구할 계획입니다.'},\n",
       "  {'from': 'human', 'value': '수익창출 등을 포함한 비즈니스 모델을 만들어줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '비즈니스 모델:\\n\\n1. 플랫폼 이용료 및 수수료:\\n제조사와 제휴사들에게 플랫폼의 이용료를 부과하고, 판매된 제품에 대한 수수료를 받습니다. 이를 통해 안정적인 수익을 창출할 수 있습니다.\\n2. 프리미엄 서비스 및 멤버십 패키지:\\n회원 등급에 따라 프리미엄 서비스 및 멤버십 패키지를 제공하며, 이에 대한 추가 요금을 부과합니다. 프리미엄 회원들에게는 더 많은 혜택과 서비스를 제공하여 고객 만족도를 높입니다.\\n3. 광고 및 프로모션:\\n플랫폼에서 제조사와 제휴사들의 광고 및 프로모션을 진행하며, 이를 통해 광고 수익을 창출합니다. 광고 측면에서도 사용자에게 맞춤화된 정보를 제공하여 고객 경험을 향상시킬 수 있습니다.\\n4. 데이터 분석 서비스:\\n플랫폼에서 발생하는 활동 정보 데이터를 분석하여, 제조사와 제휴사들에게 데이터 분석 서비스를 판매합니다. 이를 통해 기업들이 시장 트렌드와 소비자 행동 패턴을 이해하고 전략을 수립하는 데 도움을 줍니다.\\n5. 소비 보상 프로그램:\\n소비자들이 제품 개발에 참여할 수 있도록 하여, 제품 구매 시 일정 비율의 소비 보상을 제공합니다. 이를 통해 고객들의 구매 의욕을 높이고, 재구매율을 증가시킵니다.\\n6. 확장된 파트너십:\\n다양한 업계와의 파트너십을 통해 새로운 시장 기회를 탐색하고, 그 과정에서 파트너사와 함께 수익을 창출합니다.\\n\\n이러한 비즈니스 모델을 통해 수익 창출의 다양한 경로를 마련하며, 제조사, 소비자, 제휴사 간의 상호 혜택을 극대화하고 지속적인 사업 성장을 추구할 수 있습니다.'},\n",
       "  {'from': 'human', 'value': '좀더 구체적이고 명확한 내용과 수치가 들어가있는 비즈니스 모델을 만들어줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '비즈니스 모델:\\n\\n1. 플랫폼 이용료 및 수수료:\\n제조사와 제휴사들에게 월 이용료 $500을 부과하고, 판매된 제품에 대한 수수료를 5% 받습니다. 이를 통해 안정적인 수익을 창출할 수 있습니다.\\n2. 프리미엄 서비스 및 멤버십 패키지:\\n회원 등급에 따라 프리미엄 서비스 및 멤버십 패키지를 제공하며, 이에 대한 추가 요금을 부과합니다. 프리미엄 회원들에게는 더 많은 혜택과 서비스를 제공하여 고객 만족도를 높입니다. 가격은 기본 회원 $0, 실버 회원 $9.99/월, 골드 회원 $19.99/월로 설정합니다.\\n3. 광고 및 프로모션:\\n플랫폼에서 제조사와 제휴사들의 광고 및 프로모션을 진행하며, 이를 통해 광고 수익을 창출합니다. 배너 광고는 월 $1,000, 프로모션 글은 게시당 $300으로 설정합니다.\\n4. 데이터 분석 서비스:\\n플랫폼에서 발생하는 활동 정보 데이터를 분석하여, 제조사와 제휴사들에게 데이터 분석 서비스를 판매합니다. 이를 통해 기업들이 시장 트렌드와 소비자 행동 패턴을 이해하고 전략을 수립하는 데 도움을 줍니다. 분석 보고서는 기본 패키지 $500/월, 고급 패키지 $1,000/월로 설정합니다.\\n5. 소비 보상 프로그램:\\n소비자들이 제품 개발에 참여할 수 있도록 하여, 제품 구매 시 3%의 소비 보상을 제공합니다. 이를 통해 고객들의 구매 의욕을 높이고, 재구매율을 증가시킵니다.\\n6. 확장된 파트너십:\\n다양한 업계와의 파트너십을 통해 새로운 시장 기회를 탐색하고, 그 과정에서 파트너사와 함께 수익을 창출합니다. 예를 들어, 물류 회사와 파트너십을 맺어 배송비 할인을 제공하거나, 금융 기관과 협력해 소비자 대출 서비스를 제공합니다.\\n\\n이러한 비즈니스 모'},\n",
       "  {'from': 'human',\n",
       "   'value': '1.플랫폼이용수수료없음, 프리밈엄 서비스 및 멤버십 패키지 없음, 광고안함\\n2. 이커머스 본사직접 운영 제품마진률에따라 5~10% 수익발생\\n3. 소비보상플랫폼을 통해 회원증가 및 판매증가에 따른 수익발생\\n4. 마일리지 생활비 및 금융자산에 사용시 1~3%의 금액을 제휴사에서 지급\\n5. 제품개발 고객참여 시 제조사로 부터 1~3%, 소비자 보상에서 1~3% 수익발생\\n위의 내용을 추가해서 비즈니스 모델을 만들어줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '비즈니스 모델:\\n\\n1. 제품 마진 수익:\\n이커머스 플랫폼을 본사에서 직접 운영하며, 제품 마진률에 따라 5~10%의 수익을 발생시킵니다. 이를 통해 안정적인 수익 기반을 구축합니다.\\n2. 소비 보상 플랫폼을 통한 회원 증가 및 판매 증가:\\n소비자들에게 제공되는 소비 보상 플랫폼을 통해 회원 증가와 판매량 증가를 이루어 냅니다. 이를 통해 꾸준한 수익 성장을 기대할 수 있습니다.\\n3. 마일리지 생활비 및 금융자산 사용 수익:\\n사용자들이 마일리지를 생활비 또는 금융자산으로 사용할 경우, 1~3%의 금액을 제휴사로부터 지급받아 수익을 창출합니다. 이를 통해 다양한 서비스를 제공하며, 사용자들의 만족도를 높입니다.\\n4. 제품 개발 고객 참여 수익:\\n제품 개발 과정에서 고객들이 참여할 수 있도록 하여, 제조사로부터 13%의 수익을 발생시키며, 소비자 보상으로도 13%의 수익을 얻습니다. 이를 통해 제품 개발 과정에 사용자들의 참여를 유도하고, 고객 만족도를 높입니다.\\n\\n이러한 비즈니스 모델을 통해 다양한 수익원을 확보하고, 지속적인 성장을 추구할 수 있습니다. 사용자들의 만족도와 경쟁력을 높이는 동시에, 안정적인 수익 창출을 위해 지속적인 노력을 기울입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '정의된 목표시장(고객) 내 세부 고객, 경쟁사, 시장 내 대체하고자 하는 제품·서비스의 주요 정보 분석 등을 통해 사업 진출(시장 진입 등) 전략 수립해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '1. 세부 고객 분석:\\n목표시장 내 세부 고객을 다음과 같이 분류합니다.\\n* 일반 소비자: 플랫폼에서 제품을 구매하는 개인 소비자\\n* 기업 고객: 플랫폼에 제품을 판매하고자 하는 기업\\n1. 경쟁사 분석:\\n주요 경쟁사와 그들이 제공하는 서비스를 분석하여 차별화된 전략을 세울 수 있습니다. 경쟁사에 대한 정보를 수집하고, 그들의 강점과 약점을 파악합니다.\\n2. 대체 제품·서비스 분석:\\n시장 내 대체하고자 하는 제품·서비스에 대한 정보를 수집하고 분석합니다. 이를 통해 대체 제품·서비스가 제공하지 못하는 가치를 찾아내어, 차별화된 서비스를 제공할 수 있습니다.\\n\\n사업 진출 전략:\\n\\n1. 가격 전략:\\n경쟁사와 비교하여 합리적인 가격대를 설정하여 소비자들에게 경쟁력 있는 제품을 제공합니다. 이를 통해 시장 점유율을 확보할 수 있습니다.\\n2. 마케팅 전략:\\n소비자들의 인지도를 높이기 위해 다양한 마케팅 전략을 시행합니다. SNS, 온라인 광고, 이벤트 등을 활용하여 소비자들에게 브랜드 인지도를 높입니다.\\n3. 제휴 전략:\\n시장 진입을 원활하게 진행하기 위해, 현지 기업이나 유통 업체와 제휴를 맺습니다. 이를 통해 시장 진입 장벽을 낮추고, 시장 점유율을 빠르게 확보할 수 있습니다.\\n4. 서비스 품질 및 고객 지원 전략:\\n고객 만족도를 높이기 위해, 높은 품질의 서비스를 제공하며, 효과적인 고객 지원을 시행합니다. 이를 통해 고객의 신뢰와 충성도를 얻을 수 있습니다.\\n5. 지속적인 기술 개발 및 혁신 전략:\\n시장에서 지속적으로 성공하기 위해, 지속적인 기술 개발 및 혁신에 투자합니다. 이를 통해 시장의 변화에 대응할 수 있고, 지속적인 경쟁력을'},\n",
       "  {'from': 'human',\n",
       "   'value': 'ActivityRewards플랫폼의 시장진입 및 사업화 전략을 실행가능하고 현실성있게 만들어줘 예를들어 회원확보, 수익확대,인력및네트워크등\\n2023년12월31일까지 달성가능한 사업성과(매출, 투자, 고용 등)을 기재'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'ActivityRewards 플랫폼의 시장 진입 및 사업화 전략:\\n\\n1. 회원 확보 전략:\\n* 소셜 미디어, 온라인 마케팅 및 인플루언서를 활용하여 브랜드 인지도를 높이고 회원을 확보합니다.\\n* 프로모션 및 이벤트를 통해 초기 회원 가입을 유도하고, 추천 프로그램을 통해 기존 회원의 지인 유입을 촉진합니다.\\n1. 수익 확대 전략:\\n* 제품 판매 마진을 통한 수익 창출: 이커머스 플랫폼 내 직접 운영 제품의 판매를 통해 5~10%의 수익을 창출합니다.\\n* 제휴사와의 협력을 통한 수익 창출: 제품 개발에 참여한 제조사 및 소비자들로부터 1~3%의 수익을 창출합니다.\\n1. 인력 및 네트워크 확보 전략:\\n* 전문 인력을 채용하여 운영, 마케팅, 개발 등 각 부문의 업무를 원활하게 수행할 수 있도록 합니다.\\n* 산업 내의 다양한 기업과 제휴를 통해 네트워크를 확장하고, 시장 점유율을 높입니다.\\n\\n2023년 12월 31일까지 달성 가능한 사업 성과:\\n\\n1. 매출:\\n* 초기 회원 확보를 통한 매출 목표: 5,000명의 회원 확보를 목표로 하며, 회원 당 연간 평균 소비 금액을 $500으로 가정할 경우, 연간 매출은 약 $2,500,000입니다.\\n1. 투자:\\n* 시장 진입 및 초기 사업 성장을 위한 투자자로부터의 투자금 확보: $1,000,000\\n1. 고용:\\n* 초기 인력 확보: 운영팀 5명, 마케팅팀 5명, 개발팀 5명, 고객 지원팀 3명 등 총 18명의 전문 인력을 채용합니다.\\n\\n이러한 전략을 통해 ActivityRewards 플랫폼은 시장 진입 및 사업화에 성공하며, 지속적으로 성장할 수 있는 기반을 구축할 것입니다.'},\n",
       "  {'from': 'human', 'value': '원화로 표시해서 다시 작성해줘 그리고 사업화 전략을 2가지만 더 추가해서 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'ActivityRewards 플랫폼의 시장 진입 및 사업화 전략:\\n\\n1. 회원 확보 전략:\\n* 소셜 미디어, 온라인 마케팅 및 인플루언서를 활용하여 브랜드 인지도를 높이고 회원을 확보합니다.\\n* 프로모션 및 이벤트를 통해 초기 회원 가입을 유도하고, 추천 프로그램을 통해 기존 회원의 지인 유입을 촉진합니다.\\n1. 오프라인 프로모션 활동:\\n* 지역 커뮤니티 및 이벤트에 참여하여 브랜드 인지도를 높이고 고객 확보를 도모합니다.\\n* 오프라인 매장 및 체험 공간을 설치하여 직접적인 고객 접점을 갖습니다.\\n1. 제휴사 확대 전략:\\n* 다양한 업종의 제휴사를 확대하여 고객이 플랫폼에서 받는 혜택을 다양화하고, 이용률을 높입니다.\\n* 제휴사의 고객들에게도 플랫폼 회원이 될 수 있는 기회를 제공하여 회원 확보에 기여합니다.\\n\\n2023년 12월 31일까지 달성 가능한 사업 성과 (원화 기준):\\n\\n1. 매출:\\n* 초기 회원 확보를 통한 매출 목표: 5,000명의 회원 확보를 목표로 하며, 회원 당 연간 평균 소비 금액을 ₩600,000으로 가정할 경우, 연간 매출은 약 ₩3,000,000,000입니다.\\n1. 투자:\\n* 시장 진입 및 초기 사업 성장을 위한 투자자로부터의 투자금 확보: ₩1,200,000,000\\n1. 고용:\\n* 초기 인력 확보: 운영팀 5명, 마케팅팀 5명, 개발팀 5명, 고객 지원팀 3명 등 총 18명의 전문 인력을 채용합니다.\\n\\n이러한 전략을 통해 ActivityRewards 플랫폼은 시장 진입 및 사업화에 성공하며, 지속적으로 성장할 수 있는 기반을 구축할 것입니다.'},\n",
       "  {'from': 'human', 'value': '본 사업의 전체 로드맵을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'ActivityRewards 플랫폼 사업 로드맵:\\n\\n1. 2023년 1분기: 기획 및 준비 단계\\n* 시장조사 및 경쟁사 분석\\n* 비즈니스 모델 및 전략 수립\\n* 초기 투자자 확보\\n1. 2023년 2분기: 개발 단계\\n* 플랫폼 및 API 개발 시작\\n* 인력 채용 (운영팀, 마케팅팀, 개발팀, 고객 지원팀)\\n* 제휴사 확보 시작\\n1. 2023년 3분기: 테스트 및 마케팅 단계\\n* 베타 테스트 진행 및 피드백 수집\\n* 온라인 및 오프라인 마케팅 전략 수립 및 실행\\n* 제휴사 확대 및 프로모션 협력\\n1. 2023년 4분기: 론칭 및 초기 운영 단계\\n* 플랫폼 정식 론칭\\n* 회원 확보 전략 실행 (소셜 미디어, 이벤트, 추천 프로그램 등)\\n* 오프라인 프로모션 활동 진행 (지역 커뮤니티 참여, 체험 공간 설치 등)\\n1. 2024년 1분기: 성장 및 확장 단계\\n* 제휴사 확대 및 다양한 업종으로 확장\\n* 회원 수 증가 및 활성화 전략 실행\\n* 플랫폼 서비스 개선 및 최적화\\n1. 2024년 2분기 이후: 지속적 성장 및 안정화 단계\\n* 지속적인 고객 확보 및 이용률 증가\\n* 추가 투자 확보 및 인력 확충\\n* 사업 영역 확장 (글로벌 시장 진출, 다양한 소비보상 서비스 제공 등)\\n\\n이러한 로드맵을 따라 ActivityRewards 플랫폼은 초기 시장 진입부터 성장 및 안정화에 이르기까지 전략을 실행하며, 지속적으로 사업을 확장할 것입니다.'},\n",
       "  {'from': 'human', 'value': '2023년5월~2024년1월 까지 로드맵을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '2023년 5월 ~ 2024년 1월 ActivityRewards 플랫폼 사업 로드맵:\\n\\n1. 2023년 5월 - 6월: 기획 및 준비 단계\\n* 비즈니스 모델 및 전략 수정\\n* 초기 투자자와의 계약 조정\\n* 인력 채용 계획 수립\\n1. 2023년 7월 - 9월: 개발 단계\\n* 플랫폼 및 API 개발 시작\\n* 인력 채용 (운영팀, 마케팅팀, 개발팀, 고객 지원팀)\\n* 제휴사 확보 시작\\n1. 2023년 10월 - 11월: 테스트 및 마케팅 단계\\n* 베타 테스트 진행 및 피드백 수집\\n* 온라인 및 오프라인 마케팅 전략 수립 및 실행\\n* 제휴사 확대 및 프로모션 협력\\n1. 2023년 12월: 론칭 및 초기 운영 단계\\n* 플랫폼 정식 론칭\\n* 회원 확보 전략 실행 (소셜 미디어, 이벤트, 추천 프로그램 등)\\n* 오프라인 프로모션 활동 진행 (지역 커뮤니티 참여, 체험 공간 설치 등)\\n1. 2024년 1월: 성장 단계 시작\\n* 제휴사 확대 및 다양한 업종으로 확장\\n* 회원 수 증가 및 활성화 전략 실행\\n* 플랫폼 서비스 개선 및 최적화\\n\\n위의 로드맵을 따라 2023년 5월부터 2024년 1월까지 ActivityRewards 플랫폼은 기획, 개발, 테스트, 론칭 및 초기 성장 단계를 진행합니다. 이 과정을 통해 시장에 플랫폼을 성공적으로 론칭하고 초기 회원 확보와 서비스 확장을 추진할 것입니다.'},\n",
       "  {'from': 'human', 'value': '1억 으로 자금집행계획을 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '1억 원의 자금 집행 계획은 다음과 같이 구성할 수 있습니다:\\n\\n1. 플랫폼 개발 (30%): 3,000만 원\\n* API 및 시스템 개발\\n* 프론트엔드 및 백엔드 개발\\n* 테스트 및 디버깅\\n1. 인력 채용 (25%): 2,500만 원\\n* 개발팀\\n* 운영팀\\n* 마케팅팀\\n* 고객 지원팀\\n1. 마케팅 및 홍보 (20%): 2,000만 원\\n* 온라인 광고 (소셜 미디어, 검색 엔진 마케팅 등)\\n* 오프라인 프로모션 (지역 커뮤니티 참여, 체험 공간 설치 등)\\n* 이벤트 및 고객 확보 전략 실행 (추천 프로그램, 체험 이벤트 등)\\n1. 제휴 및 협력사 확보 (10%): 1,000만 원\\n* 제휴사 영업 활동\\n* 협력사와의 계약 체결\\n* 프로모션 협력\\n1. 서버 및 인프라 비용 (5%): 500만 원\\n* 클라우드 서버 구축\\n* 백업 및 보안 시스템 구축\\n* 시스템 유지보수 비용\\n1. 법률 및 특허비용 (5%): 500만 원\\n* 특허 출원 및 관련 절차 비용\\n* 법률 자문 및 계약 비용\\n1. 기타 운영비용 (5%): 500만 원\\n* 사무실 임대료\\n* 장비 구매 및 유지비용\\n* 여비 및 소모품 비용\\n\\n총 1억 원의 자금을 위와 같이 계획적으로 집행하면, 플랫폼 개발, 인력 채용, 마케팅, 제휴 및 협력사 확보, 서버 및 인프라 구축, 법률 및 특허비용, 기타 운영비용에 필요한 자금을 효율적으로 사용할 수 있습니다.'},\n",
       "  {'from': 'human', 'value': '인력채용과 기타운영비용을 제외 총 8000만원으로 자금집행계획을 작서해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '8000만 원의 자금 집행 계획은 다음과 같이 구성할 수 있습니다:\\n\\n1. 플랫폼 개발 (40%): 3,200만 원\\n* API 및 시스템 개발\\n* 프론트엔드 및 백엔드 개발\\n* 테스트 및 디버깅\\n1. 마케팅 및 홍보 (25%): 2,000만 원\\n* 온라인 광고 (소셜 미디어, 검색 엔진 마케팅 등)\\n* 오프라인 프로모션 (지역 커뮤니티 참여, 체험 공간 설치 등)\\n* 이벤트 및 고객 확보 전략 실행 (추천 프로그램, 체험 이벤트 등)\\n1. 제휴 및 협력사 확보 (15%): 1,200만 원\\n* 제휴사 영업 활동\\n* 협력사와의 계약 체결\\n* 프로모션 협력\\n1. 서버 및 인프라 비용 (10%): 800만 원\\n* 클라우드 서버 구축\\n* 백업 및 보안 시스템 구축\\n* 시스템 유지보수 비용\\n1. 법률 및 특허비용 (10%): 800만 원\\n* 특허 출원 및 관련 절차 비용\\n* 법률 자문 및 계약 비용\\n\\n총 8000만 원의 자금을 위와 같이 계획적으로 집행하면, 플랫폼 개발, 마케팅, 제휴 및 협력사 확보, 서버 및 인프라 구축, 법률 및 특허비용에 필요한 자금을 효율적으로 사용할 수 있습니다.'},\n",
       "  {'from': 'human', 'value': '제휴 및 협력사 확보제외 다시 만들어줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '제휴 및 협력사 확보를 제외한 총 8800만 원의 자금 집행 계획은 다음과 같이 구성할 수 있습니다:\\n\\n1. 플랫폼 개발 (45%): 3,960만 원\\n* API 및 시스템 개발\\n* 프론트엔드 및 백엔드 개발\\n* 테스트 및'},\n",
       "  {'from': 'human', 'value': '제휴 및 협력사 확보제외 총자금 8000만원 으로 다시 만들어줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '제휴 및 협력사 확보를 제외한 총 8000만 원의 자금 집행 계획은 다음과 같이 구성할 수 있습니다:\\n\\n1. 플랫폼 개발 (45%): 3,600만 원\\n* API 및 시스템 개발\\n* 프론트엔드 및 백엔드 개발\\n* 테스트 및 최적화 작업\\n1. 마케팅 (30%): 2,400만 원\\n* 온라인 및 오프라인 광고 및 프로모션\\n* 고객 확보 및 유지를 위한 이벤트 및 캠페인\\n* SNS 및 커뮤니티 관리\\n1. 인프라 및 서비스 운영 비용 (15%): 1,200만 원\\n* 서버 및 클라우드 서비스 비용\\n* 데이터 저장 및 백업 관련 비용\\n* 기타 서비스 운영에 필요한 인프라 비용\\n1. 기술 및 보안 (10%): 800만 원\\n* 개인정보보호 및 보안 관련 인프라 구축\\n* 기술 지원 및 유지 보수 비용\\n* 보안 감사 및 인증비용\\n\\n총 자금: 8000만 원'},\n",
       "  {'from': 'human',\n",
       "   'value': '재료비, 외주용역비, 특허권등 무형자산취득비, 인건비, 광고선전비 등 비목으로 구분해서 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '제휴 및 협력사 확보를 제외한 총 8000만 원의 자금 집행 계획을 비목별로 구분하여 작성하겠습니다:\\n\\n1. 재료비 (20%): 1,600만 원\\n* 플랫폼 개발에 필요한 소프트웨어 라이선스 구매\\n* 사무용품 및 기타 소모품 구매\\n1. 외주용역비 (35%): 2,800만 원\\n* 프로젝트 관리 및 기술 컨설팅\\n* 프론트엔드 및 백엔드 개발 외주\\n* 보안 및 개인정보보호 관련 외주 서비스\\n1. 무형자산 취득비 (10%): 800만 원\\n* 특허 출원 및 관리 비용\\n* 기술 및 소프트웨어 라이선스 구매 비용\\n1. 인건비 (20%): 1,600만 원\\n* 기술 개발 및 운영 관련 인력 고용\\n* 마케팅 및 사업 관리 인력 고용\\n1. 광고선전비 (15%): 1,200만 원\\n* 온라인 및 오프라인 광고 및 프로모션\\n* 고객 확보 및 유지를 위한 이벤트 및 캠페인\\n* SNS 및 커뮤니티 관리\\n\\n총 자금: 8000만 원'},\n",
       "  {'from': 'human', 'value': '대표이사 인건비 2000만원 추가해서 총 1억원으로 다시 정리해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '대표이사 인건비 2000만 원을 포함하여 총 1억 원의 자금 집행 계획을 비목별로 구분하여 작성하겠습니다:\\n\\n1. 재료비 (16%): 1,600만 원\\n* 플랫폼 개발에 필요한 소프트웨어 라이선스 구매\\n* 사무용품 및 기타 소모품 구매\\n1. 외주용역비 (28%): 2,800만 원\\n* 프로젝트 관리 및 기술 컨설팅\\n* 프론트엔드 및 백엔드 개발 외주\\n* 보안 및 개인정보보호 관련 외주 서비스\\n1. 무형자산 취득비 (8%): 800만 원\\n* 특허 출원 및 관리 비용\\n* 기술 및 소프트웨어 라이선스 구매 비용\\n1. 인건비 (16%): 1,600만 원\\n* 기술 개발 및 운영 관련 인력 고용\\n* 마케팅 및 사업 관리 인력 고용\\n1. 광고선전비 (12%): 1,200만 원\\n* 온라인 및 오프라인 광고 및 프로모션\\n* 고객 확보 및 유지를 위한 이벤트 및 캠페인\\n* SNS 및 커뮤니티 관리\\n1. 대표이사 인건비 (20%): 2,000만 원\\n* 대표이사 급여 및 기타 복리후생비용\\n\\n총 자금: 1억 원'},\n",
       "  {'from': 'human', 'value': '문장으로 정리해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '총 1억 원의 자금 집행 계획은 다음과 같이 구성됩니다. 재료비 1,600만 원으로 소프트웨어 라이선스 및 사무용품 구매를 진행합니다. 외주용역비 2,800만 원은 프로젝트 관리, 기술 컨설팅, 프론트엔드 및 백엔드 개발 외주, 보안 및 개인정보보호 관련 외주 서비스를 위해 사용됩니다. 무형자산 취득비 800만 원은 특허 출원 및 관리, 기술 및 소프트웨어 라이선스 구매 비용을 지불하는 데 사용됩니다. 인건비 1,600만 원은 기술 개발, 운영, 마케팅 및 사업 관리 인력 고용에 할당됩니다. 광고선전비 1,200만 원은 온라인 및 오프라인 광고, 프로모션, 고객 확보 및 유지를 위한 이벤트 및 캠페인, 그리고 SNS 및 커뮤니티 관리에 사용됩니다. 마지막으로 대표이사 인건비 2,000만 원은 대표이사 급여 및 기타 복리후생 비용을 지불하는 데 사용됩니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '자사 Activity Rewards 플랫폼 : 활동정보 기반 회원 등급결정 및 지속가능한 소비보상 플랫폼 목표 총 1억 원의 자금 집행 계획은 다음과 같이 구성됩니다. 외주용역비 4,000만 원은 프로젝트 관리, 기술 컨설팅, 프론트엔드 및 백엔드 개발 외주, 보안 및 개인정보보호 관련 외주 서비스를 위해 사용됩니다. 무형자산 취득비 800만 원은 특허 출원 및 관리, 기술 및 소프트웨어 라이선스 구매 비용을 지불하는 데 사용됩니다. 광고선전비 2,200만 원은 온라인 및 오프라인 광고, 프로모션, 고객 확보 및 유지를 위한 이벤트 및 캠페인, 그리고 SNS 및 커뮤니티 관리에 사용됩니다. 마지막으로 사업계발 및 플랫폼개발을 위한 대표자 인건비 1,000만원 기술개발 관련 직원의 인건비로 2,000만원은 자사 대응자금으로 충당합니다.\\n위의 내용을 비목별로 정리해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '자사 Activity Rewards 플랫폼의 목표인 활동정보 기반 회원 등급결정 및 지속가능한 소비보상 플랫폼 구축을 위한 총 1억 원의 자금 집행 계획은 다음과 같이 구성됩니다.\\n\\n1. 외주용역비: 4,000만 원\\n* 프로젝트 관리, 기술 컨설팅, 프론트엔드 및 백엔드 개발 외주, 보안 및 개인정보보호 관련 외주 서비스\\n1. 무형자산 취득비: 800만 원\\n* 특허 출원 및 관리, 기술 및 소프트웨어 라이선스 구매 비용\\n1. 광고선전비: 2,200만 원\\n* 온라인 및 오프라인 광고, 프로모션, 고객 확보 및 유지를 위한 이벤트 및 캠페인, SNS 및 커뮤니티 관리\\n1. 인건비\\n* 대표자 인건비: 1,000만 원\\n* 기술개발 관련 직원 인건비: 2,000만 원\\n1. 자사 대응자금: 2,000만 원\\n* 사업계발 및 플랫폼 개발을 위한 예비 자금'},\n",
       "  {'from': 'human',\n",
       "   'value': '◦ ‘00년 엔젤투자자와 액셀러레이터로부터 Seed 투자 유치 : 000원\\n◦ ’00년 0분기를 목표로 후속 투자유치 활동 : 목표 000원\\n◦ 데이터바우처 지원사업 활용을 통한 빅데이터 수집 및 분석 : 000원\\n◦ 기술/신용보증기금 보증을 통한 대출 : 000원\\n◦ 정부/지자체 창업지원사업(도약패키지) 수주 : 000원\\n위의 내용으로 자금유치 계획을 만들어줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '◦ 2023년 엔젤투자자와 액셀러레이터로부터 Seed 투자 유치: 500만 원\\n◦ 2024년 1분기를 목표로 후속 투자유치 활동: 목표 1,500만 원\\n◦ 데이터바우처 지원사업 활용을 통한 빅데이터 수집 및 분석: 300만 원\\n◦ 기술/신용보증기금 보증을 통한 대출: 1,000만 원\\n◦ 정부/지자체 창업지원사업(도약패키지) 수주: 800만 원\\n\\n위의 내용을 바탕으로 자금유치 계획은 다음과 같이 구성됩니다. 2023년에 엔젤투자자와 액셀러레이터로부터 Seed 투자를 통해 500만 원의 자금을 유치할 계획이며, 이어서 2024년 1분기를 목표로 후속 투자유치 활동을 통해 추가로 1,500만 원의 자금을 확보할 계획입니다. 또한 데이터바우처 지원사업을 활용하여 빅데이터 수집 및 분석에 필요한 자금 300만 원을 지원받을 예정입니다. 기술/신용보증기금 보증을 통한 대출을 통해 1,000만 원의 자금을 조달할 계획이고, 마지막으로 정부 및 지자체 창업지원사업인 도약패키지를 수주하여 800만 원의 자금을 확보할 예정입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '내가 사업을 성공할 수 있는 대표이사인지에 관한 보유역량을 적어야되 나에게 필요한 질문을 하면 나의 답변에 따라 나의 역량을 정리해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '1. 당신의 이전 경력 및 역할에 대해 말씀해 주시겠습니까?\\n2. 이 사업 아이디어를 어떻게 생각해 내셨고, 왜 이 아이디어에 대해 열정을 느끼시나요?\\n3. 관련 산업에 대한 이해도는 어느 정도인가요? 경험이 있다면, 그 경험은 어떻게 도움이 되나요?\\n4. 리더십과 팀 관리에 대한 경험과 역량은 어떻게 평가하시나요? 구체적인 사례를 들어 주시겠습니까?\\n5. 현재 사업에 직면한 가장 큰 도전 과제는 무엇이며, 이를 극복하기 위해 어떤 전략을 세우고 있나요?\\n6. 스트레스 상황에서 의사 결정을 내리는 데 어떤 원칙이나 방법론을 따르시나요?\\n7. 현재 사업에서 가장 중요하게 생각하는 핵심 가치는 무엇이며, 이를 팀과 함께 실천하려고 어떤 노력을 하고 있나요?\\n\\n위의 질문에 대한 답변을 바탕으로, 당신의 역량을 정리해드리겠습니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '1. 주식회사 바비조아 대표이사 2010년~현재 마케팅 전문가 자격증보유, 온,오프라인으로 유통망 구축, 온, 오프라인 영영책임.\\n2. 최종 소비자가에 제품이 전달되기 까지 너무나 많은 비용들이\\xa0발생하고 있으며 그 목은 오로지 소비자가 부담하고 있다는 것에 늘 미안함과 안타까움을 느끼게 되었음. 이익되는 소비생활이라는 의미로 소비와 보상을 통해 고객의 이익과 생산자의 안정적인 성장에 목적을 두고 아이디어와 특허를 출원함.\\n3. 관련사업에 대한 이해도는 높지 않음. 지금 까지 식품제조회사와 유통, 영업쪽으로 대부분 활동. \\n4. 약 20년간의 사업을 하면서 다양한 경험을 하였으면 새로운 것에 대한 도전과 열정이 많음.\\n5. 소비자에게 내가 만들고자 하는 플랫폼의 가치를 어떻게 전달할것인가에 대한 고민을 하고 있으며 다양한 온, 오프라인 만남을 통해 고객에게 돌아갈 수 있는 혜택에 대한 설명을 하고자함.\\n제조사, 판매사, 소비자 모두가 상호 이익될 수 있는 새로운 유통시스템 구축이 반드시 필요함.\\n6. 구조와 인과를 파악하고 본질에 충실하자\\n7. 핵심가치는 상생과 이익'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '당신의 보유 역량을 바탕으로 정리하겠습니다:\\n\\n1. 경력 및 경험: 주식회사 바비조아 대표이사로 2010년부터 활동하며, 마케팅 전문가 자격증을 보유하고 있습니다. 온라인과 오프라인 유통망 구축 및 영업 책임자로 활약한 경험이 있습니다.\\n2. 사업 아이디어 및 열정: 소비자가 부담하는 불필요한 비용을 줄이고, 소비와 보상을 결합한 이익되는 소비생활을 제공하기 위해 특허를 출원하고 사업 아이디어를 구상했습니다. 이 아이디어에 대한 열정이 높습니다.\\n3. 산업 이해도: 식품 제조회사와 유통, 영업 분야에서 활동했지만, 관련 사업에 대한 이해도는 높지 않습니다. 그러나 이전 경험을 바탕으로 새로운 도전을 시작할 준비가 되어 있습니다.\\n4. 리더십 및 팀 관리: 20년간의 사업 경험을 통해 다양한 경험을 쌓았으며, 새로운 것에 대한 도전과 열정이 많습니다.\\n5. 도전 과제 및 전략: 소비자에게 플랫폼의 가치를 어떻게 전달할지에 대한 고민을 하고 있으며, 온라인과 오프라인 만남을 통해 고객에게 돌아갈 수 있는 혜택에 대해 설명할 계획입니다. 제조사, 판매사, 소비자 모두 상호 이익이 가능한 유통 시스템 구축이 필요하다고 생각합니다.\\n6. 의사 결정 원칙: 구조와 인과를 파악하고 본질에 충실한 의사 결정을 내립니다.\\n7. 핵심 가치: 상생과 이익이라는 핵심 가치를 바탕으로 팀과 함께 사업을 성장시키려고 노력하고 있습니다.\\n\\n이러한 역량을 바탕으로 사업을 성공적으로 이끌어낼 수 있는 대표이사라고 할 수 있습니다.'},\n",
       "  {'from': 'human', 'value': '위의 사업을 하기위해 현재 우리에게 필요한 직원 채용은'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '위 사업을 실행하기 위해 필요한 직원 채용은 다음과 같습니다:\\n\\n1. 개발팀:\\n\\t* 프론트엔드 개발자: 플랫폼의 사용자 인터페이스를 구현하고 최적화합니다.\\n\\t* 백엔드 개발자: 서버, API, 데이터베이스 등 플랫폼의 핵심 기능을 개발하고 관리합니다.\\n\\t* 앱 개발자: 모바일 플랫폼에 최적화된 어플리케이션을 개발합니다.\\n\\t* QA 엔지니어: 플랫폼의 품질과 안정성을 확보하기 위해 테스트 및 버그 수정을 담당합니다.\\n2. 마케팅팀:\\n\\t* 디지털 마케팅 전문가: 온라인 광고 및 프로모션, SNS 관리, SEO 최적화 등을 담당합니다.\\n\\t* 오프라인 마케팅 전문가: 오프라인 이벤트, 협업 및 프로모션 전략을 기획하고 실행합니다.\\n\\t* 컨텐츠 크리에이터: 블로그, 뉴스레터, 영상 등 다양한 컨텐츠를 제작하여 고객들과 소통합니다.\\n3. 영업 및 파트너십 팀:\\n\\t* 영업 담당자: 소비자, 제조사 및 판매사와의 계약 체결, 사업 확장을 위한 영업 활동을 진행합니다.\\n\\t* 파트너십 담당자: 제휴사, 협력사와의 관계 구축 및 관리를 담당합니다.\\n4. 데이터 분석팀:\\n\\t* 데이터 분석가: 플랫폼에서 발생하는 데이터를 분석하여 최적화된 전략 및 서비스 개선을 제안합니다.\\n\\t* 빅데이터 엔지니어: 대용량 데이터 처리 및 분석을 위한 인프라를 구축하고 관리합니다.\\n5. 경영 및 관리팀:\\n\\t* 인사/총무 담당자: 인력 관리, 채용, 급여 관리, 사무실 운영 등 회사의 일상적인 관리 업무를 수행합니다.\\n\\t* 회계/재무 담당자: 회사의 재무 상태를 관리하고 예산을 계획하며, 세무 및 회계 업무를 처리합니다.\\n\\n이러한 직원들을 채용함으로써, 사업의 전반적인 운영 및'},\n",
       "  {'from': 'human', 'value': '중장기적 esg경영 도입계획 작성해줘'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '중장기적 ESG 경영 도입 계획은 다음과 같이 작성할 수 있습니다:\\n\\n1. ESG 경영 인식 제고 및 조직 문화 확산 (1년 내)\\n\\t* ESG 경영 원칙 및 가이드라인 제정\\n\\t* 임직원 대상 ESG 교육 및 워크숍 실시\\n\\t* ESG 경영을 담당할 전담 조직 구성 및 인력 배치\\n2. 환경 측면 (1~3년)\\n\\t* 에너지 효율 개선 및 친환경 에너지 사용 확대\\n\\t* 지속 가능한 원재료 사용 및 친환경 포장재 도입\\n\\t* 폐기물 및 배출가스 감축 목표 설정 및 관리\\n\\t* 물 사용량 최적화 및 폐수 처리 개선\\n\\t* 기후 변화 대응 및 탄소배출권 거래 활용 계획 수립\\n3. 사회 측면 (1~3년)\\n\\t* 다양성 및 포용성 증진 (성별, 연령, 인종, 문화 등)\\n\\t* 고용 창출 및 지역사회 발전에 기여\\n\\t* 건강 및 안전한 업무 환경 조성\\n\\t* 인권 및 노동 관련 법규 준수 및 윤리경영 강화\\n\\t* 고객 및 이해관계자와의 지속적 소통 및 협력\\n4. 지배구조 측면 (1~2년)\\n\\t* 독립적인 이사제 및 감사위원회 도입\\n\\t* 주주 및 이해관계자 의결권 강화\\n\\t* 투명한 경영 정보 공시 및 커뮤니케이션 강화\\n\\t* 부패 및 금융 부정행위 예방 및 대응 체계 구축\\n\\t* 지속 가능한 경영 성과를 반영한 보상 및 인센티브 제도 도입\\n5. ESG 경영 성과 측정 및 보고 (3년 이후)\\n\\t* ESG 관련 성과지표 설정 및 지속적 모니터링\\n\\t* 외부 감사 및 인증 기관과 협력하여 신뢰성 있는 ESG 보고서 발행\\n\\t* 지속적인 개선 및 벤치마킹을 통한 ESG 경영 수준 제고\\n\\n이러한 계획을 통해 ESG 경영을 중장기적으로 도입하고 지속적으로 개선하여 사회적'},\n",
       "  {'from': 'human', 'value': '우리 사업을 바탕으로 esg경영 도입 계획을 작성한거 맞어?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '아, 죄송합니다. 자세히 확인하지 못했습니다. 자사의 Activity Rewards 플랫폼 사업을 기반으로 한 ESG 경영 도입 계획은 다음과 같이 작성할 수 있습니다:\\n\\n1. ESG 경영 인식 제고 및 조직 문화 확산 (1년 내)\\n\\t* ESG 경영 원칙 및 가이드라인 제정\\n\\t* 임직원 대상 ESG 교육 및 워크숍 실시\\n\\t* ESG 경영을 담당할 전담 조직 구성 및 인력 배치\\n2. 환경 측면 (1~3년)\\n\\t* 서버 및 클라우드 인프라 에너지 효율 개선 및 친환경 에너지 사용 확대\\n\\t* 친환경 사무실 운영 및 원격 근무 환경 지원 강화\\n\\t* 전자 문서화 및 재활용품 사용 확대를 통한 자원 절약\\n3. 사회 측면 (1~3년)\\n\\t* 다양성 및 포용성 증진 (성별, 연령, 인종, 문화 등)\\n\\t* 공정한 협력사 및 파트너 관계 구축과 지속적인 성장 지원\\n\\t* 고객 데이터 보호 및 개인정보보호 강화\\n\\t* 이해관계자와의 지속적 소통 및 협력 강화\\n4. 지배구조 측면 (1~2년)\\n\\t* 독립적인 이사제 및 감사위원회 도입\\n\\t* 주주 및 이해관계자 의결권 강화\\n\\t* 투명한 경영 정보 공시 및 커뮤니케이션 강화\\n\\t* 부패 및 금융 부정행위 예방 및 대응 체계 구축\\n\\t* 지속 가능한 경영 성과를 반영한 보상 및 인센티브 제도 도입\\n5. ESG 경영 성과 측정 및 보고 (3년 이후)\\n\\t* ESG 관련 성과지표 설정 및 지속적 모니터링\\n\\t* 외부 감사 및 인증 기관과 협력하여 신뢰성 있는 ESG 보고서 발행\\n\\t* 지속적인 개선 및 벤치마킹을 통한 ESG 경영 수준 제고\\n\\n이 계획을 바탕으로 자사의 Activity Rewards 플랫폼 사업을 중심으로 ESG 경영을 도입하고 지속적으로 개선하여 사회'},\n",
       "  {'from': 'human', 'value': '지금까지 수고했어 고마워'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '감사합니다! 여러분의 질문에 도움이 되어 기쁩니다. 앞으로도 필요하신 정보나 도움이 필요하시면 언제든지 문의해주세요. 좋은 하루 보내세요!'}]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/lima_vicuna_format/lima_vicuna_format.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e1e47387-156e-4324-9f03-4081a0447508",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 55102\n",
      "__label__fr 384\n",
      "__label__es 386\n",
      "__label__km 508\n",
      "__label__id 51\n",
      "__label__ur 6\n",
      "__label__ko 470\n",
      "__label__zh 289\n",
      "__label__it 224\n",
      "__label__pt 208\n",
      "__label__vi 52\n",
      "__label__ro 24\n",
      "__label__nl 70\n",
      "__label__hu 99\n",
      "__label__sv 84\n",
      "__label__fi 23\n",
      "__label__nds 5\n",
      "__label__pa 9\n",
      "__label__ja 89\n",
      "__label__ru 107\n",
      "__label__pl 55\n",
      "__label__war 5\n",
      "__label__tl 11\n",
      "__label__he 23\n",
      "__label__fa 26\n",
      "__label__kn 11\n",
      "__label__sh 18\n",
      "__label__tr 56\n",
      "__label__no 44\n",
      "__label__sl 5\n",
      "__label__tg 1\n",
      "__label__ne 4\n",
      "__label__et 9\n",
      "__label__la 17\n",
      "__label__uz 5\n",
      "__label__oc 3\n",
      "__label__gl 5\n",
      "__label__mr 4\n",
      "__label__sw 2\n",
      "__label__ms 17\n",
      "__label__sr 17\n",
      "__label__fy 3\n",
      "__label__te 6\n",
      "__label__uk 33\n",
      "__label__hr 8\n",
      "__label__th 11\n",
      "__label__new 1\n",
      "__label__hi 11\n",
      "__label__eo 15\n",
      "__label__my 1\n",
      "__label__sk 4\n",
      "__label__ga 2\n",
      "__label__tt 5\n",
      "__label__gu 1\n",
      "__label__nn 1\n",
      "__label__de 5\n",
      "__label__lmo 3\n",
      "__label__ta 4\n",
      "__label__is 1\n",
      "__label__hy 1\n",
      "__label__ml 1\n",
      "__label__eu 6\n",
      "__label__lt 2\n",
      "__label__jbo 1\n",
      "__label__io 2\n",
      "__label__wuu 1\n",
      "__label__mk 2\n",
      "__label__ia 2\n",
      "__label__ilo 1\n",
      "__label__lv 1\n",
      "__label__rue 1\n",
      "__label__mn 1\n",
      "__label__ie 4\n",
      "__label__sq 1\n",
      "__label__gn 1\n",
      "__label__pms 1\n",
      "__label__nap 1\n",
      "__label__so 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "011adf0b-7b93-4582-a358-d291872074c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': '당신은 EMDR이라는 정신과에서 PTSD환자들을 위해서 고안한 치료법을 앱으로 구현하려고 기획중입니다. 눈을 좌우로 움직이면서 지워지지않는 생각을 상쇄시키는 효과를 볼 수 있습니다. 이러한 눈움직이기 활동을 모바일 화면을 보면서 수행할 수 있도록 활동을 기획할때 좌우로 움직이는 것뿐 아니라, 좌우, 위아래, 왼쪽 회전 오른쪽 회전 등의 활동을 30초씩 진행하고자할때 어떤 식으로 기획서를 작성할 수 있을지 아이디어를 제시하십시오'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '기획서 작성에 필요한 주요 요소들을 몇 가지 제시해 드리겠습니다. 이러한 요소들을 기반으로 기획서를 작성하시면, EMDR 앱을 효과적으로 구현할 수 있을 것입니다.\\n\\n1. 프로젝트 개요: EMDR 앱을 이용하여 PTSD 환자들에게 치료 효과를 제공할 수 있는 모바일 환경의 EMDR 치료법을 구현하고자 함을 명시합니다.\\n2. 목표 및 목적: 앱의 주요 목표는 PTSD 환자들이 스스로 치료를 진행할 수 있는 모바일 치료 도구를 제공하는 것입니다. 또한, 환자들이 눈 움직임을 좌우, 상하, 회전 등 다양한 방식으로 진행할 수 있도록 기능을 구현합니다.\\n3. 기능 및 요구 사항:\\n* 눈 움직임 유도 화면: 화면 상에 눈을 따라 움직이는 물체를 표시하여 사용자가 눈 움직임을 쉽게 따라할 수 있도록 합니다. 좌우, 상하, 왼쪽 회전, 오른쪽 회전 등의 다양한 움직임을 구현해야 합니다.\\n* 타이머 기능: 각 움직임에 대해 30초씩 진행할 수 있는 타이머를 제공합니다.\\n* 사용자 설정: 사용자가 원하는 눈 움직임 조합과 순서, 시간을 설정할 수 있는 기능을 구현합니다.\\n* 진행 기록 및 피드백: 사용자가 진행한 치료 세션의 기록을 저장하고, 그에 따른 피드백을 제공합니다.\\n1. 기술 및 도구: 앱 개발에 필요한 기술 스택과 도구를 명시합니다. 예를 들어, Android와 iOS 플랫폼을 지원하기 위한 언어와 프레임워크를 선택할 수 있습니다.\\n2. 개발 일정: 프로젝트 시작부터 완료까지의 일정을 계획하고, 각 단계별로 소요되는 시간과 마감일을 명시합니다.\\n3. 예산 및 자원: 프로젝트를 완료하는 데 필요한 예산과 인력, 기타 자원을 계획합니다.\\n4. 프로젝트 관리 및 팀 구성: 프로젝트 관리 방법론과 팀 구성을 명시하며, 팀원 간의 업무 분배와'}]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dict['__label__ko'][201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9befa-8fb0-4734-8f01-909c829ee4eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d9c794bd-99e7-4bda-b098-b78f685ba6a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3572"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d804e109-2702-4990-9868-e6cc1c14453a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for key, item in lang_dict.items():\n",
    "    if key != '__label__en':\n",
    "        new_dataset += item\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ca8e-0b78-4585-8b54-dbebbb00df99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def send_translate_request(new_dataset):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = lang_dict['__label__en'][pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        conv = data['conversations']\n",
    "        new_conv = []\n",
    "        for _data in conv:\n",
    "            value = _data['value']\n",
    "            # response\n",
    "            results = \"\"\n",
    "            text_blocks = []\n",
    "            code_blocks = []\n",
    "            for bidx, block in enumerate(value.split(\"```\")):\n",
    "                if bidx % 2 == 0:\n",
    "                    text_blocks.append(block)\n",
    "                else:\n",
    "                    code_blocks.append(block)\n",
    "\n",
    "            for tidx, text_block in enumerate(text_blocks):\n",
    "                prompt = f\"### 영어:\\n{text_block}\\n### 한국어:\\n\"\n",
    "                input_json = {\n",
    "                    \"model_name\": \"Gugugo-koen-7B-V1.1\",\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.8,\n",
    "                    \"max_new_tokens\": 4096,\n",
    "                    \"stop\": [\"</끝>\", \"###\"],\n",
    "                }\n",
    "\n",
    "                ret = requests.post(\n",
    "                    api_server_url + \"/worker_generate_stream\",\n",
    "                    json=input_json,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                for chunk in ret.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n",
    "                    if chunk:\n",
    "                        result_data = json.loads(chunk.decode())\n",
    "\n",
    "                result = result_data['text'][len(prompt):].rstrip('\\n')\n",
    "                results += result\n",
    "                if len(code_blocks) > tidx:\n",
    "                    results += \"```\" + code_blocks[tidx] + \"```\"\n",
    "            \n",
    "            new_conv.append({\n",
    "                'from': _data['from'],\n",
    "                'value': results,\n",
    "            })\n",
    "        new_dataset.append({\n",
    "            'conversations': new_conv,\n",
    "            'id': pidx,\n",
    "        })\n",
    "\n",
    "\n",
    "# code_prefixes = [\"python\", \"c++\", \"minikube\", \"docker\", \"json\", \"java\", \n",
    "#                  \"php\", \"bash\", \"c#\", \"cpp\", \"css\", \"perl\", \"html\", \"xml\", \n",
    "#                  \"ruby\", \"sql\", \"ini\", \"apt\", \"socat\", \"tcp\", \"localhost\",\n",
    "#                  \"git\"]\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(lang_dict['__label__en'])\n",
    "n_thread = 64 * 7\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_translate_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da213b4-42f4-4915-8021-08562d93e3b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34327ab4-f434-4758-817d-2d8d7eda084a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/translated_sharegpt_V3_format.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca48e51-3e07-4e5e-9eb0-8a2f1c8e7c02",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nidx=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2d9cd-7ec1-4db4-a349-3baf9fa018fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_dict['__label__en'][40] #코드처리 ```, \"\", (\"\"), {}, 반복처리, 빈 데이터(길이 이슈) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f37a95-c762-4286-bffc-58ad7b558f10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = \"\"\n",
    "text_blocks = []\n",
    "code_blocks = []\n",
    "for bidx, block in enumerate(value.split(\"```\")):\n",
    "    if bidx % 2 == 0:\n",
    "        text_blocks.append(block)\n",
    "    else:\n",
    "        code_blocks.append(block)\n",
    "\n",
    "for tidx, text_block in enumerate(text_blocks):\n",
    "    prompt = f\"### 영어:\\n{text_block}\\n### 한국어:\\n\"\n",
    "    input_json = {\n",
    "        \"model_name\": \"Gugugo-koen-7B-V1.1\",\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.8,\n",
    "        \"max_new_tokens\": 4096,\n",
    "        \"stop\": [\"</끝>\", \"###\"],\n",
    "    }\n",
    "\n",
    "    ret = requests.post(\n",
    "        api_server_url + \"/worker_generate_stream\",\n",
    "        json=input_json,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for chunk in ret.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n",
    "        if chunk:\n",
    "            result_data = json.loads(chunk.decode())\n",
    "    result = result_data['text'][len(prompt):].rstrip('\\n')\n",
    "    results += result\n",
    "\n",
    "    if len(code_blocks) > tidx:\n",
    "        results += \"```\" + code_blocks[tidx] + \"```\"\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530ffa5-b017-4e3d-89b8-44ac0ffb31e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8caf030-2dcc-478e-a76c-07a9d193ae35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset2 = load_dataset(\"json\", data_files=\"/data/llm_datasets/sharegpt_gpt4/sharegpt_V3_format.jsonl\")\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eed52f-31c1-480f-a6ca-44d65a9f1245",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset2['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a9bdf-91e0-4c9e-8bc7-c544ff694d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ae1f3-afe4-4133-879f-80767aa7e8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbb340-c1b3-428d-8775-98f854b89ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8fe4c-4f7f-4219-84a4-c1917175e523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
