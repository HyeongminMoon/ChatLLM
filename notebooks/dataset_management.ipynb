{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dpo_dataset import load_dpo_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6737de8-e4c0-4865-b86e-3e9a90417890",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "cache_dir = None\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6573a97-32e2-4466-bc1f-9412f4ef59e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "\n",
    "def extract_anthropic_prompt(prompt_and_response, search_term=\"\\n\\nAssistant:\"):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "class hankang_DPODataset:\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_path=\"/data/llm_datasets/Ultrafeedback_binarized.ko.hankang/\",\n",
    "        data_format='chat-orca',\n",
    "        search_term='\\n\\n### Assistant:',\n",
    "        num_train=None,\n",
    "        num_eval=None,\n",
    "    ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.data_format = data_format\n",
    "        self.search_term = search_term\n",
    "        self.num_train = num_train\n",
    "        self.num_eval = num_eval\n",
    "    \n",
    "    def get_prompt_and_response(self, data):\n",
    "        conv = get_conversation_template(self.data_format)\n",
    "\n",
    "        for idx, _conv in enumerate(data):\n",
    "            role = _conv['role']\n",
    "            content = _conv['content_kr']\n",
    "            if idx % 2 == 0 and role == 'user':\n",
    "                conv.append_message(conv.roles[0], content)\n",
    "            elif idx % 2 == 1 and role == 'assistant':\n",
    "                conv.append_message(conv.roles[1], content)\n",
    "            else:\n",
    "                print(\"Warning: data type invaild\")\n",
    "\n",
    "        if len(conv.messages) == 0:\n",
    "            print(\"Warning: data is empty\")\n",
    "        if len(conv.messages) % 2 != 0:\n",
    "            print(\"Warning: data has weird pair\")\n",
    "\n",
    "        return conv.get_prompt()\n",
    "    \n",
    "    def make_dpo_data_module(self):\n",
    "        def validate_prompt_and_responses(data) -> bool:\n",
    "            try:\n",
    "                prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "                prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "                prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "                promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            except AssertionError:\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        def split_prompt_and_responses(data) -> Dict[str, str]:\n",
    "            prompt_and_response = self.get_prompt_and_response(data['chosen'])\n",
    "            prompt_and_response_rejected = self.get_prompt_and_response(data['rejected'])\n",
    "            prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "            promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": prompt_and_response[len(prompt) :],\n",
    "                \"rejected\": prompt_and_response_rejected[len(promopt_rejected) :],\n",
    "            }\n",
    "                             \n",
    "                             \n",
    "        dataset = load_dataset(self.dataset_path)\n",
    "\n",
    "        train_dataset = dataset['train']\n",
    "        eval_dataset = dataset['test']\n",
    "\n",
    "        original_columns = list(train_dataset.features.keys())\n",
    "\n",
    "        if self.num_train is not None:\n",
    "            train_dataset = train_dataset.select(range(min(len(train_dataset), self.num_train)))\n",
    "        if self.num_eval is not None:\n",
    "            eval_dataset = eval_dataset.select(range(min(len(train_dataset), self.num_eval)))\n",
    "\n",
    "        train_dataset = train_dataset.filter(validate_prompt_and_responses)\n",
    "        train_dataset = train_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        eval_dataset = eval_dataset.filter(validate_prompt_and_responses)\n",
    "        eval_dataset = eval_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "\n",
    "        return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0b5bb80a-5915-4592-8989-244e11c9e4a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import hankang_DPODataset\n",
    "\n",
    "dpo_dataset = hankang_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2c224b24-013e-46ef-b8f2-6ec3a718c3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in dpo_datamodule['train_dataset']:\n",
    "    if len(data['prompt'].split(\"### User:\")) != 2:\n",
    "        print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df680a11-11cc-42bd-b5ff-3f7b16dfa17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27144451-5587-4598-a4c7-fa25a6be8b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/alpaca-gpt4-korean_dedup/\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\",\n",
    "    \"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",\n",
    "    \"/data/llm_datasets/custom/deduped/sharegpt_V3_format_ko_selected.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "correction_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/\"\n",
    "]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped/aihub_summary_data_tech_dedup/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/도서/\",\n",
    "    \"/data/llm_datasets/aihub_summary_data/법률/\",\n",
    "    \"/data/llm_datasets/custom/deduped/naver-news-summarization-ko-vicuna_dedup/\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(enko).json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_translation(koen).json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + correction_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f103ce-1d80-48ed-90c7-9c57ab4e2af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dedup2\n",
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/sharegpt_gpt4/sharegpt_gpt4.jsonl\",#\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",#\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_ko_selected_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "# correction_list = [\n",
    "#     \"/data/llm_datasets/custom/deduped2/KoreaSpellingCorrection-10000.json\",\n",
    "# ]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_tech_dedup-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_book-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_law-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/naver-news-summarization-ko-vicuna_dedup-5000.json\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3046237a-9d36-4f86-83dc-39e83d5a212a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refine\n",
    "qna_list = [\n",
    "    \"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/koalpaca_v1.1-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/alpaca-gpt4-korean_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/korquad-chat-vicuna.json\",\n",
    "    \"/data/llm_datasets/custom/refined/wizardlm_orca_vicuna_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_gpt4.json\",#\n",
    "    \"/data/llm_datasets/custom/vicuna_format/sharegpt_V3_format_others.json\",#\n",
    "    \"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\",\n",
    "    \"/data/llm_datasets/custom/refined/lima_vicuna_format_ko.json\",\n",
    "]\n",
    "\n",
    "# correction_list = [\n",
    "#     \"/data/llm_datasets/custom/deduped2/KoreaSpellingCorrection-10000.json\",\n",
    "# ]\n",
    "\n",
    "summary_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_tech_dedup-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_book-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/aihub_summary_data_law-5000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/naver-news-summarization-ko-vicuna_dedup-5000.json\",\n",
    "    \n",
    "]\n",
    "\n",
    "translation_list = [\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(enko)-10000.json\",\n",
    "    \"/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_list = qna_list + summary_list + translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "364662d1-9668-44f2-a26e-fabaf68600ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dpo v2\n",
    "dpo_list = [\n",
    "    \"/data/llm_datasets/ultrafeedback_binarized/data/train_prefs-00000-of-00001-17309c769bfe5733.parquet\",\n",
    "    \"/data/llm_datasets/orca_dpo_pairs/\",\n",
    "    \"/data/llm_datasets/distilabel-math-preference-dpo/data/\",\n",
    "]\n",
    "\n",
    "dpo_list2 = [\n",
    "    \"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/untranslated/distilabel-math-preference-dpo.json\",\n",
    "]\n",
    "\n",
    "dpo_list3 = [\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_ultrafeedback_binarized.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_orca_dpo_pairs.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_distilabel-math-preference-dpo.json\",\n",
    "]\n",
    "\n",
    "# dedup\n",
    "dpo_list4 = [\n",
    "    '/data/llm_datasets/custom/kodpo/deduped/ko_ultrafeedback_binarized.json',\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_orca_dpo_pairs.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_distilabel-math-preference-dpo.json\",\n",
    "]\n",
    "\n",
    "# refine codeblock\n",
    "dpo_list5 = [\n",
    "    '/data/llm_datasets/custom/kodpo/refined/ko_ultrafeedback_binarized.json',\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_orca_dpo_pairs.json\",\n",
    "    \"/data/llm_datasets/custom/kodpo/translated/ko_distilabel-math-preference-dpo.json\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "946c5dcd-6009-44bf-aef2-dd75d1269bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dpo_dataset(dataset_path, split='train'):\n",
    "    if dataset_path.endswith(\"json\"):\n",
    "        dataset = load_dataset(\"json\", data_files=dataset_path, split=split)\n",
    "    elif dataset_path.endswith(\"parquet\"):\n",
    "        dataset = load_dataset(\"parquet\", data_files=dataset_path, split=split)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_path, split=split)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# dataset = load_dpo_dataset(dpo_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ddd2e-0208-461e-9bb8-e34da04d1e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset_list = []\n",
    "for d in dataset_list:\n",
    "    new_dataset_list.append(\"\\\"\" + d + \"\\\"\")\n",
    "print(' '.join(new_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839cdec-b301-40ed-89eb-a0a6512e631e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = dataset_list[7]\n",
    "print(dataset_path)\n",
    "dataset_train = load_sft_dataset(dataset_path)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9c7ebad0-a770-4519-942d-0f5ddc564efa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'conversations': [{'from': Value(dtype='string', id=None),\n",
       "   'value': Value(dtype='string', id=None)}],\n",
       " 'instruction': Value(dtype='string', id=None),\n",
       " 'task_name': Value(dtype='string', id=None),\n",
       " 'system': Value(dtype='string', id=None),\n",
       " 'task': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_sft_dataset(dataset_path) for dataset_path in dataset_list])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fb94dc55-df6b-4c12-a23d-5e2afc3b199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in train_dataset:\n",
    "    new_dataset.append(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7827856c-b0da-4fe8-a0fb-a07b5dbd2a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_dataset.to_json(\"/data/llm_datasets/custom/ados_sft_v4.json\")\n",
    "with open(\"/data/llm_datasets/custom/ados_sft_v4.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fcd5c578-57f8-41d5-8979-7424a90fe754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165129 19379 27 21 48\n"
     ]
    }
   ],
   "source": [
    "\"\"\" find odd code blocks\"\"\"\n",
    "dataset_path = \"/data/llm_datasets/custom/ados_sft_v4.json\"\n",
    "# dataset = load_dataset(\"json\", dataset_path)\n",
    "dataset = load_sft_dataset(dataset_path, split=None)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# new_dataset = []\n",
    "code_prefixes = []\n",
    "\n",
    "odd_dataset = []\n",
    "oddd_dataset = []\n",
    "odd_idxs = set()\n",
    "normal_dataset = []\n",
    "flag_normal = True\n",
    "flag_code = False\n",
    "for idx, data in enumerate(train_dataset):\n",
    "    conversations = data['conversations']\n",
    "    flag_normal = True\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        if _from == 'human': continue\n",
    "        _value = conv['value']\n",
    "        flag_code = False\n",
    "        find_iter = re.finditer('```', _value)\n",
    "        temp_num = 0\n",
    "        for fidx, ftext in enumerate(find_iter):\n",
    "            flag_code = True\n",
    "            start_index = ftext.start() + 3\n",
    "            # new_dataset.append(data)\n",
    "            #TODO: 스페이스바가 바로 오는 경우..\n",
    "            candidate = re.split(r'[\\n]', _value[start_index:])[0]\n",
    "            if fidx % 2 == 0 and '```' not in candidate and candidate not in available_code_prefixes:\n",
    "                odd_dataset.append((candidate, data))\n",
    "                code_prefixes.append(candidate)\n",
    "                odd_idxs.add(idx)\n",
    "                flag_normal = False\n",
    "                break\n",
    "            temp_num += 1\n",
    "        if temp_num % 2 != 0:\n",
    "            oddd_dataset.append(('odd', data))\n",
    "            odd_idxs.add(idx)\n",
    "            flag_normal = False\n",
    "        \n",
    "        if not flag_normal:\n",
    "            break\n",
    "    \n",
    "    if flag_code and flag_normal:\n",
    "        normal_dataset.append(data)\n",
    "        num_normal += 1\n",
    "\n",
    "print(len(train_dataset), len(normal_dataset), len(odd_dataset), len(oddd_dataset), len(odd_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4ea93ac0-9764-4bd1-9235-4ffa35dbd259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('python ',\n",
       " {'system': None,\n",
       "  'task_name': None,\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '주어진 배열을 역순으로 정렬하는 Python 스크립트를 만드세요.\\n'},\n",
       "   {'from': 'gpt',\n",
       "    'value': '주어진 배열을 역순으로 정렬하는 파이썬 스크립트는 다음과 같습니다:\\n\\n```python \\narray = [10, 2, 5, -4, 92, 101]\\narray.sort(reverse=True)\\nprint(array)\\n```\\n\\n결과는 다음과 같습니다:\\n```python\\n[101, 92, 10, 5, 2, -4]\\n```\\n\\n이 스크립트는 리스트의 `sort()` 메소드를 사용하여 배열을 오름차순으로 정렬하며, `reverse` 매개 변수를 `True`로 설정하여 정렬된 리스트의 순서를 뒤집습니다.'}],\n",
       "  'task': None,\n",
       "  'instruction': None,\n",
       "  'id': '35752'})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a7141cb3-f61e-459f-8c60-343df2804357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_idxs = list(range(len(train_dataset)))\n",
    "for od in odd_idxs:\n",
    "    selected_idxs.remove(od)\n",
    "train_dataset = train_dataset.select(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5f93cba5-7840-4b15-8a30-fa289e55f9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4848.91it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1036.65it/s]\n",
      "Generating train split: 165081 examples [00:20, 8243.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'task_name', 'conversations', 'task', 'instruction', 'id'],\n",
       "        num_rows: 165081\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_sft_dataset(\"/data/llm_datasets/custom/ados_sft_v4.1.json\", split=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4ce47-64c5-425b-a651-24ab3bd7abba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_prefixes = set(code_prefixes)\n",
    "code_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cc164f84-211c-4585-acb1-a810ded34874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_code_prefixes = set([\n",
    "    '',\n",
    "    'CSS',\n",
    "    'HTML',\n",
    "    'JavaScript',\n",
    "    'Python',\n",
    "    'SQL',\n",
    "    'bash',\n",
    "    'c',\n",
    "    'c++',\n",
    "    'cpp',\n",
    "    'csharp',\n",
    "    'css',\n",
    "    'for',\n",
    "    'html',\n",
    "    'java',\n",
    "    'javascript',\n",
    "    'js',\n",
    "    'json',\n",
    "    'php',\n",
    "    'python',\n",
    "    'ruby',\n",
    "    'sass',\n",
    "    'scss',\n",
    "    'sql',\n",
    "    'sum',\n",
    "    'svg',\n",
    "    'swift',\n",
    "    'xml',\n",
    "    'yaml',\n",
    "    'C#',\n",
    "    'C++',\n",
    "    'CSS',\n",
    "    'Go',\n",
    "    'HTML',\n",
    "    'Java',\n",
    "    'LaTeX',\n",
    "    'MATLAB',\n",
    "    'Markdown',\n",
    "    'Proposals',\n",
    "    'Python',\n",
    "    'R',\n",
    "    'SELECT',\n",
    "    'SQL',\n",
    "    'Swift',\n",
    "    'VBA',\n",
    "    'echo',\n",
    "    'excel-vba',\n",
    "    'find',\n",
    "    'go',\n",
    "    'gpg',\n",
    "    'jsx',\n",
    "    'kotlin',\n",
    "    'latex',\n",
    "    'markdown',\n",
    "    'math',\n",
    "    'matlab',\n",
    "    'meditation',\n",
    "    'mermaid',\n",
    "    'mutt',\n",
    "    'nano',\n",
    "    'r',\n",
    "    'rust',\n",
    "    'scala',\n",
    "    'sh',\n",
    "    'shell',\n",
    "    'sudo',\n",
    "    'xpath',\n",
    "    '{r}',\n",
    "    '.',\n",
    "    '$',\n",
    "    'curl',\n",
    "    'xslt',\n",
    "    'Apex',\n",
    "    'DAX',\n",
    "    'Dockerfile',\n",
    "    'apex',\n",
    "    'applescript',\n",
    "    'arduino',\n",
    "    'asm',\n",
    "    'assembly',\n",
    "    'astro',\n",
    "    'autoit',\n",
    "    'batch',\n",
    "    'bicep',\n",
    "    'blade',\n",
    "    'cmake',\n",
    "    'cmd',\n",
    "    'coffee',\n",
    "    'coffeescript',\n",
    "    'cql',\n",
    "    'csv',\n",
    "    'cypher',\n",
    "    'dart',\n",
    "    'delphi',\n",
    "    'diff',\n",
    "    'dockerfile',\n",
    "    'dot',\n",
    "    'emacs',\n",
    "    'erb',\n",
    "    'fsharp',\n",
    "    'glsl',\n",
    "    'gradle',\n",
    "    'graphql',\n",
    "    'graphviz',\n",
    "    'groovy',\n",
    "    'haskell',\n",
    "    'hcl',\n",
    "    'hlsl',\n",
    "    'html+erb',\n",
    "    'ini',\n",
    "    'jinja',\n",
    "    'ladder',\n",
    "    'lasso',\n",
    "    'less',\n",
    "    'lisp',\n",
    "    'lldb',\n",
    "    'llvm',\n",
    "    'logo',\n",
    "    'lua',\n",
    "    'makefile',\n",
    "    'mathematica',\n",
    "    'metal',\n",
    "    'nginx',\n",
    "    'nix',\n",
    "    'objc',\n",
    "    'objective',\n",
    "    'objectivec',\n",
    "    'pascal',\n",
    "    'perl',\n",
    "    'plaintext',\n",
    "    'plantuml',\n",
    "    'powershell',\n",
    "    'prisma',\n",
    "    'properties',\n",
    "    'proto',\n",
    "    'protobuf',\n",
    "    'py',\n",
    "    'reg',\n",
    "    'rego',\n",
    "    'scheme',\n",
    "    'scratch',\n",
    "    'solidity',\n",
    "    'spss',\n",
    "    'stata',\n",
    "    'stencil',\n",
    "    'terraform',\n",
    "    'toml',\n",
    "    'ts',\n",
    "    'tsx',\n",
    "    'txt',\n",
    "    'typescript',\n",
    "    'vb',\n",
    "    'vba',\n",
    "    'vbnet',\n",
    "    'verilog',\n",
    "    'yml',\n",
    "    'jsp',\n",
    "    'prolog',\n",
    "    'razor',\n",
    "    'CMD',\n",
    "    'G',\n",
    "    'GraphQL',\n",
    "    'Makefile',\n",
    "    'apache',\n",
    "    'c#',\n",
    "    'cython',\n",
    "    'elixir',\n",
    "    'jinja2',\n",
    "    'julia',\n",
    "    'ocaml',\n",
    "    'systemverilog',\n",
    "    'vbscript',\n",
    "    'vhdl',\n",
    "    'vue',\n",
    "    'wasm',\n",
    "    'wolfram',\n",
    "    'zsh',\n",
    "    'regex',\n",
    "    ' Java',\n",
    "    ' Python',\n",
    "    ' c++',\n",
    "    ' python',\n",
    "    ' java',\n",
    "    ' css',\n",
    "    ' html',\n",
    "    ' js',\n",
    "    ' python',\n",
    "    ' scala',\n",
    "    'md',\n",
    "    ' Bash',\n",
    "    ' MATLAB',\n",
    "    ' Matlab',\n",
    "    ' PHP',\n",
    "    ' R',\n",
    "    ' Swift',\n",
    "    ' bash',\n",
    "    ' batteryruntime',\n",
    "    ' cpp',\n",
    "    ' csharp',\n",
    "    ' makefile',\n",
    "    ' r',\n",
    "    ' ruby',\n",
    "    ' sql',\n",
    "    ' swift',\n",
    "    ' ts',\n",
    "    'ABAP',\n",
    "    'AutoIt',\n",
    "    'Bash',\n",
    "    'C',\n",
    "    'CSharp',\n",
    "    'EL',\n",
    "    'GDScript',\n",
    "    'Kotlin',\n",
    "    'M',\n",
    "    'Maple',\n",
    "    'MatLab',\n",
    "    'Matlab',\n",
    "    'PowerShell',\n",
    "    'PyTorch',\n",
    "    'Shell',\n",
    "    'VHDL',\n",
    "    'Vagrantfile',\n",
    "    'XML',\n",
    "    'ada',\n",
    "    'afrikaans',\n",
    "    'amsmath',\n",
    "    'arc',\n",
    "    'aspx',\n",
    "    'autohotkey',\n",
    "    'azurepowershell',\n",
    "    'bash ',\n",
    "    'bat',\n",
    "    'c ',\n",
    "    'cfscript',\n",
    "    'cl',\n",
    "    'clojure',\n",
    "    'cobol',\n",
    "    'command',\n",
    "    'cplusplus',\n",
    "    'cron',\n",
    "    'cs',\n",
    "    'csh',\n",
    "    'dax',\n",
    "    'debug',\n",
    "    'deluge',\n",
    "    'dos',\n",
    "    'ejs',\n",
    "    'excel',\n",
    "    'fortran',\n",
    "    'gdscript',\n",
    "    'glslx',\n",
    "    'golang',\n",
    "    'guj',\n",
    "    'handlebars',\n",
    "    'js ',\n",
    "    'language',\n",
    "    'libraries',\n",
    "    'lustrum',\n",
    "    'max',\n",
    "    'mikrotik',\n",
    "    'npm',\n",
    "    'objective-c',\n",
    "    'octave',\n",
    "    'ogdl',\n",
    "    'pddl',\n",
    "    'pine',\n",
    "    'pseudo',\n",
    "    'psuedocode',\n",
    "    'py ',\n",
    "    'qlik',\n",
    "    'rb',\n",
    "    'registry',\n",
    "    'rpgle',\n",
    "    'sas',\n",
    "    'scala ',\n",
    "    'scribble',\n",
    "    'simple_java',\n",
    "    'smalltalk',\n",
    "    'soap',\n",
    "    'spreadsheet',\n",
    "    'tcl',\n",
    "    'tex',\n",
    "    'text',\n",
    "    'typescript ',\n",
    "    'views',\n",
    "    'webidl',\n",
    "    'xaml',\n",
    "    'xquery',\n",
    "])\n",
    "\n",
    "# for pre in available_code_prefixes:\n",
    "with open(\"available_code_prefixes.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(available_code_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387a1b4-47f2-4688-b06b-ef9c8b23a113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_code_prefixes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809e09-c947-4724-80ab-965e2f66816f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            summary = context_data['summary']\n",
    "\n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "    print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31df058-d4f2-4df8-8d24-57a3330db159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['TL_EE_train'] + dataset_dict['TL_LA_train'] + dataset_dict['TL_ED_train'] + dataset_dict['TL_NA_train']\n",
    "eval_dataset_list = dataset_dict['TL_EE_val'] + dataset_dict['TL_LA_val'] + dataset_dict['TL_ED_val'] + dataset_dict['TL_NA_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf58516-415a-4d89-813e-0712e1486acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b508d1c-fc90-417a-9681-27cdcec772e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef997b-df6c-442e-ad81-64eb7a74f844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        documents = json_data['documents']\n",
    "        for document in documents:\n",
    "            text = document['text']\n",
    "            abstractive = document['abstractive']\n",
    "\n",
    "            summary = abstractive[0]\n",
    "            context = []\n",
    "            for _text in text:\n",
    "                _context = ' '.join([_index_text['sentence'] for _index_text in _text])\n",
    "                context.append(_context)\n",
    "            context = '\\n'.join(context)\n",
    "            \n",
    "            data_row = {\n",
    "                'id': f\"{file_name}_{idx}\",\n",
    "                'task': 'summarization',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': context},\n",
    "                                    {'from': 'gpt', 'value': summary},\n",
    "                                 ],\n",
    "            }\n",
    "            new_dataset.append(data_row)\n",
    "            idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400f4d6-274c-4591-9315-1cd9702dc78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c989c-7f09-4326-b3a8-8d606b376917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_law_train']\n",
    "eval_dataset_list = dataset_dict['summary_law_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b13db5-1c0f-4991-bd1e-446127d4dac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/법률/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310ab0d-cb0b-4a4e-97fb-4606c736d216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*summary_book*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '**/*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context = json_data['passage']\n",
    "        summary = json_data['summary']\n",
    "        \n",
    "        data_row = {\n",
    "            'id': f\"{file_name}_{idx}\",\n",
    "            'task': 'summarization',\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': context},\n",
    "                                {'from': 'gpt', 'value': summary},\n",
    "                             ],\n",
    "        }\n",
    "        new_dataset.append(data_row)\n",
    "        idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3337-aa61-49fe-8cb2-eea7a14fcb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610abe01-befd-4bc1-b67a-ddf26127d1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['summary_book_train']\n",
    "eval_dataset_list = dataset_dict['summary_book_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1eb8e0-7c27-4d26-916d-77ab6cc5925a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_summary_data/도서/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ff1b9-148b-40a4-93c7-e918b5f5d1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_summary_data/도서\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36141362-162e-4cea-a636-b6f876481896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "file_paths = glob.glob(\"/workspaces/data/llm_datasets/aihub/*VL*[!tar|!sh]\")\n",
    "\n",
    "dataset_dict = {}\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    new_dataset = []\n",
    "    idx = 0\n",
    "\n",
    "    paths = glob.glob(os.path.join(file_path, '*.json'))\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        context_info = json_data['dataset']['context_info']\n",
    "        for context_data in context_info:\n",
    "            context = context_data['context']\n",
    "            qas = context_data['qas']\n",
    "\n",
    "            for _qas in qas:\n",
    "                question = _qas['question-1']\n",
    "                answer = _qas['answer']\n",
    "                question_level = _qas['question_level']\n",
    "                if question_level != '상': continue\n",
    "                data_row = {\n",
    "                    'id': f\"{file_name}_{idx}\",\n",
    "                    'task': 'contextqa',\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                }\n",
    "                new_dataset.append(data_row)\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"file_name:{file_name} idx:{idx}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    dataset_dict[file_name] = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83775c61-d0a7-4483-b833-7775c9dd9449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in dataset_dict.keys():\n",
    "    print(key, len(dataset_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6a348-e8fc-4b42-ac62-e6f9adb46743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list = dataset_dict['VL_EE_train'] + dataset_dict['VL_NA_train'] + dataset_dict['VL_LA_train']+ dataset_dict['VL_ED_train']\n",
    "eval_dataset_list = dataset_dict['VL_EE_val'] + dataset_dict['VL_NA_val'] + dataset_dict['VL_LA_val']+ dataset_dict['VL_ED_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db99aa-0bbc-49d9-8df4-87dcc4daee19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/train.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list, json_file)\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학/test.json\", \"w\") as json_file:\n",
    "    json.dump(eval_dataset_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7d65-84d7-41f3-b10c-3aef5ffcfdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_list_0 = train_dataset_list[:120000]\n",
    "train_dataset_list_1 = train_dataset_list[120000:240000]\n",
    "train_dataset_list_2 = train_dataset_list[240000:]\n",
    "\n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split0.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_0, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split1.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_1, json_file)\n",
    "    \n",
    "with open(\"/workspaces/data/llm_datasets/aihub_contextqa_data/기술과학/train_split2.json\", \"w\") as json_file:\n",
    "    json.dump(train_dataset_list_2, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503836eb-d006-410c-9986-65d737526d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/aihub_contextqa_data_hard/기술과학\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bfe09-7e0a-4c23-b9f5-fff6478c1d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d8cbd-68a3-4480-ac47-6a9f0fabb09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/workspaces/data/llm_datasets/gpt4_evol_1.3k/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687e5db-9ef7-4218-91ad-b65279e1bd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dataset['train'][0]\n",
    "# answer = data['answer']\n",
    "# question = data['question']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afea3a-efc8-4ddb-92c7-c06297e69260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    answer = data['answer']\n",
    "    question = data['question']\n",
    "\n",
    "    data_row = {\n",
    "        'id': f\"gpt_evol_1.3k_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': question},\n",
    "                            {'from': 'gpt', 'value': answer},\n",
    "                         ],\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194f908-d12d-4045-8d78-d1da6ab8375c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716623c-7fb9-482f-83e3-8ebce535c41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115478e-7ce6-4b57-830c-5c6c794e1334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/WizardLM_Orca/wizardlm_orca.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b5486-8910-4cb8-9895-055e64ef5e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['train']:\n",
    "    output = data['output']\n",
    "    system = data['system']\n",
    "    instruction = data['instruction']\n",
    "    data_row = {\n",
    "        'id': f\"WizardLM_Orca_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': instruction},\n",
    "                            {'from': 'gpt', 'value': output},\n",
    "                         ],\n",
    "        'task': 'system_instruct',\n",
    "        'system': system,\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a898f0b-379f-42e4-a762-cf30ee4123e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/wizardlm_orca_vicuna.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391b407-27d5-487b-8b69-b5d5d7c10983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/data/llm_datasets/KoreaSpellingCorrection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af63b70-f50d-4b53-8d52-4da167c7889d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "idx = 0\n",
    "for data in dataset['test']:\n",
    "    wrong = data['wrong']\n",
    "    correct = data['correct']\n",
    "    data_row = {\n",
    "        'id': f\"KoreaSpelling_Correction_{idx}\",\n",
    "        'conversations': [\n",
    "                            {'from': 'human', 'value': wrong},\n",
    "                            {'from': 'gpt', 'value': correct},\n",
    "                         ],\n",
    "        'task': 'correction',\n",
    "    }\n",
    "    new_dataset.append(data_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a29e00-b31e-46a3-818c-9fab9ca3aaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcf31a-81f3-4147-9cac-cd609b3a62b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/KoreaSpellingCorrection/test.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44bae8-d7cc-4aea-8251-0c0124bda876",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dpo_list[2])\n",
    "dataset = load_dpo_dataset(dpo_list[2])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f8f2cb5-d6be-45cd-8ffc-1c276ffe8e01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset:\n",
    "    new_dataset.append({\n",
    "        'id': f\"Ko_ultrafeedback_binarized_{data['prompt_id']}\",\n",
    "        'input': data['prompt'],\n",
    "        'chosen': data['chosen'][1]['content'],\n",
    "        'rejected': data['rejected'][1]['content'],\n",
    "        'task': \"dpo\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e57e955a-e66d-458a-89b0-d651368d3b01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "# idx = 0\n",
    "for idx, data in enumerate(dataset):\n",
    "    new_dataset.append({\n",
    "        'id': f\"Ko_orca_dpo_pairs_{idx}\",\n",
    "        'input': data['question'],\n",
    "        'chosen': data['chosen'],\n",
    "        'rejected': data['rejected'],\n",
    "        'task': \"dpo_system\",\n",
    "        'system': data['system'],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "770fe090-dba0-4741-8212-f96c7d25e97a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "# idx = 0\n",
    "for idx, data in enumerate(dataset):\n",
    "    new_dataset.append({\n",
    "        'id': f\"Ko_distilabel-math-preference-dpo_{idx}\",\n",
    "        'input': data['instruction'],\n",
    "        'chosen': data['chosen_response'],\n",
    "        'rejected': data['rejected_response'],\n",
    "        'task': \"dpo\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c06a934-0004-40c2-9b84-d8166b61209f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/kodpo/untranslated/distilabel-math-preference-dpo.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458618e3-7e40-4ed5-b5ef-f1e06ee1741b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def send_request(new_dataset):\n",
    "    global idx\n",
    "    for _ in range(2):\n",
    "        if idx > len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        data = dataset[subset][idx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "        \n",
    "        _id = data['id']\n",
    "        context = data['context']\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        if answer.lower() == 'yes':\n",
    "            answer = '네'\n",
    "        elif answer.lower() == 'no':\n",
    "            answer = '아니오'\n",
    "        \n",
    "        # response\n",
    "        result = generate_refiner(\n",
    "            model_name,\n",
    "            context,\n",
    "            question,\n",
    "            answer\n",
    "        )\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'id': _id,\n",
    "            'conversations': [\n",
    "                                {'from': 'human', 'value': question},\n",
    "                                {'from': 'gpt', 'value': result},\n",
    "                             ],\n",
    "            'task_name': \"instruct\",\n",
    "            'instruction': context,\n",
    "        })\n",
    "\n",
    "\n",
    "model_name = \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\"\n",
    "subset = 'train'\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(dataset[subset])\n",
    "n_thread = 64\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"make lang_dict(sft)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "661bfc45-9396-4f6b-a6ff-12acc03a0b2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 688.72it/s]\n",
      "Generating train split: 2418 examples [00:00, 14178.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make lang_dict(dpo)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/distilabel-math-preference-dpo.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    \n",
    "    langs = {}\n",
    "\n",
    "    lang_chosen = lang_detect.predict(chosen.replace('\\n', ' '))[0][0]\n",
    "    if lang_chosen not in langs:\n",
    "        langs[lang_chosen] = 1\n",
    "    else:\n",
    "        langs[lang_chosen] += 1\n",
    "        \n",
    "    lang_rejected = lang_detect.predict(rejected.replace('\\n', ' '))[0][0]\n",
    "    if lang_rejected not in langs:\n",
    "        langs[lang_rejected] = 1\n",
    "    else:\n",
    "        langs[lang_rejected] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6279a941-d152-4693-b5e6-44d63a86acbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 2418\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460862df-7bbf-4a40-982d-1f7782fd22ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ca8e-0b78-4585-8b54-dbebbb00df99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"sft translate\"\"\"\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def send_translate_request(new_dataset):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx >= len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = lang_dict['__label__en'][pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        conv = data['conversations']\n",
    "        new_conv = []\n",
    "        for _data in conv:\n",
    "            value = _data['value']\n",
    "            # response\n",
    "            results = \"\"\n",
    "            text_blocks = []\n",
    "            code_blocks = []\n",
    "            for bidx, block in enumerate(value.split(\"```\")):\n",
    "                if bidx % 2 == 0:\n",
    "                    text_blocks.append(block)\n",
    "                else:\n",
    "                    code_blocks.append(block)\n",
    "\n",
    "            for tidx, text_block in enumerate(text_blocks):\n",
    "                prompt = f\"### 영어:\\n{text_block}\\n### 한국어:\\n\"\n",
    "                input_json = {\n",
    "                    \"model_name\": \"Gugugo-koen-7B-V1.1\",\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.8,\n",
    "                    \"max_new_tokens\": 4096,\n",
    "                    \"stop\": [\"</끝>\", \"###\"],\n",
    "                }\n",
    "\n",
    "                ret = requests.post(\n",
    "                    api_server_url + \"/worker_generate_stream\",\n",
    "                    json=input_json,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                for chunk in ret.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n",
    "                    if chunk:\n",
    "                        result_data = json.loads(chunk.decode())\n",
    "\n",
    "                result = result_data['text'][len(prompt):].rstrip('\\n')\n",
    "                results += result\n",
    "                if len(code_blocks) > tidx:\n",
    "                    results += \"```\" + code_blocks[tidx] + \"```\"\n",
    "            \n",
    "            new_conv.append({\n",
    "                'from': _data['from'],\n",
    "                'value': results,\n",
    "            })\n",
    "        new_dataset.append({\n",
    "            'conversations': new_conv,\n",
    "            'id': data['id'],\n",
    "        })\n",
    "\n",
    "\n",
    "# code_prefixes = [\"python\", \"c++\", \"minikube\", \"docker\", \"json\", \"java\", \n",
    "#                  \"php\", \"bash\", \"c#\", \"cpp\", \"css\", \"perl\", \"html\", \"xml\", \n",
    "#                  \"ruby\", \"sql\", \"ini\", \"apt\", \"socat\", \"tcp\", \"localhost\",\n",
    "#                  \"git\"]\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(lang_dict['__label__en'])\n",
    "n_thread = 64 * 7\n",
    "\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_translate_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d1d4-7a7c-45c2-94c8-5f07c79ac036",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70b0f35d-5acb-4d28-9085-a1aeb6b71e10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 41s, sys: 1min 1s, total: 6min 43s\n",
      "Wall time: 10min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"dpo translate\"\"\"\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def send_translate_request(new_dataset):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx >= len_dataset:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = lang_dict['__label__en'][pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_dataset}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        _input = data['input']\n",
    "        chosen = data['chosen']\n",
    "        rejected = data['rejected']\n",
    "        \n",
    "        new_conv = {'input': '', 'chosen': '', 'rejected': ''}\n",
    "        for vidx, value in enumerate([_input, chosen, rejected]):\n",
    "            # response\n",
    "            results = \"\"\n",
    "            text_blocks = []\n",
    "            code_blocks = []\n",
    "            for bidx, block in enumerate(value.split(\"```\")):\n",
    "                if bidx % 2 == 0:\n",
    "                    text_blocks.append(block)\n",
    "                else:\n",
    "                    code_blocks.append(block)\n",
    "\n",
    "            for tidx, text_block in enumerate(text_blocks):\n",
    "                prompt = f\"### 영어:\\n{text_block}\\n### 한국어:\\n\"\n",
    "                input_json = {\n",
    "                    \"model_name\": \"Gugugo-koen-7B-V1.1\",\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.8,\n",
    "                    \"max_new_tokens\": 4096,\n",
    "                    \"stop\": [\"</끝>\", \"###\"],\n",
    "                }\n",
    "\n",
    "                ret = requests.post(\n",
    "                    api_server_url + \"/worker_generate_stream\",\n",
    "                    json=input_json,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                for chunk in ret.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n",
    "                    if chunk:\n",
    "                        result_data = json.loads(chunk.decode())\n",
    "\n",
    "                result = result_data['text'][len(prompt):].rstrip('\\n')\n",
    "                results += result\n",
    "                if len(code_blocks) > tidx:\n",
    "                    results += \"```\" + code_blocks[tidx] + \"```\"\n",
    "                    \n",
    "            if vidx == 0:\n",
    "                new_conv['input'] = results\n",
    "            elif vidx == 1:\n",
    "                new_conv['chosen'] = results\n",
    "            else:\n",
    "                new_conv['rejected'] = results\n",
    "            \n",
    "        new_dataset.append({\n",
    "            **data,\n",
    "            **new_conv,\n",
    "        })\n",
    "\n",
    "\n",
    "new_dataset = []\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_dataset = len(lang_dict['__label__en'])\n",
    "n_thread = 64 * 7\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_translate_request, args=(new_dataset,)) # \n",
    "    t.start()\n",
    "    # time.sleep(0.5)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b0e77912-fd55-4774-b413-0a14b2d3bdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '`2x^2 - 3y + 7` 식에 `x = 5`와 `y = -2`를 대입하면 결과는 무엇입니까?\\n심호흡을 하고 단계별로 천천히 생각하여 정확한 답을 내세요.',\n",
       " 'chosen': '식 `2x^2 - 3y + 7`에 `x = 5`와 `y = -2`를 대입하면 결과를 구할 수 있습니다.\\n\\n식에서 `x`와 `y` 변수에 `x = 5`와 `y = -2`를 대입하면 다음과 같습니다:\\n\\n2(5)^2 - 3(-2) + 7\\n\\n이를 단순화하면 다음과 같습니다:\\n2(25) + 6 + 7\\n\\n이제 연산을 수행해 보겠습니다:\\n50 + 6 + 7 = 63\\n\\n따라서 식 `2x^2 - 3y + 7`에 `x = 5`와 `y = -2`를 대입하면 결과는 63입니다.',\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': '식 `2x^2 - 3y + 7`에 `x = 5`와 `y = -2`를 대입하면 식을 `x`와 `y`를 식에서 `5`와 `-2`로 바꾸어 단순화해야 합니다. \\n\\nx = 5`와 `y = -2`를 대입합니다:\\n2(5)^2 - 3(-2) + 7\\n\\n더 단순화:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\n따라서 `2x^2 - 3y + 7` 식에 `x = 5`와 `y = -2`를 대입하면 63이라는 결과가 나옵니다.'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dff716d9-b345-4f39-9380-22bb3bbec251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 111092.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': \"To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we will substitute these values for the variables `x` and `y` in the given expression and simplify.\\n\\nSubstituting `x = 5` and `y = -2` into the expression, we get:\\n\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying, we have:\\n\\n2(25) + 6 + 7\\n\\nNow, let's perform the operations:\\n\\n50 + 6 + 7 = 63\\n\\nTherefore, the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` is 63.\",\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': 'To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we need to replace `x` with `5` and `y` with `-2` in the expression and simplify it. \\n\\nSubstituting `x = 5` and `y = -2`:\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying further:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\nTherefore, substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` gives us a result of 63.'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_distilabel-math-preference-dpo_1'\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e79701-693c-4413-8448-73fd1550de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46462/46462 [00:01<00:00, 40620.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rejected': '최선을 다하겠습니다.',\n",
       " 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d',\n",
       " 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.',\n",
       " 'input': '내 질문에 10단어 이내로 답변하세요.',\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d'\n",
    "filtered_row = temp_dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "427ff39e-1850-4544-b75e-abeebd1f119f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_dataset = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27752692-5158-464b-ac1e-eb47b8ad3aea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57174"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d440c-e3d6-4d2d-8172-846840c2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고, 빈 데이터 채운 뒤에 합치기, 저장, 이후 dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59121201-1890-4a24-949f-a9dd305d3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고\n",
    "with open(\"/data/llm_datasets/custom/kodpo/temp_en.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)\n",
    "    \n",
    "# temp_dataset = load_dataset(\"json\", data_files=)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fd8fee2c-0553-4160-a60d-5345c7750b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1259.17it/s]\n",
      "Generating train split: 2418 examples [00:00, 8511.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/temp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c7485e6e-38ac-49d6-9ac4-030a5d6ca67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 2471.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/2418 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2309/2309 [00:00<00:00, 36886.87 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2309 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2309/2309 [01:02<00:00, 37.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/2309 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = dedup_repetition(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_short(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_too_much_token(temp_dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45689375-7241-4230-972a-a70f91fdbe00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62a62cc-cd85-43f1-939b-c3358bae6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/distilabel-math-preference-dpo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e1a050d-8bda-4edc-b579-e539f7fb2b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'chosen', 'task', 'id', 'rejected'],\n",
       "    num_rows: 2418\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "655d38b0-e098-4960-b08d-d5e2dc9f0b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 35069.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 데이터 채운 뒤에 합치기\n",
    "# dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\")\n",
    "selected_id = temp_dataset['id']\n",
    "\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] not in selected_id)\n",
    "\n",
    "new_dataset = []\n",
    "for data in temp_dataset:\n",
    "    new_dataset.append(data)\n",
    "\n",
    "for data in filtered_row:\n",
    "    new_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "be6eee56-4869-42fa-886f-cb1f3a2c2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2296, 122)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_dataset), len(filtered_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bbe021dc-5102-4f9f-b2d1-3e94744a06f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2418, 2418)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cd81c44d-dff5-4892-98e9-c249a6c7eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '선형 방정식에서 Y-절편은 어떤 역할을 하나요?\\n 심호흡을 하고 한 단계씩 생각하여 정확한 답변을 하세요.',\n",
       " 'chosen': '선형 방정식의 y-절편은 독립 변수(x)가 0일 때 종속 변수(y)의 값을 나타냅니다. 즉, 선이 y축을 교차하거나 교차하는 지점을 나타냅니다. y-절편은 일반적으로 좌표(0, b)로 표시되며, 여기서 b는 y-절편 값입니다. y-절편은 그래프에서 선이 수직 y축을 교차하는 지점을 결정하는 데 도움이 됩니다. y-절편은 방정식으로 모델링되는 관계의 초기 조건 또는 기준값에 대한 정보를 제공하기 때문에 선형 방정식에서 중요한 특징입니다.',\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_188',\n",
       " 'rejected': '선형 방정식의 y-절편은 독립 변수(일반적으로 \"x\"로 표시)가 0일 때 종속 변수(일반적으로 \"y\"로 표시)의 값을 나타냅니다. 기하학적으로 y-절점은 선이 y축과 교차하는 지점입니다. y-절점은 일정한 값으로 선의 모든 지점에서 동일하게 유지됩니다. 선의 초기 값 또는 시작 지점을 결정하는 데 도움이 되며 \"x\"가 변함에 따라 선의 동작에 대한 정보를 제공합니다.'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bcbeea78-756b-42dc-a121-71f78bd17fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "with open(\"/data/llm_datasets/custom/kodpo/translated/ko_distilabel-math-preference-dpo.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aef5be7c-1930-417c-aa93-454202e9ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1099.71it/s]\n",
      "Generating train split: 12859 examples [00:01, 9282.99 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'chosen', 'system', 'task', 'id', 'rejected'],\n",
       "    num_rows: 12859\n",
       "})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/translated/ko_orca_dpo_pairs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abff4343-5361-484e-b258-3718510a5fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61964/61964 [00:04<00:00, 14176.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 이후 dedup\n",
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "# dataset\n",
    "prompt_template='dpo'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'dpo':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['input'][:target_text_len]\n",
    "        }\n",
    "    \n",
    "question_dataset = dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac8631e-eed8-4866-b245-8ac6fd8249ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    # documents = result['documents'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "\n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "dataset = dataset.select(selected_ids)\n",
    "\n",
    "# return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49f8a37a-a4ef-4fdb-9400-66492f99a20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=\"이 과제는 벵골어로 된 문장을 영어로 번역하는 것입니다.\\nQ: অথবা আপনি হয়ত বলবেন, 'না, আমার খেয়াল হয়েছে যে আমি আসলেই মহিলাদ\", n_results=min(n_results, len(query_ids)), include=['distances', 'documents'])\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafa2d3-234d-4bea-9429-464bb1c60e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dabb8540-457b-4acc-8449-c7ad15d60bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dedup_non_pair(dataset, data_format='sft'):\n",
    "    def validate_non_pair(data):\n",
    "        dedup_flag = False\n",
    "        if data_format == 'sft':\n",
    "            conversations = data['conversations']\n",
    "\n",
    "            if len(conversations) == 0: # empty\n",
    "                return False\n",
    "\n",
    "            if conversations[0][\"from\"] != 'human': # skip first if it's not human\n",
    "                conversations = conversations[1:]\n",
    "\n",
    "            for idx, conv in enumerate(conversations): # check right pairs\n",
    "                role = conv['from']\n",
    "                if idx % 2 == 0 and role != 'human':\n",
    "                    dedup_flag = True\n",
    "                    break\n",
    "                elif idx % 2 == 1 and role != 'gpt':\n",
    "                    dedup_flag = True\n",
    "                    break\n",
    "        elif data_format == 'dpo':\n",
    "            if data['input'] and data['chosen'] and data['rejected']:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "                \n",
    "        # if dedup_flag:\n",
    "        #     print(conv)\n",
    "        \n",
    "        return not dedup_flag\n",
    "    \n",
    "    start = len(dataset)\n",
    "    dataset = dataset.filter(validate_non_pair)\n",
    "    print(f\"{start - len(dataset)}/{start} deduped\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1282f99e-00b3-4a35-95a8-6ba6f4135b79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/kodpo/deduped/ko_ultrafeedback_binarized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7108.99it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 861.08it/s]\n",
      "Generating train split: 61807 examples [00:06, 9836.59 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'chosen', 'task', 'id', 'rejected'],\n",
       "    num_rows: 61807\n",
       "})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = dpo_list3[2]\n",
    "print(dataset_path)\n",
    "dataset = load_dpo_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "587eb420-0603-421d-9b30-a446668597b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 63124.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2418 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_ = dedup_non_pair(dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1f4f9253-705b-4984-8572-f51e45b0fde8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_:\n",
    "    new_dataset.append(data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/kodpo/deduped/ko_ultrafeedback_binarized.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "524deefb-a350-4de0-933f-3a4657af343c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/kodpo/translated/ko_distilabel-math-preference-dpo.json\n",
      "2418 3 2 0 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" find odd code blocks(dpo)\"\"\"\n",
    "dataset_path = dpo_list4[2]\n",
    "print(dataset_path)\n",
    "# dataset = load_dataset(\"json\", dataset_path)\n",
    "dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# new_dataset = []\n",
    "code_prefixes = []\n",
    "\n",
    "odd_dataset = []\n",
    "oddd_dataset = []\n",
    "odd_idxs = set()\n",
    "normal_dataset = []\n",
    "flag_normal = True\n",
    "flag_code = False\n",
    "num_normal = 0\n",
    "for idx, data in enumerate(train_dataset):\n",
    "    # conversations = data['conversations']\n",
    "    flag_normal = True\n",
    "    for conv in [data['input'], data['chosen'], data['rejected']]:\n",
    "        _value = conv\n",
    "        flag_code = False\n",
    "        find_iter = re.finditer('```', _value)\n",
    "        temp_num = 0\n",
    "        for fidx, ftext in enumerate(find_iter):\n",
    "            flag_code = True\n",
    "            start_index = ftext.start() + 3\n",
    "            # new_dataset.append(data)\n",
    "            #TODO: 스페이스바가 바로 오는 경우..\n",
    "            candidate = re.split(r'[\\n]', _value[start_index:])[0]\n",
    "            if fidx % 2 == 0 and '```' not in candidate and candidate not in available_code_prefixes:\n",
    "                odd_dataset.append((candidate, data))\n",
    "                code_prefixes.append(candidate)\n",
    "                odd_idxs.add(idx)\n",
    "                flag_normal = False\n",
    "                break\n",
    "            temp_num += 1\n",
    "        if temp_num % 2 != 0:\n",
    "            oddd_dataset.append(('odd', data))\n",
    "            odd_idxs.add(idx)\n",
    "            flag_normal = False\n",
    "        \n",
    "        if not flag_normal:\n",
    "            break\n",
    "    \n",
    "    if flag_code and flag_normal:\n",
    "        normal_dataset.append(data)\n",
    "        num_normal += 1\n",
    "\n",
    "print(len(train_dataset), len(normal_dataset), len(odd_dataset), len(oddd_dataset), len(odd_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "cfc0718e-3bc3-4953-862d-59c864220762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' ',\n",
       " {'input': '행렬과 가우시안 제거를 사용하여 선형 방정식 시스템을 푸는 절차를 자세히 설명하세요.\\n 심호흡을 하고 단계별로 천천히 생각한 후 정확한 답변을 제공하세요.',\n",
       "  'chosen': '행렬과 가우시안 제거를 사용하여 선형 방정식 문제를 풀려면 다음 단계를 따르세요:1. 주어진 선형 방정식 시스템을 행렬 형식으로 적습니다. 예를 들어, 다음과 같은 방정식이 주어졌다고 가정합니다:   - x + 2y - z = 4   - 2x - y + z = 1   - 3x + y + z = 2   행렬 표현은 다음과 같습니다:   [1  2 -1 | 4]   [2 -1 | 1]   [3 | 1] 1 | 2]2. 가우시안 제거를 적용하여 행렬을 행렬의 대각선 형태로 변환합니다. 주요 목표는 행렬의 왼쪽 아래 부분이 모두 0이 되도록 하는 것입니다. 왼쪽 열(열 1)에 초점을 맞추고 행렬을 행으로 나누어 대각선 요소 아래에 0이 되도록 행렬을 행으로 나눕니다.   - 행렬에 왼쪽 위쪽 모서리에 0이 아닌 요소가 있는 행이 있으면 그 행을 같은 열에 0이 아닌 요소가 있는 행과 바꾸세요. 이렇게 하면 나중에 0으로 나누는 것을 피할 수 있습니다.   - 행을 대각선 값으로 나누어 행을 1로 만듭니다.   행을 행으로 나누어 행을 행으로 나눕니다. 3. 행렬이 행렬 대각선 형식이 되면 행에서 아래로 내려가서 주요 대각선(역수법)에서 0이 아닌 요소를 제거합니다. 오른쪽 열(아직 0이 아닌 경우)을 먼저 시작하고 행 곱셈을 사용하여 피벗 위에 있는 요소를 0으로 만듭니다. 대각선 위에 있는 모든 0이 아닌 요소를 제거할 때까지 열별로 아래로 내려가면서 작업합니다.4. 결과 행렬은 행렬 대각선 형식이 됩니다. 오른쪽 열은 방정식 시스템의 변수에 대한 해를 나타냅니다.   - 행렬에 해가 없거나 무한한 해가 있으면 행렬 대각선 형식에서 분명해질 것입니다.   - 각 변수에 고유한 해가 있으면 행렬은 해로 구성된 행렬의 형태로 변합니다.1단계의 예를 사용하면 다음과 같이 단계가 진행됩니다:   [1  2 -1 | 4]   [2 -1 | 1]   [3 | 1] 1 | 2]   [1  2 -1 | 4]   [0 -5 |-7]   [0 -5 |-7]   [1 | 1.4]   [0 | 0 | 0]   [1 | 0.4 | 1.2]   [0 | 0 | 0]   행렬 대각선 형식은 x = 1.2 - 0.4z, y = 1.4 + 0.6z, z는 어떤 값을 취할 수 있음을 나타냅니다.작업을 다시 확인하고 원래 방정식에 다시 대입하여 주어진 방정식을 모두 만족하는지 확인하여 솔루션의 정확성을 확인하는 것을 잊지 마세요.',\n",
       "  'task': 'dpo',\n",
       "  'id': 'Ko_distilabel-math-preference-dpo_1778',\n",
       "  'rejected': '네! 다음은 행렬과 가우시안 제거를 사용하여 선형 방정식 문제를 푸는 자세한 절차입니다:\\n\\n1. 행렬 방정식으로 식을 서술합니다. 예를 들어, 세 개의 변수가 있는 세 개의 방정식 시스템이 있다고 가정해 보겠습니다:``` \\n   2x + 3y - z = 7  \\n   x - 2y + 4z = -4  \\n   3x + y - 5z = 10  \\n   ```\\n  -로 표현할 수 있습니다:\\n  -로 표현할 수 있습니다.\\n  -로 표현할 수 있습니다.```\\n   [2  3  -1] [x]   [7]  \\n   [1  -2  4] [y] = [-4] \\n   [3  1   -5] [z]   [10] \\n   ```\\n\\n\\n2. 이 행렬은 보강 행렬이라고 합니다. 이 행렬은 방정식의 오른쪽 변에 있는 변수와 상수들의 계수들로 구성됩니다.\\n3. 가우시안 제거를 사용하여 보강 행렬을 단순화하기 위해 행렬을 행렬로 수행합니다. 목표는 보강 행렬을 행렬 echelon 형식으로 또는 행렬 echelon 형식으로 변환하는 것입니다. 이를 위해 세 가지 유형의 행렬 연산을 사용할 수 있습니다:\\n   - 두 행을 교환합니다.\\n   - 행을 0이 아닌 스칼라로 곱합니다.\\n   - 행을 다른 행의 일부에 곱하기 또는 더하거나 빼기.\\n4. 행렬을 행렬식으로 처리하여 보강 행렬의 아래에 있는 변수를 제거하기 위해 행렬식을 순차적으로 적용합니다. 첫 번째 열에서 시작하여 행렬식을 사용하여 모든 요소를 메인 대각선 아래로 0으로 만듭니다.\\n5. 행렬을 단순화하면 행렬이 행렬 echelon 형식이어야 하며, 왼쪽에서 오른쪽으로 \"선행 1\"인 대각선이 있어야 합니다. 다른 요소는 0입니다.\\n7. 행렬의 오른쪽 열은 방정식 시스템의 변수 값(x, y, z)에 해당합니다. 이 값을 읽어서 선형 방정식 시스템의 해를 구합니다.\\n8. 행렬의 행이 모두 0이면 시스템에 무한한 해가 있습니다. 이러한 경우 자유 변수가 있습니다.\\n그게 다입니다! 이 단계를 따르면 행렬과 가우시안 제거를 사용하여 선형 방정식을 풀 수 있습니다.'})"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "10f500ce-5942-47e6-8cb8-a9913a8cefea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in combined_dataset:\n",
    "    new_dataset.append(data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/dpo/ados_dpo_v2.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e0c2c690-4738-4264-ac4e-a132408776a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9554.22it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1120.87it/s]\n",
      "Generating train split: 61418 examples [00:06, 10150.79 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': Value(dtype='string', id=None),\n",
       " 'chosen': Value(dtype='string', id=None),\n",
       " 'task': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None),\n",
       " 'rejected': Value(dtype='string', id=None),\n",
       " 'system': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_dpo_dataset(dataset_path) for dataset_path in dpo_list5])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "18301dab-2f66-4fb3-8a8c-ab8977fffd75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '백악관 - 도널드 트럼프 대통령은 월요일 자신에게 \"절대적인 권리\"가 있으며 2016년 대선 캠페인와 러시아와의 관련성에 대한 연방 정부의 조사를 \"완전히 위헌적\"이라고 주장하면서 트위터에 트윗을 올렸습니다. 트럼프는 기존 규정을 무시하고 대통령의 권한에 대한 이해된 한계를 무시하는 경우가 많지만, 이번 발언은 일부 법률 학자들에 의해 전례가 없는 것으로 간주되며 헌법의 한계를 시험하고 있습니다. 조지타운 대학의 법학 교수인 수잔 로브 블러치는 \"트럼프의 트윗에 충격을 받았다\"고 말하며 \"대통령은 자신을 사면할 수 없습니다. 법원이 \"아니요\"라고 말했고, 대통령이 사면을 주면 \"대부분의 의회는 탄핵할 것\"이라고 말했다. 이 교수는 대통령이 재임 중 기소되고 재판을 받을 수 있는지에 대해 상원에서 증언한 바 있는 교수로, 특검은 \"완전히 헌법에 부합한다\"고 말합니다. 프린스턴 대학의 다른 헌법학자인 키스 휘트링턴 교수는 \"대통령의 사면에 관한 심각한 논쟁이 법학자들 사이에서 논의되고 있는 점을 언급한 것은 중요하지만, 이는 궁극적으로 법원에서 해결될 어려운 특이한 헌법적 질문이라고 생각합니다.\"라고 말했습니다. 노스웨스턴 대학교 법대 교수 앤드류 코펠먼에 따르면, \"사면에 관한 트럼프의 입장은 헌법의 텍스트에 부합하지만 전반적인 구조와 모순됩니다. 대통령이 사면을 통해 사형이나 다른 연방 범죄를 저지른 후 즉시 자신을 사면할 수 있다는 의미입니다.\"라고 말했습니다. \"헌법 전체에 맞지 않습니다.\" 트럼프가 자신에 대한 사면 가능성에 대해 기자에게 질문하자 백악관 대변인 사라 샌더스 대변인은 \"질문: 어느 대통령이 자신을 사면하려 한 적이 없다고 VOA에 말했나요? 옵션: A. 조지타운 대학의 수잔 로브 블러치 B. 백악관 대변인 사라 샌더스 C. 프린스턴 대학의 키트 휘트링 D. 정보가 충분하지 않음 === 정답은',\n",
       " 'chosen': 'A. 조지 워싱턴 대학교의 수잔 로우 블로치',\n",
       " 'task': 'dpo_system',\n",
       " 'id': 'Ko_orca_dpo_pairs_10426',\n",
       " 'rejected': ' 제공된 정보에 따르면 정답은 A. 조지타운 대학교의 수잔 로우 블로치입니다. 텍스트에 따르면 수잔 로우 블로치는 대통령이 자신을 사면하려고 시도한 적이 없다고 말했으며, 그녀는 조지타운 대학교의 법학 교수로 알려져 있습니다. 다른 옵션은 텍스트에 의해 뒷받침되지 않습니다.',\n",
       " 'system': 'You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.'}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset[70002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ef3a82-8770-447d-a54e-dd685c1072b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ados_DPODataset:\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_path=\"/data/llm_datasets/custom/ados/dpo/ados_dpo_v2.json\",\n",
    "        eval_dataset_path = \"\", #/data/llm_datasets/Ultrafeedback_binarized.ko.hankang/test_prefs.json.kr\n",
    "        data_format='chat-orca',\n",
    "        # search_term='\\n\\n### Assistant:',\n",
    "        num_train=None,\n",
    "        num_eval=None,\n",
    "    ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.eval_dataset_path = dataset_path\n",
    "        if eval_dataset_path:\n",
    "            self.eval_dataset_path = eval_dataset_path\n",
    "        self.data_format = data_format\n",
    "        # self.search_term = search_term\n",
    "        self.num_train = num_train\n",
    "        self.num_eval = num_eval\n",
    "    \n",
    "    def get_prompt_and_response(self, data):\n",
    "        conv = get_conversation_template(self.data_format)\n",
    "        if data['system']:\n",
    "            conv.system_message = conv.tasks['system_instruct'].format(system=data['system'])\n",
    "        conv.append_message(conv.roles[0], data['input'])\n",
    "        conv.append_message(conv.roles[1], '')\n",
    "        prompt = conv.get_prompt()\n",
    "        conv.update_last_message(data['chosen'])\n",
    "        chosen = conv.get_prompt()[len(prompt):]\n",
    "        conv.update_last_message(data['rejected'])\n",
    "        rejected = conv.get_prompt()[len(prompt):]\n",
    "        \n",
    "        return prompt, chosen, rejected\n",
    "    \n",
    "    def make_dpo_data_module(self):\n",
    "        def split_prompt_and_responses(data) -> Dict[str, str]:\n",
    "            prompt, chosen, rejected = self.get_prompt_and_response(data)\n",
    "            # prompt = extract_anthropic_prompt(prompt_and_response, self.search_term)\n",
    "            # promopt_rejected = extract_anthropic_prompt(prompt_and_response_rejected, self.search_term)\n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected,\n",
    "            }\n",
    "                             \n",
    "        train_dataset = load_dpo_dataset(self.dataset_path)\n",
    "        eval_dataset = load_dpo_dataset(self.eval_dataset_path)\n",
    "\n",
    "        original_columns = list(train_dataset.features.keys())\n",
    "        original_columns_eval = list(eval_dataset.features.keys())\n",
    "\n",
    "        if self.num_train is not None:\n",
    "            train_dataset = train_dataset.select(range(min(len(train_dataset), self.num_train)))\n",
    "        if self.num_eval is not None:\n",
    "            eval_dataset = eval_dataset.select(range(min(len(train_dataset), self.num_eval)))\n",
    "\n",
    "        train_dataset = train_dataset.map(split_prompt_and_responses, remove_columns=original_columns)\n",
    "        eval_dataset = eval_dataset.map(split_prompt_and_responses, remove_columns=original_columns_eval)\n",
    "\n",
    "        return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b204ad5-421e-413f-98a9-8089d91b09a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76695/76695 [00:07<00:00, 9769.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import ados_DPODataset\n",
    "\n",
    "dpo_dataset = ados_DPODataset()\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "822481ba-65ba-41a6-bb2f-7efbacd2600a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': ' 질문은 다음과 같을 수 있습니다: \"필립 바드의 연구에 따르면 감각 정보와 운동 정보 외에 어떤 유형의 정보가 더 이상 처리되기 전에 중뇌를 통과해야 하는 정보에는 어떤 것이 있었습니까?\"</s>',\n",
       " 'rejected': '  이 글에서 제공된 정보에 따르면, 감각, 운동, 생리적 정보 모두 추가 처리를 받기 전에 감각계(특히 시상)를 통과해야 했습니다. 따라서 질문에 대한 정답은 다음과 같습니다:\\n생리적 정보.\\n\\n카논은 감각적 사건이 의식을 촉발하기 전에 생리적 반응을 유발할 수 있는 것은 해부학적으로 불가능하며, 감정적 자극은 생리적 반응과 경험적 측면의 감정을 동시에 유발해야 한다고 주장했기 때문입니다.</s>',\n",
       " 'prompt': '### System:\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.\\n\\n### User: 이 기사에 대한 질문은 무엇인가요? 질문이 불가능한 경우 \"불가능\"이라고 말하세요.\\n\\n필립 바드는 동물 연구를 통해 이론에 기여했습니다. 바드는 감각, 운동 및 생리적 정보가 더 이상의 처리를 받기 전에 모두 시상(특히 시상)을 통과해야 한다는 것을 발견했습니다. 따라서 카논은 감각적 이벤트가 의식적 인식을 촉발하기 전에 생리적 반응을 유발할 수 없으며, 감정적 자극이 생리적 및 경험적 측면의 감정을 동시에 유발해야 한다고 주장했습니다.\\n\\n 감각 및 운동 정보와 함께 처리된 후 시상에는 어떤 정보가 통과해야 했나요?\\n\\n### Assistant:'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_datamodule['train_dataset'][-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a00a9a7f-e892-4673-8706-775cce38d063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combined_dataset[70002]\n",
    "\n",
    "conv = get_conversation_template('chat-orca')\n",
    "conv.system_message = conv.tasks['system_instruct'].format(system=data['system'])\n",
    "conv.append_message(conv.roles[0], data['input'])\n",
    "conv.append_message(conv.roles[1], '')\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "conv.update_last_message(data['chosen'])\n",
    "chosen = conv.get_prompt()[len(prompt):]\n",
    "conv.update_last_message(data['rejected'])\n",
    "rejected = conv.get_prompt()[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5c87a9b4-1d5e-4053-8f41-10cfbf7f3b72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### System:\\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\\n\\n### User: 백악관 - 도널드 트럼프 대통령은 월요일 자신에게 \"절대적인 권리\"가 있으며 2016년 대선 캠페인와 러시아와의 관련성에 대한 연방 정부의 조사를 \"완전히 위헌적\"이라고 주장하면서 트위터에 트윗을 올렸습니다. 트럼프는 기존 규정을 무시하고 대통령의 권한에 대한 이해된 한계를 무시하는 경우가 많지만, 이번 발언은 일부 법률 학자들에 의해 전례가 없는 것으로 간주되며 헌법의 한계를 시험하고 있습니다. 조지타운 대학의 법학 교수인 수잔 로브 블러치는 \"트럼프의 트윗에 충격을 받았다\"고 말하며 \"대통령은 자신을 사면할 수 없습니다. 법원이 \"아니요\"라고 말했고, 대통령이 사면을 주면 \"대부분의 의회는 탄핵할 것\"이라고 말했다. 이 교수는 대통령이 재임 중 기소되고 재판을 받을 수 있는지에 대해 상원에서 증언한 바 있는 교수로, 특검은 \"완전히 헌법에 부합한다\"고 말합니다. 프린스턴 대학의 다른 헌법학자인 키스 휘트링턴 교수는 \"대통령의 사면에 관한 심각한 논쟁이 법학자들 사이에서 논의되고 있는 점을 언급한 것은 중요하지만, 이는 궁극적으로 법원에서 해결될 어려운 특이한 헌법적 질문이라고 생각합니다.\"라고 말했습니다. 노스웨스턴 대학교 법대 교수 앤드류 코펠먼에 따르면, \"사면에 관한 트럼프의 입장은 헌법의 텍스트에 부합하지만 전반적인 구조와 모순됩니다. 대통령이 사면을 통해 사형이나 다른 연방 범죄를 저지른 후 즉시 자신을 사면할 수 있다는 의미입니다.\"라고 말했습니다. \"헌법 전체에 맞지 않습니다.\" 트럼프가 자신에 대한 사면 가능성에 대해 기자에게 질문하자 백악관 대변인 사라 샌더스 대변인은 \"질문: 어느 대통령이 자신을 사면하려 한 적이 없다고 VOA에 말했나요? 옵션: A. 조지타운 대학의 수잔 로브 블러치 B. 백악관 대변인 사라 샌더스 C. 프린스턴 대학의 키트 휘트링 D. 정보가 충분하지 않음 === 정답은\\n\\n### Assistant: A. 조지 워싱턴 대학교의 수잔 로우 블로치</s>'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c31fde64-db64-439b-bd31-b6a4c95b81c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### System:\\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\\n\\n### User: 백악관 - 도널드 트럼프 대통령은 월요일 자신에게 \"절대적인 권리\"가 있으며 2016년 대선 캠페인와 러시아와의 관련성에 대한 연방 정부의 조사를 \"완전히 위헌적\"이라고 주장하면서 트위터에 트윗을 올렸습니다. 트럼프는 기존 규정을 무시하고 대통령의 권한에 대한 이해된 한계를 무시하는 경우가 많지만, 이번 발언은 일부 법률 학자들에 의해 전례가 없는 것으로 간주되며 헌법의 한계를 시험하고 있습니다. 조지타운 대학의 법학 교수인 수잔 로브 블러치는 \"트럼프의 트윗에 충격을 받았다\"고 말하며 \"대통령은 자신을 사면할 수 없습니다. 법원이 \"아니요\"라고 말했고, 대통령이 사면을 주면 \"대부분의 의회는 탄핵할 것\"이라고 말했다. 이 교수는 대통령이 재임 중 기소되고 재판을 받을 수 있는지에 대해 상원에서 증언한 바 있는 교수로, 특검은 \"완전히 헌법에 부합한다\"고 말합니다. 프린스턴 대학의 다른 헌법학자인 키스 휘트링턴 교수는 \"대통령의 사면에 관한 심각한 논쟁이 법학자들 사이에서 논의되고 있는 점을 언급한 것은 중요하지만, 이는 궁극적으로 법원에서 해결될 어려운 특이한 헌법적 질문이라고 생각합니다.\"라고 말했습니다. 노스웨스턴 대학교 법대 교수 앤드류 코펠먼에 따르면, \"사면에 관한 트럼프의 입장은 헌법의 텍스트에 부합하지만 전반적인 구조와 모순됩니다. 대통령이 사면을 통해 사형이나 다른 연방 범죄를 저지른 후 즉시 자신을 사면할 수 있다는 의미입니다.\"라고 말했습니다. \"헌법 전체에 맞지 않습니다.\" 트럼프가 자신에 대한 사면 가능성에 대해 기자에게 질문하자 백악관 대변인 사라 샌더스 대변인은 \"질문: 어느 대통령이 자신을 사면하려 한 적이 없다고 VOA에 말했나요? 옵션: A. 조지타운 대학의 수잔 로브 블러치 B. 백악관 대변인 사라 샌더스 C. 프린스턴 대학의 키트 휘트링 D. 정보가 충분하지 않음 === 정답은\\n\\n### Assistant:'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d0fc128b-f4f9-4b01-bfe1-053c8c646f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A. 조지 워싱턴 대학교의 수잔 로우 블로치</s>'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "873ccadb-7b1a-45f0-9c15-5f20b46f381e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  제공된 정보에 따르면 정답은 A. 조지타운 대학교의 수잔 로우 블로치입니다. 텍스트에 따르면 수잔 로우 블로치는 대통령이 자신을 사면하려고 시도한 적이 없다고 말했으며, 그녀는 조지타운 대학교의 법학 교수로 알려져 있습니다. 다른 옵션은 텍스트에 의해 뒷받침되지 않습니다.</s>'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04d261-89a1-4773-b614-442b8fac593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = {\n",
    "    \"win_fraction_heatmap\": [],\n",
    "    \"battle_count_heatmap\": [],\n",
    "    \"bootstrap_elo_rating\": [],\n",
    "    \"average_win_rate_bar\": [],\n",
    "}\n",
    "\n",
    "\n",
    "with open('../logs/elo_result.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
