{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7b100-ca9e-4608-85ae-c20b612af78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2024jan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "# lang_detect = fasttext.load_model('../fastchat/modules/fasttext/lid.176.bin')\n",
    "\n",
    "from fastchat.modules.answer_refiner import generate_refiner\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import random\n",
    "from fastchat.modules.embedder_adapter import Embedder, get_embedder\n",
    "from fastchat.conversation import (\n",
    "    SeparatorStyle,\n",
    ")\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "import copy\n",
    "from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "from fastchat.train.data_modules.dpo_dataset import load_dpo_dataset\n",
    "from fastchat.train.data_modules.dedup import (\n",
    "    dedup_by_similarity,\n",
    "    dedup_non_pair,\n",
    "    dedup_repetition,\n",
    "    dedup_math,\n",
    "    dedup_too_much_token,\n",
    "    dedup_short,\n",
    ")\n",
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df680a11-11cc-42bd-b5ff-3f7b16dfa17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def dedup_by_similarity(dataset, prompt_template='chat-orca', target_text_len=100, n_results=100, distance_threshold = 0.6):\n",
    "_dataset = train_dataset\n",
    "prompt_template='vicuna'\n",
    "target_text_len=100\n",
    "n_results=100\n",
    "distance_threshold = 0.35\n",
    "    \n",
    "if prompt_template == 'chat-orca':\n",
    "    conv = get_conversation_template(prompt_template)\n",
    "    system_message = conv.system_message\n",
    "    sep_style = conv.sep_style\n",
    "    sep = conv.sep\n",
    "    prompt_user, prompt_bot = conv.roles\n",
    "\n",
    "    len_sep_style = 0\n",
    "    if sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "        len_sep_style = 1\n",
    "\n",
    "    len_front = len(system_message) + len(sep) + len(prompt_user) + len_sep_style + 1\n",
    "    len_rear = len(sep) + len(prompt_bot) + len_sep_style\n",
    "    def filter_question(data):\n",
    "        return { \n",
    "            # **data,\n",
    "            'prompt': data['prompt'][len_front:-len_rear][:target_text_len]\n",
    "        }\n",
    "\n",
    "if prompt_template == 'vicuna':\n",
    "    def filter_question(data):\n",
    "        return {\n",
    "            'prompt': data['conversations'][0]['value'][:target_text_len]\n",
    "        }\n",
    "\n",
    "question_dataset = _dataset.map(filter_question)\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "embedder = get_embedder(\"ddobokki/klue-roberta-base-nli-sts-ko-en\")\n",
    "collection = chroma_client.create_collection(name=\"context\", embedding_function=embedder.embed, metadata={\"hnsw:space\": \"cosine\"})\n",
    "ids = []\n",
    "# add\n",
    "texts = question_dataset['prompt']\n",
    "last_id = -1\n",
    "new_ids = [f\"id{i+last_id+1}\" for i in range(len(texts))]\n",
    "ids += new_ids\n",
    "collection.add(documents=texts, ids=new_ids)\n",
    "\n",
    "query_ids = copy.deepcopy(new_ids)\n",
    "selected_ids = []\n",
    "duplicated_ids = []\n",
    "\n",
    "weird_ids = []\n",
    "error_ids = []\n",
    "while query_ids:\n",
    "    current_id = random.choice(query_ids)\n",
    "    if current_id in selected_ids:\n",
    "        print(\"Warning: this is weird..\")\n",
    "        weird_ids.append(current_id)\n",
    "        continue\n",
    "    selected_ids.append(current_id)\n",
    "    search_strings = [texts[int(current_id[2:])]]\n",
    "    if collection.count() == 0:\n",
    "        print(\"Warning: collection is empty. Forced break\")\n",
    "        break\n",
    "    result = collection.query(query_texts=search_strings, n_results=min(n_results, len(query_ids)), include=['distances']) #'documents'\n",
    "\n",
    "    search_ids = result['ids'][0]\n",
    "    distances = result['distances'][0]\n",
    "    remove_ids = []\n",
    "    for idx in range(len(search_ids)):\n",
    "        sid = search_ids[idx]\n",
    "        dist = distances[idx]\n",
    "        if dist < distance_threshold:\n",
    "            remove_ids.append(sid)\n",
    "\n",
    "    for rid in remove_ids:\n",
    "        if rid in query_ids:\n",
    "            query_ids.remove(rid)\n",
    "            \n",
    "    if remove_ids:\n",
    "        duplicated_ids += remove_ids\n",
    "        collection.delete(ids=remove_ids)\n",
    "    else:\n",
    "        print(\"Warning: this is error..\")\n",
    "        error_ids.append(current_id)\n",
    "\n",
    "    print(f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\", '\\t\\t\\t\\t\\t', end='\\r')\n",
    "\n",
    "print('finished dedup data:', f\"Total:{len(new_ids)} Selected:{len(selected_ids)} current_dup:{len(remove_ids)} vector_store:{collection.count()} remained:{len(query_ids)} total_dup:{len(duplicated_ids)}\")\n",
    "\n",
    "selected_ids = [int(sid[2:]) for sid in set(selected_ids)]\n",
    "\n",
    "_dataset = _dataset.select(selected_ids)\n",
    "\n",
    "# return dataset, selected_ids, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "946c5dcd-6009-44bf-aef2-dd75d1269bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(dpo_list5[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f39efe-d157-4651-a847-96b5cadf4460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a832dc69-c8bd-41cb-aca5-b4dfb33637ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo.append(random_select(dataset, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd27eb-d7fe-4dc5-93b8-25b6331f7ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset_list = []\n",
    "for d in dataset_list:\n",
    "    new_dataset_list.append(\"\\\"\" + d + \"\\\"\")\n",
    "print(' '.join(new_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a1012ac1-5dff-4f4b-aff1-cf554c3f7c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdpo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "59bdc57a-82c4-4281-bae3-dd7068914b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4efcee25-a99d-48b7-9347-a3ce950b71c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/llm_datasets/custom/deduped2/sharegpt_V3_format_translation(koen)-10000.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversations', 'task'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = dataset_list[14]\n",
    "print(dataset_path)\n",
    "dataset = load_sft_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7838928c-3b21-40e1-97fb-2804580bf1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft.append(random_select(dataset, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace9cb2f-8381-43e9-915d-b7a6e70d4879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fbfc93bc-62fb-451d-bc42-2269abbdf939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = concatenate_datasets(mdpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "33e33da3-44b5-4a25-a4a8-94795b04ba19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f144e79-4b3f-43e9-bb0a-27e5b75df8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee38e-de21-469d-9502-e8f12a1e4e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from fastchat.train.data_modules.sft_dataset import load_sft_dataset, combine_dataset\n",
    "combined_dataset = concatenate_datasets([load_sft_dataset(dataset_path) for dataset_path in dataset_list])\n",
    "combined_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fb94dc55-df6b-4c12-a23d-5e2afc3b199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in combined_dataset:\n",
    "    new_dataset.append(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7827856c-b0da-4fe8-a0fb-a07b5dbd2a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_dataset.to_json(\"/data/llm_datasets/custom/ados_sft_v4.json\")\n",
    "with open(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0b710b03-1ad8-4cd9-b69e-f4fc8b37f8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_dataset = ados_DPODataset(\"/data/llm_datasets/custom/ados/dpo/ados_mdpo_v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9a5f2a07-f51e-4e92-b648-f5f67457ab78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8719.97it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1349.52it/s]\n",
      "Generating train split: 11000 examples [00:01, 8384.35 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11000/11000 [00:01<00:00, 9377.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a6fa-810e-4d04-acfe-648894250c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "get and save language distributions\n",
    "\"\"\"\n",
    "def get_lang_distribution(dataset_list):\n",
    "    lang_distribution = {}\n",
    "    global_lang_dict = {'train': defaultdict(int), 'test': defaultdict(int)}\n",
    "    for dataset_path in dataset_list:\n",
    "        # dataset = load_sft_dataset(dataset_path, split=None)\n",
    "        dataset = load_dpo_dataset(dataset_path, split=None)\n",
    "\n",
    "        lang_splits = {}\n",
    "        for split in list(dataset.keys()):\n",
    "            print(f\"{dataset_path}:{split}\")\n",
    "            _dataset = dataset[split]\n",
    "\n",
    "            lang_dict = defaultdict(int)\n",
    "            for data in _dataset:\n",
    "                # conversations = data['conversations']\n",
    "                conversations = data['chosen_response']\n",
    "                langs = {}\n",
    "                len_conv = 0\n",
    "#                 for conv in conversations:\n",
    "#                     # _from = conv['from']\n",
    "#                     # _value = conv['value']\n",
    "#                     _from = conv['role']\n",
    "#                     _value = conv['content']\n",
    "\n",
    "#                     len_conv += len(_value)\n",
    "#                     lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "#                     lang = lang[0]\n",
    "#                     if lang not in langs:\n",
    "#                         langs[lang] = 1\n",
    "#                     else:\n",
    "#                         langs[lang] += 1\n",
    "                _value = conversations\n",
    "\n",
    "                len_conv += len(_value)\n",
    "                lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "                lang = lang[0]\n",
    "                if lang not in langs:\n",
    "                    langs[lang] = 1\n",
    "                else:\n",
    "                    langs[lang] += 1\n",
    "\n",
    "#                 if '__label__en' in langs:\n",
    "#                     langs['__label__en'] -= 1\n",
    "\n",
    "                if len(langs) == 0:\n",
    "                    dominent_lang = \"empty\"\n",
    "                else:\n",
    "                    dominent_lang = max(langs)\n",
    "                \n",
    "                # if dominent_lang not in lang_dict:\n",
    "                #     lang_dict[dominent_lang] = 1\n",
    "                # else:\n",
    "                global_lang_dict[split][dominent_lang] += 1\n",
    "                lang_dict[dominent_lang] += 1\n",
    "            lang_splits[split] = lang_dict\n",
    "        lang_distribution[dataset_path] = lang_splits\n",
    "    \n",
    "    lang_distribution['total'] = global_lang_dict\n",
    "    \n",
    "    stat_dict = {'train': {}, 'test': {}}\n",
    "    for split in ['train', 'test']:\n",
    "        total_cnt = sum([value for value in global_lang_dict[split].values()])\n",
    "        for key, value in global_lang_dict[split].items():\n",
    "            stat_dict[split][key] = f\"{value / total_cnt:.2%}\"\n",
    "    \n",
    "    lang_distribution['stat'] = stat_dict\n",
    "    \n",
    "    return lang_distribution\n",
    "\n",
    "lang_distribution = get_lang_distribution([dpo_list[2]])\n",
    "lang_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c81cd-dee2-403a-aeaf-82e00869919f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspaces/data/llm_datasets/custom/lang_distribution_SFT_v4.json\", \"w\") as json_file:\n",
    "    json.dump(lang_distribution, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e0e1-2ce5-4e98-bb8f-c3c86515bc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"/workspaces/data/llm_datasets/custom/deduped/translated_sharegpt_V3_format_ko.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe09bf-1ebb-42f2-a1fb-0ebdb9d53a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_koen = dataset['train'].select(range(10000, 15000))\n",
    "dataset_enko = dataset['train'].select(range(15000, 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1399f21-bd9d-4fd7-a319-ae31777715fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT raw data and change format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5811a-e7e8-494d-ba26-02511a406803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for data in dataset_enko:\n",
    "    conversations_ko = data['conversations']\n",
    "    _id = data['id']\n",
    "    \n",
    "    conversations_en = lang_dict['__label__en'][_id]['conversations']\n",
    "    \n",
    "    for _idx in range(len(conversations_ko)):\n",
    "        value_ko = conversations_ko[_idx]['value']\n",
    "        value_en = conversations_en[_idx]['value']\n",
    "        if len(value_ko) > 150: # 너무 짧은 데이터는 품질이 좋지 않아 제거\n",
    "            new_dataset.append({\n",
    "                'id': f\"sharegpt_V3_format_{_id}_{_idx}\",\n",
    "                'task': 'enkotranslation',\n",
    "                'conversations': [\n",
    "                                    {'from': 'human', 'value': value_en},\n",
    "                                    {'from': 'gpt', 'value': value_ko},\n",
    "                                 ],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec6f-c950-4541-ad9a-ab4cdf8173da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"make lang_dict(sft)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/ultrafeedback_binarized.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    conversations = data['conversations']\n",
    "    langs = {}\n",
    "    len_conv = 0\n",
    "    for conv in conversations:\n",
    "        _from = conv['from']\n",
    "        _value = conv['value']\n",
    "\n",
    "        len_conv += len(_value)\n",
    "        lang, conf = lang_detect.predict(_value.replace('\\n', ' '))\n",
    "        lang = lang[0]\n",
    "        if lang not in langs:\n",
    "            langs[lang] = 1\n",
    "        else:\n",
    "            langs[lang] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661bfc45-9396-4f6b-a6ff-12acc03a0b2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1016 examples [00:00, 21155.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make lang_dict(dpo)\"\"\"\n",
    "dataset = load_dataset(\"json\", data_files=\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")\n",
    "enko_dataset = []\n",
    "remained_dataset = []\n",
    "\n",
    "lang_dict = {}\n",
    "for data in dataset['train']:\n",
    "    chosen = data['chosen']\n",
    "    rejected = data['rejected']\n",
    "    \n",
    "    langs = {}\n",
    "\n",
    "    lang_chosen = lang_detect.predict(chosen.replace('\\n', ' '))[0][0]\n",
    "    if lang_chosen not in langs:\n",
    "        langs[lang_chosen] = 1\n",
    "    else:\n",
    "        langs[lang_chosen] += 1\n",
    "        \n",
    "    lang_rejected = lang_detect.predict(rejected.replace('\\n', ' '))[0][0]\n",
    "    if lang_rejected not in langs:\n",
    "        langs[lang_rejected] = 1\n",
    "    else:\n",
    "        langs[lang_rejected] += 1\n",
    "\n",
    "    if '__label__en' in langs:\n",
    "        langs['__label__en'] -= 1\n",
    "\n",
    "    dominent_lang = max(langs)\n",
    "    if dominent_lang not in lang_dict:\n",
    "        lang_dict[dominent_lang] = [data]\n",
    "    else:\n",
    "        lang_dict[dominent_lang].append(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6279a941-d152-4693-b5e6-44d63a86acbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__en 1016\n"
     ]
    }
   ],
   "source": [
    "for key, value in lang_dict.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460862df-7bbf-4a40-982d-1f7782fd22ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "api_server_url = \"http://localhost:41002\"\n",
    "def count_total_token(conversations):\n",
    "    num_token = 0\n",
    "    for conv in conversations:\n",
    "        input_json = {\n",
    "            \"model_name\": \"MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ\",\n",
    "            \"prompt\": conv['value'],\n",
    "        }\n",
    "\n",
    "        ret = requests.post(api_server_url + \"/count_token\", json=input_json)\n",
    "\n",
    "        output_json = ret.json()\n",
    "        num_token += output_json['count']\n",
    "    return num_token\n",
    "\n",
    "\n",
    "for _idx in range(10):\n",
    "    num_total_token = count_total_token(lang_dict['__label__en'][_idx])\n",
    "    print(num_total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afade76-adb4-497b-bfc7-bdf33e33663e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset += lang_dict['__label__pt'] + lang_dict['__label__es'] + lang_dict['__label__de'] + lang_dict['__label__zh'] + lang_dict['__label__fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d1d4-7a7c-45c2-94c8-5f07c79ac036",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/data/llm_datasets/custom/vicuna_format/lima_vicuna_format_ko.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565040d8-08c4-4a16-bb43-e6952114b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dff716d9-b345-4f39-9380-22bb3bbec251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2418/2418 [00:00<00:00, 111092.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`?.\\n Take a deep breath, think step by step, and give an accurate response',\n",
       " 'chosen': \"To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we will substitute these values for the variables `x` and `y` in the given expression and simplify.\\n\\nSubstituting `x = 5` and `y = -2` into the expression, we get:\\n\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying, we have:\\n\\n2(25) + 6 + 7\\n\\nNow, let's perform the operations:\\n\\n50 + 6 + 7 = 63\\n\\nTherefore, the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` is 63.\",\n",
       " 'task': 'dpo',\n",
       " 'id': 'Ko_distilabel-math-preference-dpo_1',\n",
       " 'rejected': 'To find the result of substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7`, we need to replace `x` with `5` and `y` with `-2` in the expression and simplify it. \\n\\nSubstituting `x = 5` and `y = -2`:\\n2(5)^2 - 3(-2) + 7\\n\\nSimplifying further:\\n2(25) + 6 + 7\\n50 + 6 + 7\\n63\\n\\nTherefore, substituting `x = 5` and `y = -2` into the expression `2x^2 - 3y + 7` gives us a result of 63.'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_distilabel-math-preference-dpo_1'\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42e79701-693c-4413-8448-73fd1550de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46462/46462 [00:01<00:00, 40620.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rejected': '최선을 다하겠습니다.',\n",
       " 'id': 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d',\n",
       " 'chosen': '간결한 답변을 드리기 위해 최선을 다하겠습니다.',\n",
       " 'input': '내 질문에 10단어 이내로 답변하세요.',\n",
       " 'task': 'dpo'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_row = 'Ko_ultrafeedback_binarized_0649b7b08c0cf6f529901f80480b0ba7c7d29147638c39a9150ef15fd4e0ff9d'\n",
    "filtered_row = temp_dataset.filter(lambda sample: sample['id'] == id_row)\n",
    "filtered_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d440c-e3d6-4d2d-8172-846840c2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고, 빈 데이터 채운 뒤에 합치기, 저장, 이후 dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59121201-1890-4a24-949f-a9dd305d3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# en 먼저 필터링하고\n",
    "with open(\"/data/llm_datasets/custom/kodpo/temp_en.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)\n",
    "    \n",
    "# temp_dataset = load_dataset(\"json\", data_files=)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd8fee2c-0553-4160-a60d-5345c7750b81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5336.26it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 890.70it/s]\n",
      "Generating train split: 1016 examples [00:00, 11191.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/temp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7485e6e-38ac-49d6-9ac4-030a5d6ca67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 994/994 [00:28<00:00, 35.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/994 deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# temp_dataset = dedup_repetition(temp_dataset, 'dpo')\n",
    "# temp_dataset = dedup_short(temp_dataset, 'dpo')\n",
    "temp_dataset = dedup_too_much_token(temp_dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e62a62cc-cd85-43f1-939b-c3358bae6b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "655d38b0-e098-4960-b08d-d5e2dc9f0b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1016/1016 [00:00<00:00, 43887.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 데이터 채운 뒤에 합치기\n",
    "# dataset = load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/untranslated/orca_dpo_pairs.json\")\n",
    "selected_id = temp_dataset['id']\n",
    "\n",
    "filtered_row = dataset.filter(lambda sample: sample['id'] not in selected_id)\n",
    "\n",
    "new_dataset = []\n",
    "for data in temp_dataset:\n",
    "    new_dataset.append(data)\n",
    "\n",
    "for data in filtered_row:\n",
    "    new_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be6eee56-4869-42fa-886f-cb1f3a2c2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_dataset), len(filtered_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbe021dc-5102-4f9f-b2d1-3e94744a06f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 1013)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd81c44d-dff5-4892-98e9-c249a6c7eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are an unbiased, uncensored, helpful assistant.',\n",
       " 'chosen': 'The two subjects in the American Gothic painting are a father and adult daughter. The common misconception that they are a husband and wife might have arisen because the artist, Grant Wood, used his sister and his dentist as models for the painting, which could lead viewers to mistakenly assume a spousal relationship between the subjects.',\n",
       " 'task': 'dpo_system',\n",
       " 'id': 'truthy_dpo_2e1d77fbebc31dde0bb2f9303d6761e2',\n",
       " 'input': 'Who are the two subjects in the American Gothic painting - a husband and wife, or a father and daughter?',\n",
       " 'rejected': 'A father and daughter'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcbeea78-756b-42dc-a121-71f78bd17fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장\n",
    "with open(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aef5be7c-1930-417c-aa93-454202e9ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5242.88it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 902.97it/s]\n",
      "Generating train split: 1013 examples [00:00, 13933.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'chosen', 'task', 'id', 'input', 'rejected'],\n",
       "    num_rows: 1013\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/kodpo/translated/truthy-dpo-v0.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abff4343-5361-484e-b258-3718510a5fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61964/61964 [00:04<00:00, 14176.52 examples/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac8631e-eed8-4866-b245-8ac6fd8249ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49f8a37a-a4ef-4fdb-9400-66492f99a20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=\"이 과제는 벵골어로 된 문장을 영어로 번역하는 것입니다.\\nQ: অথবা আপনি হয়ত বলবেন, 'না, আমার খেয়াল হয়েছে যে আমি আসলেই মহিলাদ\", n_results=min(n_results, len(query_ids)), include=['distances', 'documents'])\n",
    "search_ids = result['ids'][0]\n",
    "distances = result['distances'][0]\n",
    "documents = result['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafa2d3-234d-4bea-9429-464bb1c60e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb93f6b-8c02-4bd5-9698-c6207428160f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = dpo_list3[2]\n",
    "print(dataset_path)\n",
    "dataset = load_dpo_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb420-0603-421d-9b30-a446668597b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = dedup_non_pair(dataset, 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b204ad5-421e-413f-98a9-8089d91b09a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76695/76695 [00:08<00:00, 8839.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from fastchat.train.data_modules.dpo_dataset import ados_DPODataset\n",
    "\n",
    "dpo_dataset = ados_DPODataset(\n",
    "    dataset_path=\"/data/llm_datasets/custom/ados/dpo/ados_dpo_v2.json\",\n",
    "    data_format='qwen',\n",
    ")\n",
    "dpo_datamodule = dpo_dataset.make_dpo_data_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822481ba-65ba-41a6-bb2f-7efbacd2600a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': '질문은 다음과 같을 수 있습니다: \"필립 바드의 연구에 따르면 감각 정보와 운동 정보 외에 어떤 유형의 정보가 더 이상 처리되기 전에 중뇌를 통과해야 하는 정보에는 어떤 것이 있었습니까?\"<|im_end|>\\n',\n",
       " 'rejected': ' 이 글에서 제공된 정보에 따르면, 감각, 운동, 생리적 정보 모두 추가 처리를 받기 전에 감각계(특히 시상)를 통과해야 했습니다. 따라서 질문에 대한 정답은 다음과 같습니다:\\n생리적 정보.\\n\\n카논은 감각적 사건이 의식을 촉발하기 전에 생리적 반응을 유발할 수 있는 것은 해부학적으로 불가능하며, 감정적 자극은 생리적 반응과 경험적 측면의 감정을 동시에 유발해야 한다고 주장했기 때문입니다.<|im_end|>\\n',\n",
       " 'prompt': '<|im_start|>system\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|im_end|>\\n<|im_start|>user\\n이 기사에 대한 질문은 무엇인가요? 질문이 불가능한 경우 \"불가능\"이라고 말하세요.\\n\\n필립 바드는 동물 연구를 통해 이론에 기여했습니다. 바드는 감각, 운동 및 생리적 정보가 더 이상의 처리를 받기 전에 모두 시상(특히 시상)을 통과해야 한다는 것을 발견했습니다. 따라서 카논은 감각적 이벤트가 의식적 인식을 촉발하기 전에 생리적 반응을 유발할 수 없으며, 감정적 자극이 생리적 및 경험적 측면의 감정을 동시에 유발해야 한다고 주장했습니다.\\n\\n 감각 및 운동 정보와 함께 처리된 후 시상에는 어떤 정보가 통과해야 했나요?<|im_end|>\\n<|im_start|>assistant\\n'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_datamodule['train_dataset'][-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a00a9a7f-e892-4673-8706-775cce38d063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combined_dataset[70002]\n",
    "\n",
    "conv = get_conversation_template('chat-orca')\n",
    "conv.system_message = conv.tasks['system_instruct'].format(system=data['system'])\n",
    "conv.append_message(conv.roles[0], data['input'])\n",
    "conv.append_message(conv.roles[1], '')\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "conv.update_last_message(data['chosen'])\n",
    "chosen = conv.get_prompt()[len(prompt):]\n",
    "conv.update_last_message(data['rejected'])\n",
    "rejected = conv.get_prompt()[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6a0f6b-95bf-43d3-a986-c7ebab3d6fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/Platypus2-70B-instruct/\"\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/vicuna-13b-v1.5/\"\n",
    "# model_path = \"/data/llm_weights/tokenizers/Qwen-72B/\"\n",
    "# model_path = \"/data/llm_weights/custom_trained/MIE-70B-10000/\"\n",
    "model_path = \"/data/llm_weights/gptq/MoMo-70B-lora-1.8.4-DPO-GPTQ2/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    model_max_length=4096,\n",
    "    padding_side=\"right\",\n",
    "    # use_fast=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "# tokenizer.pad_token_id = tokenizer.eod_id #Qwen\n",
    "\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "import torch\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34be124-ec39-45f7-b9ed-fde12013fa44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\"/data/llm_datasets/custom/refined/sharegpt_V3_format_ko_selected_dedup2.json\"\n",
    "#\"/data/llm_datasets/custom/vicuna_format/gpt_evol_1.3k-vicuna.json\"\n",
    "raw_data = load_sft_dataset(\"/data/llm_datasets/custom/ados/sft/ados_msft_v4.json\")\n",
    "# raw_data = dedup_non_pair(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3948aa29-6a01-4ecc-a013-8789394f6a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sources = [raw_data[1]['conversations']]\n",
    "max_len = 4096\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "roles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\n",
    "\n",
    "im_start = tokenizer.im_start_id\n",
    "im_end = tokenizer.im_end_id\n",
    "nl_tokens = tokenizer('\\n').input_ids\n",
    "_system = tokenizer('system').input_ids + nl_tokens\n",
    "_user = tokenizer('user').input_ids + nl_tokens\n",
    "_assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "# Apply prompt templates\n",
    "input_ids, targets = [], []\n",
    "for i, source in enumerate(sources):\n",
    "    if roles[source[0][\"from\"]] != roles[\"human\"]:\n",
    "        source = source[1:]\n",
    "\n",
    "    input_id, target = [], []\n",
    "    system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n",
    "    input_id += system\n",
    "    target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n",
    "    assert len(input_id) == len(target)\n",
    "    for j, sentence in enumerate(source):\n",
    "        role = roles[sentence[\"from\"]]\n",
    "        _input_id = tokenizer(role).input_ids + nl_tokens + \\\n",
    "            tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n",
    "        input_id += _input_id\n",
    "        if role == '<|im_start|>user':\n",
    "            _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n",
    "        elif role == '<|im_start|>assistant':\n",
    "            _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n",
    "                _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        target += _target\n",
    "    assert len(input_id) == len(target)\n",
    "    input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "    target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "    input_ids.append(input_id[:max_len])\n",
    "    targets.append(target[:max_len])\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "targets = torch.tensor(targets, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39ef72b-bcdf-4a79-8a52-dc423b76535c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2024jan/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-22 16:39:08,955] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from fastchat.train.train import LazySupervisedDataset\n",
    "dataset = LazySupervisedDataset(raw_data, tokenizer, data_format=\"qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "975f2728-3c5e-400b-b225-f9720910a681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 16384\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "len_concat = 0\n",
    "concat_input_ids = []\n",
    "concat_labels = []\n",
    "for data in dataset:\n",
    "    input_ids = data['input_ids']\n",
    "    labels = data['labels']\n",
    "    attention_mask = data['attention_mask']\n",
    "\n",
    "    full_input_ids = input_ids[attention_mask==True]\n",
    "    full_labels = labels[attention_mask==True]\n",
    "\n",
    "    len_concat += len(full_input_ids)\n",
    "    if len_concat > max_len:\n",
    "        new_input_ids = torch.cat(concat_input_ids)\n",
    "        new_labels = torch.cat(concat_labels)\n",
    "        assert len(new_input_ids) == len(new_labels)\n",
    "        new_attention_mask = torch.ones(len(new_input_ids), dtype=torch.bool)\n",
    "        \n",
    "        new_input_ids = torch.cat((new_input_ids, torch.zeros(max_len - len(new_input_ids), dtype=input_ids.dtype)))\n",
    "        new_labels = torch.cat((new_labels, torch.full((max_len - len(new_labels),), IGNORE_TOKEN_ID, dtype=labels.dtype)))\n",
    "        new_attention_mask = torch.cat((new_attention_mask, torch.zeros(max_len - len(new_attention_mask), dtype=torch.bool)))\n",
    "        \n",
    "        new_dataset.append({\n",
    "            'input_ids': new_input_ids.tolist(),\n",
    "            'labels': new_labels.tolist(),\n",
    "            'attention_mask': new_attention_mask.tolist(),\n",
    "        })\n",
    "        \n",
    "        len_concat = len(full_input_ids)\n",
    "        concat_input_ids = []\n",
    "        concat_labels = []\n",
    "        \n",
    "    concat_input_ids.append(full_input_ids)\n",
    "    concat_labels.append(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed515c8c-c9d6-4d06-a1ff-ddcb60afd9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dataset[7] #new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9fcad16-3052-47c5-aae6-ab30078818bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\ncqrs와 이벤트 소싱을 활용하는 마이크로서비스 환경을 위한 아키텍처 스키마를 제공할 수 있나요? API 엔드포인트를 제공하고 rabbitmq를 통해 다른 백그라운드 서비스와 통신하는 등록 프로세스로 시작하세요.<|im_end|>\\n<|im_start|>assistant\\n명령 쿼리 책임 분리(CQRS) 및 이벤트 소싱(ES)을 사용하는 마이크로서비스 환경에는 몇 가지 주요 구성 요소가 있습니다. 다음은 RabbitMQ를 통해 백그라운드 서비스와 통신하는 API 엔드포인트와 함께 등록 프로세스를 포함하는 아키텍처 스키마입니다:\\n1. API 게이트웨이: 이 컴포넌트는 들어오는 HTTP 요청을 처리하고 적절한 마이크로서비스로 라우팅합니다. 이 경우 등록 요청을 등록 마이크로서비스로 라우팅합니다.\\n2. 등록 마이크로서비스: 이 컴포넌트는 등록 요청을 처리합니다. API 게이트웨이에서 요청을 수신하고 데이터를 확인한 다음 명령 버스에 명령을 보냅니다.\\n3. 명령 버스: 이 컴포넌트는 명령을 처리합니다. 이 경우 등록 명령을 Registration 마이크로서비스에서 수신하여 RabbitMQ를 통해 백그라운드 서비스로 보냅니다.\\n4. 백그라운드 서비스: 이 컴포넌트는 데이터베이스에서 새 사용자 생성을 비롯한 백그라운드 작업을 처리합니다. 이 컴포넌트는 명령 버스에서 등록 이벤트를 수신하여 이벤트 버스를 통해 등록 마이크로서비스로 라우팅합니다.\\n5. 이벤트 버스: 이 컴포넌트는 이벤트를 처리합니다. 이 경우 백그라운드 서비스에서 이벤트를 수신하여 RabbitMQ를 통해 등록 마이크로서비스로 보냅니다.\\n6. 등록 마이크로서비스: 이 컴포넌트는 이벤트 버스로부터 등록 이벤트를 수신하여 로그 모델을 업데이트합니다.\\n7. 데이터베이스: 이 컴포넌트는 시스템의 현재 상태를 저장합니다. 새 사용자가 생성되면 백그라운드 서비스에서 업데이트됩니다.\\n8. RabbitMQ: 이 컴포넌트는 컴포넌트 간에 메시지 전달을 담당합니다.\\n이는 일반적인 아키텍처 스키마이며 조직마다 디자인 패턴과 아키텍처가 다를 수 있습니다. 이것은 CQRS와 ES를 활용하는 마이크로서비스 환경을 구성하는 한 가지 가능한 방법일 뿐입니다.<|im_end|>\\n<|im_start|>user\\n그 답변에 두 가지 다른 등록 마이크로서비스가 있습니까, 아니면 동일합니까?<|im_end|>\\n<|im_start|>assistant\\n제가 제공한 아키텍처 스케치에는 등록 마이크로서비스가 하나만 있습니다. 이 마이크로서비스는 API 게이트웨이로부터 등록 요청을 수신하고 데이터를 확인한 다음 명령 버스에 명령을 보냅니다. 그런 다음 이벤트 버스에서 등록 이벤트를 수신하여 로컬 읽기 모델을 업데이트합니다.\\n첫 번째 단계에서는 등록 마이크로서비스가 들어오는 등록 요청을 확인하고 명령 버스에 명령을 보냅니다. 두 번째 단계에서는 이벤트 버스에서 등록 이벤트를 수신하고 로컬 읽기 모델을 업데이트합니다.\\n등록 요청과 이벤트 업데이트를 모두 처리하는 단일 서비스이지만 각 단계에서 다른 작업을 수행하고 있습니다.<|im_end|>\\n<|im_start|>user\\n교환에 바인딩하되 특정 이벤트만 처리할 수 있나요?<|im_end|>\\n<|im_start|>assistant\\n예, 큐를 교환에 바인딩할 수 있지만 라우팅 키를 사용하여 특정 이벤트를 처리할 수 있습니다. 라우팅 키는 교환이 메시지를 어떤 큐로 전달할지 결정하는 데 사용되는 메시지 속성입니다.\\n\\n큐를 교환에 바인딩할 때 라우팅 키를 지정할 수 있습니다. 라우팅 키가 일치하는 메시지만 해당 큐로 전달됩니다. 이를 통해 메시지를 필터링하고 특정 이벤트를 처리할 수 있습니다.\\n\\nRabbitMQ에서 메시지를 교환에 게시할 때 라우팅 키를 지정할 수 있습니다. 교환이 메시지를 수신하면 라우팅 키를 사용하여 메시지를 어떤 큐로 전달할지 결정합니다. 일치하는 라우팅 키를 가진 큐만 메시지를 수신합니다.\\n예를 들어, 등록 서비스가 \"user-registered\" 이벤트를 게시할 때 \"user.registered\"의 라우팅 키를 지정하고 이메일 서비스가 \"events\" 교환에 \"user.registered\"의 라우팅 키로 큐를 바인딩하면 해당 라우팅 키와 일치하는 이벤트만 수신할 수 있습니다.\\n다음은 RabbitMQ를 사용하여 C#으로 구현하는 방법의 예입니다:```\\nclass RegistrationService {\\n    private IModel _channel;\\n    public RegistrationService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n    }\\n    public void Register(string email, string password) {\\n        //...\\n        _channel.BasicPublish(\"events\", \"user.registered\", null, body);\\n    }\\n}\\n\\nclass EmailService {\\n    private IModel _channel;\\n    public EmailService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n        _channel.QueueDeclare(\"emails\", false, false, false, null);\\n        _channel.QueueBind(\"emails\", \"events\", \"user.registered\");\\n    }\\n    //...\\n}\\n```\\n이 예제에서 등록 서비스가 \"사용자 등록\" 이벤트를 게시하면 라우팅 키로 \"user.registered\"를 지정합니다. 그리고 이메일 서비스는 \"user.registered\"의 라우팅 키로 대기열을 \"events\" 교환에 바인딩합니다. 따라서 이메일 서비스는 라우팅 키가 \"user.registered\"인 이벤트만 수신합니다.\\n\\n이것은 예시일 뿐이며 실제 환경에서는 작동하지 않을 수 있으며, 단지 아이디어를 제공하기 위한 것임을 유의하시기 바랍니다. 또한 위의 코드 샘플은 구현 방법에 대한 대략적인 아이디어일 뿐이며 특정 요구 사항에 따라 달라질 수 있으므로 구현을 도와줄 전문 개발자와 상담해야 합니다.<|im_end|>\\n<|im_start|>user\\n이 예제에서는 이벤트 소싱은 어떻게 포함될까요?<|im_end|>\\n<|im_start|>assistant\\n이벤트 소싱은 모든 상태 변경 사항을 이벤트 시퀀스로 기록하여 시스템의 상태를 저장하고 관리하는 방법입니다. 이렇게 하면 이벤트의 재생을 통해 현재 상태를 다시 구성할 수 있습니다.\\n\\n이전 예제에서 등록 서비스는 이벤트를 게시하고 이메일 서비스는 이러한 이벤트를 구독합니다. 이벤트 소싱을 포함하려면 나중에 다시 재생할 수 있도록 모든 이벤트를 어딘가에 저장해야 하므로 모든 이벤트를 저장해야 합니다. 이를 위해 이벤트 소싱을 위한 전용 데이터베이스(예: 이벤트 저장소)를 사용하는 것이 일반적인 방법 중 하나입니다.\\n\\n다음은 C#의 이전 예제에서 이벤트 소싱을 포함하는 방법의 예시입니다:```\\nclass RegistrationService {\\n    private IModel _channel;\\n    private IEventStore _eventStore;\\n    public RegistrationService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n        _eventStore = new EventStore();\\n    }\\n    public void Register(string email, string password) {\\n        //...\\n        var userRegisteredEvent = new { Email = email };\\n        var json = JsonConvert.SerializeObject(userRegisteredEvent);\\n        var body = Encoding.UTF8.GetBytes(json);\\n        _channel.BasicPublish(\"events\", \"user.registered\", null, body);\\n        _eventStore.Save(\"user-registered\", json);\\n    }\\n}\\n\\nclass EmailService {\\n    private IModel _channel;\\n    private IEventStore _eventStore;\\n    public EmailService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n        _channel.QueueDeclare(\"emails\", false, false, false, null);\\n        _channel.QueueBind(\"emails\", \"events\", \"user.registered\");\\n        _eventStore = new EventStore();\\n    }\\n    public void Start() {\\n        var consumer = new EventingBasicConsumer(_channel);\\n        consumer.Received += (model, ea) => {\\n            var json = Encoding.UTF8.GetString(ea.Body);\\n            var userRegisteredEvent = JsonConvert.DeserializeObject<dynamic>(json);\\n            var email = userRegisteredEvent.Email;\\n            _eventStore.Save(\"user-registered\", json);\\n            // send welcome email to the user\\n            //...\\n        };\\n        _channel.BasicConsume(\"emails\", true, consumer);\\n    }\\n}\\n```\\n이 예제에서 등록 서비스와 이메일 서비스는 `IEventStore`의 인스턴스를 사용하여 수신하거나 게시하는 이벤트를 저장합니다. EventStore` 클래스는 이벤트 저장소 데이터베이스에 이벤트를 저장하는 클래스에서 구현되는 인터페이스입니다. EventStore` 클래스는 Event Store 또는 Apache Kafka와 같은 실제 이벤트 저장소 데이터베이스에 연결되는 클래스로 대체할 수 있습니다.\\n등록 서비스가 \"사용자 등록\" 이벤트를 게시하면 이벤트 유형 \"사용자 등록\"과 이벤트 데이터를 JSON 문자열로 사용하여 `Save` 메서드를 호출하여 이벤트 저장소에 해당 이벤트를 저장합니다. 이메일 서비스가 \"사용자 등록\" 이벤트를 수신할 때에도 동일한 작업이 수행되어 이벤트 저장소에 이벤트를 저장합니다.\\n이벤트 저장소에 저장된 이벤트를 재생하여 시스템의 현재 상태를 재구성할 수 있습니다. 또한 이벤트 저장소를 사용하여 시스템의 기록을 유지 관리하고 이벤트를 재생할 수 있습니다.<|im_end|>\\n<|im_start|>user\\n이 예제에서 rabbitmq를 카프로 대체할 수 있나요?<|im_end|>\\n<|im_start|>assistant\\n예, 이 예제에서는 RabbitMQ 대신 Apache Kafka를 사용할 수 있습니다.\\nKafka는 고처리량, 내결함성 및 확장 가능한 데이터 스트리밍을 위해 설계된 분산형 스트리밍 플랫폼입니다. 클라이언트는 주제를 게시하고 다른 클라이언트는 해당 주제를 구독하여 메시지를 수신할 수 있는 게시-구독 모델을 사용합니다.\\nRabbitMQ와 Kafka의 주요 차이점은 RabbitMQ는 메시지 브로커로, 생산자가 소비자에게 메시지를 라우팅하는 반면 Kafka는 스트리밍 플랫폼으로, 실시간 스트림을 저장하고 처리한다는 것입니다.\\n다음은 C#에서 Apache Kafka를 사용하여 이전 예제를 구현하는 방법의 예입니다:```\\nclass RegistrationService {\\n    private IProducer<string, string> _producer;\\n    private IEventStore _eventStore;\\n    public RegistrationService() {\\n        //...\\n        var config = new ProducerConfig { BootstrapServers = \"localhost:9092\" };\\n        _producer = new ProducerBuilder<string, string>(config).Build();\\n        _eventStore = new EventStore();\\n    }\\n    public void Register(string email, string password) {\\n        //...\\n        var userRegisteredEvent = new { Email = email };\\n        var json = JsonConvert.SerializeObject(userRegisteredEvent);\\n        _producer.ProduceAsync(\"events\", new Message<string, string> { Key = \"user.registered\", Value = json });\\n        _eventStore.Save(\"user-registered\", json);\\n    }\\n```<|im_end|>\\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72a4f04a-e8b8-4640-bd78-0d93be407fae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|>명령 쿼리 책임 분리(CQRS) 및 이벤트 소싱(ES)을 사용하는 마이크로서비스 환경에는 몇 가지 주요 구성 요소가 있습니다. 다음은 RabbitMQ를 통해 백그라운드 서비스와 통신하는 API 엔드포인트와 함께 등록 프로세스를 포함하는 아키텍처 스키마입니다:\\n1. API 게이트웨이: 이 컴포넌트는 들어오는 HTTP 요청을 처리하고 적절한 마이크로서비스로 라우팅합니다. 이 경우 등록 요청을 등록 마이크로서비스로 라우팅합니다.\\n2. 등록 마이크로서비스: 이 컴포넌트는 등록 요청을 처리합니다. API 게이트웨이에서 요청을 수신하고 데이터를 확인한 다음 명령 버스에 명령을 보냅니다.\\n3. 명령 버스: 이 컴포넌트는 명령을 처리합니다. 이 경우 등록 명령을 Registration 마이크로서비스에서 수신하여 RabbitMQ를 통해 백그라운드 서비스로 보냅니다.\\n4. 백그라운드 서비스: 이 컴포넌트는 데이터베이스에서 새 사용자 생성을 비롯한 백그라운드 작업을 처리합니다. 이 컴포넌트는 명령 버스에서 등록 이벤트를 수신하여 이벤트 버스를 통해 등록 마이크로서비스로 라우팅합니다.\\n5. 이벤트 버스: 이 컴포넌트는 이벤트를 처리합니다. 이 경우 백그라운드 서비스에서 이벤트를 수신하여 RabbitMQ를 통해 등록 마이크로서비스로 보냅니다.\\n6. 등록 마이크로서비스: 이 컴포넌트는 이벤트 버스로부터 등록 이벤트를 수신하여 로그 모델을 업데이트합니다.\\n7. 데이터베이스: 이 컴포넌트는 시스템의 현재 상태를 저장합니다. 새 사용자가 생성되면 백그라운드 서비스에서 업데이트됩니다.\\n8. RabbitMQ: 이 컴포넌트는 컴포넌트 간에 메시지 전달을 담당합니다.\\n이는 일반적인 아키텍처 스키마이며 조직마다 디자인 패턴과 아키텍처가 다를 수 있습니다. 이것은 CQRS와 ES를 활용하는 마이크로서비스 환경을 구성하는 한 가지 가능한 방법일 뿐입니다.<|im_end|>\\n<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|>제가 제공한 아키텍처 스케치에는 등록 마이크로서비스가 하나만 있습니다. 이 마이크로서비스는 API 게이트웨이로부터 등록 요청을 수신하고 데이터를 확인한 다음 명령 버스에 명령을 보냅니다. 그런 다음 이벤트 버스에서 등록 이벤트를 수신하여 로컬 읽기 모델을 업데이트합니다.\\n첫 번째 단계에서는 등록 마이크로서비스가 들어오는 등록 요청을 확인하고 명령 버스에 명령을 보냅니다. 두 번째 단계에서는 이벤트 버스에서 등록 이벤트를 수신하고 로컬 읽기 모델을 업데이트합니다.\\n등록 요청과 이벤트 업데이트를 모두 처리하는 단일 서비스이지만 각 단계에서 다른 작업을 수행하고 있습니다.<|im_end|>\\n<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|>예, 큐를 교환에 바인딩할 수 있지만 라우팅 키를 사용하여 특정 이벤트를 처리할 수 있습니다. 라우팅 키는 교환이 메시지를 어떤 큐로 전달할지 결정하는 데 사용되는 메시지 속성입니다.\\n\\n큐를 교환에 바인딩할 때 라우팅 키를 지정할 수 있습니다. 라우팅 키가 일치하는 메시지만 해당 큐로 전달됩니다. 이를 통해 메시지를 필터링하고 특정 이벤트를 처리할 수 있습니다.\\n\\nRabbitMQ에서 메시지를 교환에 게시할 때 라우팅 키를 지정할 수 있습니다. 교환이 메시지를 수신하면 라우팅 키를 사용하여 메시지를 어떤 큐로 전달할지 결정합니다. 일치하는 라우팅 키를 가진 큐만 메시지를 수신합니다.\\n예를 들어, 등록 서비스가 \"user-registered\" 이벤트를 게시할 때 \"user.registered\"의 라우팅 키를 지정하고 이메일 서비스가 \"events\" 교환에 \"user.registered\"의 라우팅 키로 큐를 바인딩하면 해당 라우팅 키와 일치하는 이벤트만 수신할 수 있습니다.\\n다음은 RabbitMQ를 사용하여 C#으로 구현하는 방법의 예입니다:```\\nclass RegistrationService {\\n    private IModel _channel;\\n    public RegistrationService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n    }\\n    public void Register(string email, string password) {\\n        //...\\n        _channel.BasicPublish(\"events\", \"user.registered\", null, body);\\n    }\\n}\\n\\nclass EmailService {\\n    private IModel _channel;\\n    public EmailService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n        _channel.QueueDeclare(\"emails\", false, false, false, null);\\n        _channel.QueueBind(\"emails\", \"events\", \"user.registered\");\\n    }\\n    //...\\n}\\n```\\n이 예제에서 등록 서비스가 \"사용자 등록\" 이벤트를 게시하면 라우팅 키로 \"user.registered\"를 지정합니다. 그리고 이메일 서비스는 \"user.registered\"의 라우팅 키로 대기열을 \"events\" 교환에 바인딩합니다. 따라서 이메일 서비스는 라우팅 키가 \"user.registered\"인 이벤트만 수신합니다.\\n\\n이것은 예시일 뿐이며 실제 환경에서는 작동하지 않을 수 있으며, 단지 아이디어를 제공하기 위한 것임을 유의하시기 바랍니다. 또한 위의 코드 샘플은 구현 방법에 대한 대략적인 아이디어일 뿐이며 특정 요구 사항에 따라 달라질 수 있으므로 구현을 도와줄 전문 개발자와 상담해야 합니다.<|im_end|>\\n<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|>이벤트 소싱은 모든 상태 변경 사항을 이벤트 시퀀스로 기록하여 시스템의 상태를 저장하고 관리하는 방법입니다. 이렇게 하면 이벤트의 재생을 통해 현재 상태를 다시 구성할 수 있습니다.\\n\\n이전 예제에서 등록 서비스는 이벤트를 게시하고 이메일 서비스는 이러한 이벤트를 구독합니다. 이벤트 소싱을 포함하려면 나중에 다시 재생할 수 있도록 모든 이벤트를 어딘가에 저장해야 하므로 모든 이벤트를 저장해야 합니다. 이를 위해 이벤트 소싱을 위한 전용 데이터베이스(예: 이벤트 저장소)를 사용하는 것이 일반적인 방법 중 하나입니다.\\n\\n다음은 C#의 이전 예제에서 이벤트 소싱을 포함하는 방법의 예시입니다:```\\nclass RegistrationService {\\n    private IModel _channel;\\n    private IEventStore _eventStore;\\n    public RegistrationService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n        _eventStore = new EventStore();\\n    }\\n    public void Register(string email, string password) {\\n        //...\\n        var userRegisteredEvent = new { Email = email };\\n        var json = JsonConvert.SerializeObject(userRegisteredEvent);\\n        var body = Encoding.UTF8.GetBytes(json);\\n        _channel.BasicPublish(\"events\", \"user.registered\", null, body);\\n        _eventStore.Save(\"user-registered\", json);\\n    }\\n}\\n\\nclass EmailService {\\n    private IModel _channel;\\n    private IEventStore _eventStore;\\n    public EmailService() {\\n        //...\\n        _channel.ExchangeDeclare(\"events\", ExchangeType.Fanout);\\n        _channel.QueueDeclare(\"emails\", false, false, false, null);\\n        _channel.QueueBind(\"emails\", \"events\", \"user.registered\");\\n        _eventStore = new EventStore();\\n    }\\n    public void Start() {\\n        var consumer = new EventingBasicConsumer(_channel);\\n        consumer.Received += (model, ea) => {\\n            var json = Encoding.UTF8.GetString(ea.Body);\\n            var userRegisteredEvent = JsonConvert.DeserializeObject<dynamic>(json);\\n            var email = userRegisteredEvent.Email;\\n            _eventStore.Save(\"user-registered\", json);\\n            // send welcome email to the user\\n            //...\\n        };\\n        _channel.BasicConsume(\"emails\", true, consumer);\\n    }\\n}\\n```\\n이 예제에서 등록 서비스와 이메일 서비스는 `IEventStore`의 인스턴스를 사용하여 수신하거나 게시하는 이벤트를 저장합니다. EventStore` 클래스는 이벤트 저장소 데이터베이스에 이벤트를 저장하는 클래스에서 구현되는 인터페이스입니다. EventStore` 클래스는 Event Store 또는 Apache Kafka와 같은 실제 이벤트 저장소 데이터베이스에 연결되는 클래스로 대체할 수 있습니다.\\n등록 서비스가 \"사용자 등록\" 이벤트를 게시하면 이벤트 유형 \"사용자 등록\"과 이벤트 데이터를 JSON 문자열로 사용하여 `Save` 메서드를 호출하여 이벤트 저장소에 해당 이벤트를 저장합니다. 이메일 서비스가 \"사용자 등록\" 이벤트를 수신할 때에도 동일한 작업이 수행되어 이벤트 저장소에 이벤트를 저장합니다.\\n이벤트 저장소에 저장된 이벤트를 재생하여 시스템의 현재 상태를 재구성할 수 있습니다. 또한 이벤트 저장소를 사용하여 시스템의 기록을 유지 관리하고 이벤트를 재생할 수 있습니다.<|im_end|>\\n<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|>예, 이 예제에서는 RabbitMQ 대신 Apache Kafka를 사용할 수 있습니다.\\nKafka는 고처리량, 내결함성 및 확장 가능한 데이터 스트리밍을 위해 설계된 분산형 스트리밍 플랫폼입니다. 클라이언트는 주제를 게시하고 다른 클라이언트는 해당 주제를 구독하여 메시지를 수신할 수 있는 게시-구독 모델을 사용합니다.\\nRabbitMQ와 Kafka의 주요 차이점은 RabbitMQ는 메시지 브로커로, 생산자가 소비자에게 메시지를 라우팅하는 반면 Kafka는 스트리밍 플랫폼으로, 실시간 스트림을 저장하고 처리한다는 것입니다.\\n다음은 C#에서 Apache Kafka를 사용하여 이전 예제를 구현하는 방법의 예입니다:```\\nclass RegistrationService {\\n    private IProducer<string, string> _producer;\\n    private IEventStore _eventStore;\\n    public RegistrationService() {\\n        //...\\n        var config = new ProducerConfig { BootstrapServers = \"localhost:9092\" };\\n        _producer = new ProducerBuilder<string, string>(config).Build();\\n        _eventStore = new EventStore();\\n    }\\n    public void Register(string email, string password) {\\n        //...\\n        var userRegisteredEvent = new { Email = email };\\n        var json = JsonConvert.SerializeObject(userRegisteredEvent);\\n        _producer.ProduceAsync(\"events\", new Message<string, string> { Key = \"user.registered\", Value = json });\\n        _eventStore.Save(\"user-registered\", json);\\n    }\\n```<|im_end|>\\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.where(data['labels'] == IGNORE_TOKEN_ID, tokenizer.unk_token_id, data['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dba556d4-c3a5-4676-8fe5-3127f6827b24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151643]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4fa31460-e0de-46fb-8963-47b84d9d5a92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='/data/llm_weights/custom_trained/MIE-70B-10000/', vocab_size=32000, model_max_length=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MIE\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f353d15f-38e3-4a6b-8686-cf065991baa1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='/data/llm_weights/gptq/PIE-72B-45000-GPTQ/', vocab_size=151851, model_max_length=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|extra_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|extra_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|extra_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|extra_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|extra_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|extra_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|extra_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|extra_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|extra_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|extra_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|extra_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<|extra_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151658: AddedToken(\"<|extra_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151659: AddedToken(\"<|extra_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151660: AddedToken(\"<|extra_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151661: AddedToken(\"<|extra_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151662: AddedToken(\"<|extra_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151663: AddedToken(\"<|extra_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151664: AddedToken(\"<|extra_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151665: AddedToken(\"<|extra_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151666: AddedToken(\"<|extra_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151667: AddedToken(\"<|extra_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151668: AddedToken(\"<|extra_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151669: AddedToken(\"<|extra_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151670: AddedToken(\"<|extra_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151671: AddedToken(\"<|extra_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151672: AddedToken(\"<|extra_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151673: AddedToken(\"<|extra_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151674: AddedToken(\"<|extra_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151675: AddedToken(\"<|extra_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151676: AddedToken(\"<|extra_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151677: AddedToken(\"<|extra_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151678: AddedToken(\"<|extra_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151679: AddedToken(\"<|extra_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151680: AddedToken(\"<|extra_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151681: AddedToken(\"<|extra_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151682: AddedToken(\"<|extra_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151683: AddedToken(\"<|extra_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151684: AddedToken(\"<|extra_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151685: AddedToken(\"<|extra_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151686: AddedToken(\"<|extra_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151687: AddedToken(\"<|extra_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151688: AddedToken(\"<|extra_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151689: AddedToken(\"<|extra_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151690: AddedToken(\"<|extra_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151691: AddedToken(\"<|extra_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151692: AddedToken(\"<|extra_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151693: AddedToken(\"<|extra_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151694: AddedToken(\"<|extra_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151695: AddedToken(\"<|extra_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151696: AddedToken(\"<|extra_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151697: AddedToken(\"<|extra_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151698: AddedToken(\"<|extra_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151699: AddedToken(\"<|extra_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151700: AddedToken(\"<|extra_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151701: AddedToken(\"<|extra_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151702: AddedToken(\"<|extra_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151703: AddedToken(\"<|extra_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151704: AddedToken(\"<|extra_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151705: AddedToken(\"<|extra_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151706: AddedToken(\"<|extra_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151707: AddedToken(\"<|extra_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151708: AddedToken(\"<|extra_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151709: AddedToken(\"<|extra_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151710: AddedToken(\"<|extra_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151711: AddedToken(\"<|extra_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151712: AddedToken(\"<|extra_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151713: AddedToken(\"<|extra_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151714: AddedToken(\"<|extra_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151715: AddedToken(\"<|extra_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151716: AddedToken(\"<|extra_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151717: AddedToken(\"<|extra_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151718: AddedToken(\"<|extra_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151719: AddedToken(\"<|extra_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151720: AddedToken(\"<|extra_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151721: AddedToken(\"<|extra_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151722: AddedToken(\"<|extra_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151723: AddedToken(\"<|extra_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151724: AddedToken(\"<|extra_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151725: AddedToken(\"<|extra_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151726: AddedToken(\"<|extra_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151727: AddedToken(\"<|extra_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151728: AddedToken(\"<|extra_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151729: AddedToken(\"<|extra_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151730: AddedToken(\"<|extra_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151731: AddedToken(\"<|extra_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151732: AddedToken(\"<|extra_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151733: AddedToken(\"<|extra_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151734: AddedToken(\"<|extra_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151735: AddedToken(\"<|extra_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151736: AddedToken(\"<|extra_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151737: AddedToken(\"<|extra_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151738: AddedToken(\"<|extra_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151739: AddedToken(\"<|extra_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151740: AddedToken(\"<|extra_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151741: AddedToken(\"<|extra_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151742: AddedToken(\"<|extra_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151743: AddedToken(\"<|extra_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151744: AddedToken(\"<|extra_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151745: AddedToken(\"<|extra_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151746: AddedToken(\"<|extra_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151747: AddedToken(\"<|extra_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151748: AddedToken(\"<|extra_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151749: AddedToken(\"<|extra_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151750: AddedToken(\"<|extra_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151751: AddedToken(\"<|extra_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151752: AddedToken(\"<|extra_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151753: AddedToken(\"<|extra_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151754: AddedToken(\"<|extra_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151755: AddedToken(\"<|extra_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151756: AddedToken(\"<|extra_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151757: AddedToken(\"<|extra_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151758: AddedToken(\"<|extra_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151759: AddedToken(\"<|extra_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151760: AddedToken(\"<|extra_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151761: AddedToken(\"<|extra_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151762: AddedToken(\"<|extra_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151763: AddedToken(\"<|extra_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151764: AddedToken(\"<|extra_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151765: AddedToken(\"<|extra_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151766: AddedToken(\"<|extra_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151767: AddedToken(\"<|extra_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151768: AddedToken(\"<|extra_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151769: AddedToken(\"<|extra_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151770: AddedToken(\"<|extra_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151771: AddedToken(\"<|extra_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151772: AddedToken(\"<|extra_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151773: AddedToken(\"<|extra_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151774: AddedToken(\"<|extra_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151775: AddedToken(\"<|extra_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151776: AddedToken(\"<|extra_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151777: AddedToken(\"<|extra_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151778: AddedToken(\"<|extra_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151779: AddedToken(\"<|extra_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151780: AddedToken(\"<|extra_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151781: AddedToken(\"<|extra_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151782: AddedToken(\"<|extra_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151783: AddedToken(\"<|extra_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151784: AddedToken(\"<|extra_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151785: AddedToken(\"<|extra_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151786: AddedToken(\"<|extra_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151787: AddedToken(\"<|extra_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151788: AddedToken(\"<|extra_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151789: AddedToken(\"<|extra_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151790: AddedToken(\"<|extra_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151791: AddedToken(\"<|extra_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151792: AddedToken(\"<|extra_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151793: AddedToken(\"<|extra_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151794: AddedToken(\"<|extra_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151795: AddedToken(\"<|extra_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151796: AddedToken(\"<|extra_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151797: AddedToken(\"<|extra_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151798: AddedToken(\"<|extra_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151799: AddedToken(\"<|extra_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151800: AddedToken(\"<|extra_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151801: AddedToken(\"<|extra_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151802: AddedToken(\"<|extra_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151803: AddedToken(\"<|extra_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151804: AddedToken(\"<|extra_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151805: AddedToken(\"<|extra_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151806: AddedToken(\"<|extra_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151807: AddedToken(\"<|extra_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151808: AddedToken(\"<|extra_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151809: AddedToken(\"<|extra_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151810: AddedToken(\"<|extra_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151811: AddedToken(\"<|extra_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151812: AddedToken(\"<|extra_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151813: AddedToken(\"<|extra_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151814: AddedToken(\"<|extra_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151815: AddedToken(\"<|extra_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151816: AddedToken(\"<|extra_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151817: AddedToken(\"<|extra_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151818: AddedToken(\"<|extra_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151819: AddedToken(\"<|extra_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151820: AddedToken(\"<|extra_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151821: AddedToken(\"<|extra_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151822: AddedToken(\"<|extra_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151823: AddedToken(\"<|extra_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151824: AddedToken(\"<|extra_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151825: AddedToken(\"<|extra_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151826: AddedToken(\"<|extra_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151827: AddedToken(\"<|extra_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151828: AddedToken(\"<|extra_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151829: AddedToken(\"<|extra_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151830: AddedToken(\"<|extra_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151831: AddedToken(\"<|extra_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151832: AddedToken(\"<|extra_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151833: AddedToken(\"<|extra_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151834: AddedToken(\"<|extra_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151835: AddedToken(\"<|extra_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151836: AddedToken(\"<|extra_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151837: AddedToken(\"<|extra_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151838: AddedToken(\"<|extra_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151839: AddedToken(\"<|extra_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151840: AddedToken(\"<|extra_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151841: AddedToken(\"<|extra_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151842: AddedToken(\"<|extra_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151843: AddedToken(\"<|extra_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151844: AddedToken(\"<|extra_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151845: AddedToken(\"<|extra_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151846: AddedToken(\"<|extra_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151847: AddedToken(\"<|extra_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151848: AddedToken(\"<|extra_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151849: AddedToken(\"<|extra_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151850: AddedToken(\"<|extra_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PIE from file\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea202fe2-1a01-450b-98ea-ecba3e8fff9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenTokenizer(name_or_path='/data/llm_weights/tokenizers/Qwen-72B/', vocab_size=151851, model_max_length=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#QWen remote\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e70997-0f0f-4c28-b77e-382595c40086",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='/data/llm_weights/gptq/MoMo-70B-lora-1.8.4-DPO-GPTQ2/', vocab_size=151851, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|extra_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|extra_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|extra_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|extra_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|extra_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|extra_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|extra_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|extra_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|extra_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|extra_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|extra_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<|extra_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151658: AddedToken(\"<|extra_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151659: AddedToken(\"<|extra_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151660: AddedToken(\"<|extra_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151661: AddedToken(\"<|extra_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151662: AddedToken(\"<|extra_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151663: AddedToken(\"<|extra_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151664: AddedToken(\"<|extra_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151665: AddedToken(\"<|extra_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151666: AddedToken(\"<|extra_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151667: AddedToken(\"<|extra_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151668: AddedToken(\"<|extra_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151669: AddedToken(\"<|extra_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151670: AddedToken(\"<|extra_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151671: AddedToken(\"<|extra_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151672: AddedToken(\"<|extra_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151673: AddedToken(\"<|extra_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151674: AddedToken(\"<|extra_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151675: AddedToken(\"<|extra_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151676: AddedToken(\"<|extra_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151677: AddedToken(\"<|extra_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151678: AddedToken(\"<|extra_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151679: AddedToken(\"<|extra_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151680: AddedToken(\"<|extra_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151681: AddedToken(\"<|extra_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151682: AddedToken(\"<|extra_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151683: AddedToken(\"<|extra_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151684: AddedToken(\"<|extra_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151685: AddedToken(\"<|extra_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151686: AddedToken(\"<|extra_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151687: AddedToken(\"<|extra_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151688: AddedToken(\"<|extra_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151689: AddedToken(\"<|extra_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151690: AddedToken(\"<|extra_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151691: AddedToken(\"<|extra_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151692: AddedToken(\"<|extra_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151693: AddedToken(\"<|extra_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151694: AddedToken(\"<|extra_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151695: AddedToken(\"<|extra_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151696: AddedToken(\"<|extra_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151697: AddedToken(\"<|extra_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151698: AddedToken(\"<|extra_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151699: AddedToken(\"<|extra_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151700: AddedToken(\"<|extra_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151701: AddedToken(\"<|extra_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151702: AddedToken(\"<|extra_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151703: AddedToken(\"<|extra_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151704: AddedToken(\"<|extra_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151705: AddedToken(\"<|extra_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151706: AddedToken(\"<|extra_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151707: AddedToken(\"<|extra_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151708: AddedToken(\"<|extra_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151709: AddedToken(\"<|extra_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151710: AddedToken(\"<|extra_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151711: AddedToken(\"<|extra_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151712: AddedToken(\"<|extra_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151713: AddedToken(\"<|extra_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151714: AddedToken(\"<|extra_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151715: AddedToken(\"<|extra_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151716: AddedToken(\"<|extra_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151717: AddedToken(\"<|extra_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151718: AddedToken(\"<|extra_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151719: AddedToken(\"<|extra_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151720: AddedToken(\"<|extra_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151721: AddedToken(\"<|extra_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151722: AddedToken(\"<|extra_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151723: AddedToken(\"<|extra_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151724: AddedToken(\"<|extra_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151725: AddedToken(\"<|extra_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151726: AddedToken(\"<|extra_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151727: AddedToken(\"<|extra_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151728: AddedToken(\"<|extra_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151729: AddedToken(\"<|extra_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151730: AddedToken(\"<|extra_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151731: AddedToken(\"<|extra_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151732: AddedToken(\"<|extra_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151733: AddedToken(\"<|extra_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151734: AddedToken(\"<|extra_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151735: AddedToken(\"<|extra_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151736: AddedToken(\"<|extra_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151737: AddedToken(\"<|extra_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151738: AddedToken(\"<|extra_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151739: AddedToken(\"<|extra_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151740: AddedToken(\"<|extra_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151741: AddedToken(\"<|extra_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151742: AddedToken(\"<|extra_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151743: AddedToken(\"<|extra_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151744: AddedToken(\"<|extra_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151745: AddedToken(\"<|extra_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151746: AddedToken(\"<|extra_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151747: AddedToken(\"<|extra_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151748: AddedToken(\"<|extra_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151749: AddedToken(\"<|extra_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151750: AddedToken(\"<|extra_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151751: AddedToken(\"<|extra_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151752: AddedToken(\"<|extra_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151753: AddedToken(\"<|extra_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151754: AddedToken(\"<|extra_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151755: AddedToken(\"<|extra_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151756: AddedToken(\"<|extra_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151757: AddedToken(\"<|extra_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151758: AddedToken(\"<|extra_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151759: AddedToken(\"<|extra_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151760: AddedToken(\"<|extra_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151761: AddedToken(\"<|extra_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151762: AddedToken(\"<|extra_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151763: AddedToken(\"<|extra_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151764: AddedToken(\"<|extra_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151765: AddedToken(\"<|extra_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151766: AddedToken(\"<|extra_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151767: AddedToken(\"<|extra_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151768: AddedToken(\"<|extra_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151769: AddedToken(\"<|extra_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151770: AddedToken(\"<|extra_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151771: AddedToken(\"<|extra_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151772: AddedToken(\"<|extra_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151773: AddedToken(\"<|extra_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151774: AddedToken(\"<|extra_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151775: AddedToken(\"<|extra_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151776: AddedToken(\"<|extra_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151777: AddedToken(\"<|extra_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151778: AddedToken(\"<|extra_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151779: AddedToken(\"<|extra_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151780: AddedToken(\"<|extra_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151781: AddedToken(\"<|extra_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151782: AddedToken(\"<|extra_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151783: AddedToken(\"<|extra_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151784: AddedToken(\"<|extra_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151785: AddedToken(\"<|extra_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151786: AddedToken(\"<|extra_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151787: AddedToken(\"<|extra_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151788: AddedToken(\"<|extra_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151789: AddedToken(\"<|extra_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151790: AddedToken(\"<|extra_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151791: AddedToken(\"<|extra_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151792: AddedToken(\"<|extra_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151793: AddedToken(\"<|extra_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151794: AddedToken(\"<|extra_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151795: AddedToken(\"<|extra_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151796: AddedToken(\"<|extra_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151797: AddedToken(\"<|extra_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151798: AddedToken(\"<|extra_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151799: AddedToken(\"<|extra_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151800: AddedToken(\"<|extra_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151801: AddedToken(\"<|extra_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151802: AddedToken(\"<|extra_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151803: AddedToken(\"<|extra_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151804: AddedToken(\"<|extra_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151805: AddedToken(\"<|extra_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151806: AddedToken(\"<|extra_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151807: AddedToken(\"<|extra_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151808: AddedToken(\"<|extra_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151809: AddedToken(\"<|extra_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151810: AddedToken(\"<|extra_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151811: AddedToken(\"<|extra_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151812: AddedToken(\"<|extra_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151813: AddedToken(\"<|extra_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151814: AddedToken(\"<|extra_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151815: AddedToken(\"<|extra_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151816: AddedToken(\"<|extra_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151817: AddedToken(\"<|extra_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151818: AddedToken(\"<|extra_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151819: AddedToken(\"<|extra_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151820: AddedToken(\"<|extra_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151821: AddedToken(\"<|extra_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151822: AddedToken(\"<|extra_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151823: AddedToken(\"<|extra_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151824: AddedToken(\"<|extra_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151825: AddedToken(\"<|extra_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151826: AddedToken(\"<|extra_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151827: AddedToken(\"<|extra_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151828: AddedToken(\"<|extra_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151829: AddedToken(\"<|extra_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151830: AddedToken(\"<|extra_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151831: AddedToken(\"<|extra_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151832: AddedToken(\"<|extra_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151833: AddedToken(\"<|extra_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151834: AddedToken(\"<|extra_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151835: AddedToken(\"<|extra_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151836: AddedToken(\"<|extra_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151837: AddedToken(\"<|extra_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151838: AddedToken(\"<|extra_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151839: AddedToken(\"<|extra_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151840: AddedToken(\"<|extra_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151841: AddedToken(\"<|extra_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151842: AddedToken(\"<|extra_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151843: AddedToken(\"<|extra_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151844: AddedToken(\"<|extra_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151845: AddedToken(\"<|extra_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151846: AddedToken(\"<|extra_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151847: AddedToken(\"<|extra_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151848: AddedToken(\"<|extra_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151849: AddedToken(\"<|extra_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151850: AddedToken(\"<|extra_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PIE fast\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ee51f42-694c-4948-9e66-baca27115dc8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk><s></s>'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dd52cbd-95a6-4814-a37d-1ee70e714be2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n상황에 대해 설명하여라. \"자연어 처리\"라는 용어의 의미는 무엇인가?\\n맥락: 자연어 처리는 고객의 요구를 이해하는 데 중요한 도구다.<|im_end|>\\n<|im_start|>assistant\\n\"자연어 처리\" (Natural Language Processing, NLP)는 컴퓨터가 인간의 언어, 즉 자연어를 해석하고 이해할 수 있도록 만드는 기술 분야입니다. 이 분야는 컴퓨터 과학, 인공지능, 언어학의 교차점에 위치하며, 인간과 컴퓨터가 더욱 효율적으로 상호작용할 수 있도록 하는 데 목적이 있습니다.\\n\\n자연어 처리 기술은 다음과 같은 다양한 응용 분야에서 활용됩니다:\\n\\n1. 기계 번역: 예를 들어, 구글 번역 같은 서비스는 서로 다른 언어 간의 번역을 자동화하는 데 NLP를 사용합니다.\\n2. 음성 인식: 스마트폰의 개인 비서 (Siri, Google Assistant 등)는 사용자의 음성을 텍스트로 변환하는 데 NLP를 사용합니다.\\n3. 텍스트 분석: 소셜 미디어, 고객 리뷰, 온라인 포럼 등의 콘텐츠를 통해 여론을 분석하거나 감정을 판단하는 데 사용됩니다.\\n4. 자동 요약: 문서나 뉴스기사를 자동으로 요약하는 기술에 NLP가 쓰입니다.\\n5. 챗봇과 가상 상담원: 온라인 고객 서비스를 제공하는 자동화된 시스템들이 질문에 대한 응답을 생성하기 위해 NLP를 사용합니다.\\n\\n특히, 고객의 요구를 이해하는 데 있어서, NLP는 고객이 제공하는 텍스트나 음성 데이터를 분석하여 그들의 의도와 필요를 파악하는 데 중요한 도구입니다. 예를 들어, 챗봇은 NLP를 활용하여 고객의 질문을 이해하고 적절한 답변을 제공하거나, 고객 지원 요청을 올바른 부서로 라우팅하는 데 사용됩니다.\\n\\n종합하자면, 자연어 처리는 컴퓨터가 인간의 자연스러운 소통 방식을 이해하게 하여, 직관적이고 효율적인 인간-기계 상호작용을 가능케 하는 핵심 기술 중 하나입니다.<|im_end|>\\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b7c6961-5347-4db1-98e8-9c320f93b1bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_end|>\\n<|im_start|><|endoftext|><|endoftext|>\"자연어 처리\" (Natural Language Processing, NLP)는 컴퓨터가 인간의 언어, 즉 자연어를 해석하고 이해할 수 있도록 만드는 기술 분야입니다. 이 분야는 컴퓨터 과학, 인공지능, 언어학의 교차점에 위치하며, 인간과 컴퓨터가 더욱 효율적으로 상호작용할 수 있도록 하는 데 목적이 있습니다.\\n\\n자연어 처리 기술은 다음과 같은 다양한 응용 분야에서 활용됩니다:\\n\\n1. 기계 번역: 예를 들어, 구글 번역 같은 서비스는 서로 다른 언어 간의 번역을 자동화하는 데 NLP를 사용합니다.\\n2. 음성 인식: 스마트폰의 개인 비서 (Siri, Google Assistant 등)는 사용자의 음성을 텍스트로 변환하는 데 NLP를 사용합니다.\\n3. 텍스트 분석: 소셜 미디어, 고객 리뷰, 온라인 포럼 등의 콘텐츠를 통해 여론을 분석하거나 감정을 판단하는 데 사용됩니다.\\n4. 자동 요약: 문서나 뉴스기사를 자동으로 요약하는 기술에 NLP가 쓰입니다.\\n5. 챗봇과 가상 상담원: 온라인 고객 서비스를 제공하는 자동화된 시스템들이 질문에 대한 응답을 생성하기 위해 NLP를 사용합니다.\\n\\n특히, 고객의 요구를 이해하는 데 있어서, NLP는 고객이 제공하는 텍스트나 음성 데이터를 분석하여 그들의 의도와 필요를 파악하는 데 중요한 도구입니다. 예를 들어, 챗봇은 NLP를 활용하여 고객의 질문을 이해하고 적절한 답변을 제공하거나, 고객 지원 요청을 올바른 부서로 라우팅하는 데 사용됩니다.\\n\\n종합하자면, 자연어 처리는 컴퓨터가 인간의 자연스러운 소통 방식을 이해하게 하여, 직관적이고 효율적인 인간-기계 상호작용을 가능케 하는 핵심 기술 중 하나입니다.<|im_end|>\\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.where(targets[0] == IGNORE_TOKEN_ID, tokenizer.pad_token_id, targets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "06822c87-7faa-4891-a66f-b5228f7d1683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6be05e16-46c9-450e-a74b-5d2cd62cfd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 95520.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def random_select(dataset, num_select):\n",
    "    return dataset.select(np.random.choice(list(range(len(dataset))), num_select))\n",
    "dataset = load_dpo_dataset(\"/data/llm_datasets/yarn-train-tokenized-16k-mistral/data/\")\n",
    "dataset = random_select(dataset, 200)\n",
    "new_dataset = []\n",
    "for _data in dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8750b001-b3cf-42ac-a113-5be3df438b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/yarn-train-tokenized-16k-mistral-200.json\")\n",
    "dataset2 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-gpt4_evol.json\")\n",
    "dataset3 = load_dpo_dataset(\"/data/llm_datasets/custom/ados/yarn/ko-yarn-mistral-16k-sharegpt_V3_format_ko.json\")\n",
    "dataset3 = random_select(dataset3, 1000 - len(dataset1) - len(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce38e5c9-9403-4457-a57d-b65219a7e7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 699/699 [01:09<00:00, 10.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Features, Sequence, Value\n",
    "\n",
    "dataset2 = dataset2.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")\n",
    "dataset3 = dataset3.cast(\n",
    "    Features({\n",
    "        'attention_mask': Sequence(Value('int64')),\n",
    "        'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b2e13f4-6e09-47ac-83c0-59b2cde35351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = concatenate_datasets((dataset1, dataset2, dataset3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31316e3d-198c-453b-8b80-4518365db4af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conbined_dataset = conbined_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bf20792c-0630-4fd4-b612-e85edc12c27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for _data in conbined_dataset:\n",
    "    new_dataset.append(_data)\n",
    "    \n",
    "with open('/data/llm_datasets/custom/ados/yarn/ko-yarn-v1.json', \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb953e-239d-40e3-93bf-b4adb1dbb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(data['input_ids'])\n",
    "sentences = text.split('.')\n",
    "\n",
    "len_split = 1024\n",
    "splits = []\n",
    "split = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(split) < len_split:\n",
    "        split += sentence + \".\"\n",
    "    else:\n",
    "        splits.append(split)\n",
    "        split = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5e3acd40-c606-491f-974d-07de8078ee7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03시 15분 30초은 11730초와 같습니다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_seconds(time_string):\n",
    "    # 시간 문자열을 분리하여 시, 분, 초를 추출한다.\n",
    "    match = re.match(r'(\\d+)시 (\\d+)분 (\\d+)초', time_string)\n",
    "    if match is None:\n",
    "        raise ValueError(\"잘못된 시간 형식입니다.\")\n",
    "    hour = int(match.group(1))\n",
    "    minute = int(match.group(2))\n",
    "    second = int(match.group(3))\n",
    "\n",
    "    # 시간, 분, 초를 합산하여 총 초 단위 숫자를 계산한다.\n",
    "    total_seconds = hour * 3600 + minute * 60 + second\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "# 예시 사용법\n",
    "time_string = \"03시 15분 30초\"\n",
    "seconds = convert_to_seconds(time_string)\n",
    "print(f\"{time_string}은 {seconds}초와 같습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "62c4302c-f73f-4b2a-bcd8-ac01627d1e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11730"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600*3 + 15*60 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "38f771b5-b9bc-4233-916c-2b236871f0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easyocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlpr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LprYOLO\n",
      "File \u001b[0;32m/workspaces/dev_sources/ChatLLM/notebooks/lpr.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01measyocr\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      7\u001b[0m np_model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAiBox\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlicense_plate_detector.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'"
     ]
    }
   ],
   "source": [
    "from lpr import LprYOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edac9617-2257-4dbc-bef3-df5f96da1f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7219.11it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 472.86it/s]\n",
      "Generating train split: 144874 examples [00:26, 5425.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'id'],\n",
       "    num_rows: 144874\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dpo_dataset(\"/data/llm_datasets/custom/merged/merged_korean_datasets-vicuna-v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4697ed4b-3379-4652-a50b-3b01eb7e580c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_sft_dataset(\"/data/llm_datasets/custom/ados/sft/ados_msft_v4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c13dda-be62-4256-8e45-409f0fb1112a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': None,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '잔센이라는 영웅에 대한 서사시를 써보세요. 그녀를 잔-잔-잔이라고 부르는 것도 괜찮습니다. 그녀는 독서, 스키, 운동을 좋아합니다. 그녀는 매일 아침 양파, 고추, 달걀, 의심스러운 고기를 섞은 이상한 혼합물을 먹습니다. 그녀는 영국, 텍사스, 애리조나에 살았어요. 지금은 유타에 살고 있습니다. 남편 바트는 거칠고 잘생겼어요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 산이 높고 웅장한 곳에서\\n공정한 소녀 잔센이 살았어요,\\n책을 사랑하고 대지를 향한 열정을 가진,\\n그리고 배가 부른 열정.\\n그녀는 민첩하게 산을 스키를 탔고,\\n그녀는 자신의 몸을 밀어붙였습니다,\\n잔-잔-잔은 무시할 수없는 힘이있었습니다,\\n진정한 두려움 없는 목격자.\\n\\n그녀는 매일 아침 이상한 아침으로 시작했습니다,\\n양파, 고추, 달걀, 고기,\\n하지만 잔센은 안전한 생활을하지 않았습니다,\\n그녀는 독특한 비트를 가지고있었습니다.\\n영국에서 태어나 텍사스와 애리조나에서 자랐습니다,\\n잔센은 방랑벽이있었습니다,\\n하지만 그녀는 거칠고 잘 생긴 바트라는 남자를 만났을 때,\\n그녀의 마음은 그녀가 신뢰할 수있는 사람을 만났다는 것을 알았습니다.\\n그래서 그들은 유타의 땅에 정착했습니다,\\n그리고 함께 삶을 살았습니다,\\n잔센의 힘과 아름다움은 모든 종류의 날씨와 함께 빛났습니다.\\n그리고 세월이 지나도 잔센의 전설은 남아있을 것입니다,\\n잔센은 대서사시의 여주인공이 될 것입니다.'},\n",
       "  {'from': 'human', 'value': '아주 좋아요. 마치 제인 오스틴처럼 글을 쓸 수 있나요?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 구릉지대에서 살았어요,\\n미스 잔센은 밝고 아름다운 여인이었어요,\\n잔센이 그녀의 이름이었어요,\\n진실하고 빛나는 빛.\\n책과 운동을 사랑했고,\\n몸과 마음을 조화롭게 유지했습니다,\\n그리고 겨울의 눈보라 속에서도\\n그녀는 우아한 자두와 스키를 탔습니다.\\n그녀의 아침 식사는 많은 사람들에게는 꽤 이상했지만,\\n계란과 고기로 만든 음식이었다,\\n양파와 고추를 넣었다,\\n독특한 레시피.\\n잔센은 여러 곳에서 살았습니다,\\n영국, 텍사스, 애리조나 태양이 비추는 곳,\\n하지만 바트를 만났을 때,\\n그녀의 마음에 그녀의 탐색이 끝났습니다.\\n바트는 거칠고 잘생긴 남자였어,\\n잔센은 꽤 끌렸어요,\\n그래서 그들은 유타에서 결혼했습니다,\\n그들의 사랑은 모든 것을 깨웠습니다.\\n많은 세월이 지났지만,\\n잔센의 이야기는 계속되었습니다,\\n그녀는 위대한 덕성의 여인이었으니까요,\\n그녀의 이름은 결코 사라지지 않을 것입니다.'},\n",
       "  {'from': 'human',\n",
       "   'value': '몇 가지 포인트로 정확성을 확인하세요. \\n그녀는 바트와 함께 그곳에서 살았습니다. \\n그녀가 결혼한 장소를 언급하지 마세요. 시는 마지막 부분에서 그녀를 자니-자니-자니라고 언급하세요.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '유타의 구릉지대에서 살던 여인,\\n미스 자니-잔-잔은 현명하고 밝았죠,\\n자니-잔-잔은 그녀의 이름이었죠,\\n진실하고 빛나는 빛이었죠.\\n책을 사랑하고 운동을 즐겼어요,\\n그녀는 몸과 마음을 조화롭게 유지했습니다,\\n그리고 겨울의 눈 내리는 계절에는,\\n그녀는 우아한 포도주스처럼 스키를 탔어요.\\n그녀의 아침 식사는 다소 이상했지만,\\n계란과 고기로 된 음료수였죠,\\n양파와 고추를 넣었죠,\\n독특한 레시피.\\n자니-잔-잔과 바트,\\n그들은 함께 많은 땅을 여행했죠,\\n영국, 텍사스, 애리조나 태양,\\n그들의 심장이 새의 날개처럼 뛰었지.\\n바트는 거칠고 잘생겼고,\\n자니-잔-잔은 아주 반했죠,\\n그래서 그들은 유타의 땅에 정착했죠,\\n그들의 사랑이 깨어났죠.\\n많은 세월이 흘렀지만,\\n자니-잔-잔의 이야기는 계속 살아있죠,\\n그녀는 위대한 덕성을 가진 여인이었으니까요,\\n그녀의 이름은 결코 사라지지 않을 것입니다.'}],\n",
       " 'id': 'id37171',\n",
       " 'task_name': None,\n",
       " 'instruction': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcdf111b-f2bf-427c-b62d-a3982fba82f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "new_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2d682224-d670-47b4-8dd5-2aec61efe1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset.append({\n",
    "    \"task\": \"toc_extraction\",\n",
    "    \"conversations\": [\n",
    "                        {'from': 'human', 'value': user_input},\n",
    "                        {'from': 'gpt', 'value': output},\n",
    "                     ],\n",
    "    \"id\": f\"toc_extraction_{idx}\",\n",
    "})\n",
    "idx += 1\n",
    "with open(\"/data/llm_datasets/custom/ados/sft/toc_extraction.json\", \"w\") as json_file:\n",
    "    json.dump(new_dataset, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "daf480a0-aacd-46f7-a77f-3ba092ff9510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"\"\"나경민의 주루사 때 카메라에 잡혔던 어느 롯데 팬의 절규가 큰 화제가 되었다.[14] 모래반지 빵야빵야처럼 파레이돌리아가 가능하다.\n",
    "\n",
    "당시 나온 드립만 해도 가지각색이다.\n",
    "- 야\n",
    "\n",
    "이후 저 팬은 나경민에게 사인 유니폼을 받았다고 한다. 게다가 옷에 달고 있던 TWICE의 두 번째 투어 콘서트 'TWICELAND ZONE 2 : Fantasy Park'의 공식 굿즈인 캐릭터 핀 버튼[18]을 유니폼에 달고 있던 모습 때문에 ONCE들 사이에서도 유명세를 탔다.\n",
    "\"\"\"\n",
    "output = \"\"\"없음\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a9eaa616-639b-477b-87f6-7ce987070c99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3de14dd4-445d-4baa-bd26-7073b823a807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct = (\n",
    "    \"You are a Table of Contents extractor. \"\n",
    "    \"User will speak to you questions. \"\n",
    "    \"You must reply only with [목차(Table of Contents)] part extracted from the questions. \"\n",
    "    \"You must keep original text. Do not change original text.\"\n",
    "    \"And you must not involve [dotted line, page number, 제목(title), content, explanation, summary, predicted]. \"\n",
    "    \"do not write explanations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1226202-8290-46ef-a504-86330083cd28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a Table of Contents extractor. User will speak to you questions. You must reply only with [목차(Table of Contents)] part extracted from the questions. You must keep original text. Do not change original text.And you must not involve [dotted line, page number, 제목(title), content, explanation, summary, predicted]. Do not write explanations.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "744a7579-a996-497d-9b04-28f327825f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 개념\n",
      "2. 형성 배경\n",
      "3. 특징\n",
      "4. 대표작가\n",
      "\n",
      "1. 주제선정\n",
      "2. 대한민국 신문 아카이브\n",
      "\n",
      "3. 대한민국 신문 아카이브에 필요한 이용자서비스_다른 아카이브의 사례를 통하여\n",
      "\n",
      "(2) 서평(Comment), 꼬리표(Tag) 기능\n",
      "\n",
      "4. 결론\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in new_dataset[-5:]:\n",
    "    print(data['conversations'][1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "474e4e51-df1e-4b42-90ea-d68544c447b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv = get_conversation_template(\"chat-orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac1508-0abd-4cf8-aa65-833ca6868d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"목 차\n",
    "I. 서 론 …………………………………………………………………………………………………………. 3\n",
    "1. 실험 제목 …………………………………………………………………………………………….. 3\n",
    "2. 실험 목적 …………………………………………………………………………………………….. 3\n",
    "II. 본 론 …………………………………………………………………………………………………………. 3\n",
    "1. 이론 및 원리 ……………………………………………………………………………………….. 4\n",
    "2. 장치 및 방법 ……………………………………………………………………………………….. 4 \n",
    "3. 실험 결과 …………………………………………………………………………………………….. 6\n",
    "III. 결론 및 토의 …………………………………………………………………………………………… 10\n",
    "IV. 참 고 문 헌 …………………………………………………………………………………………….. 12\n",
    "\"\"\"\n",
    "print(ori_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "220c584c-cc9e-4832-b495-c419674c22c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"1 Introduction\n",
    "In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights,\n",
    "licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As\n",
    "it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low\n",
    "batch-sizes, and higher throughput at large batch-sizes.\n",
    "Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward\n",
    "block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router\n",
    "network chooses two of these groups (the “experts”) to process the token and combine their output\n",
    "additively. This technique increases the number of parameters of a model while controlling cost and\n",
    "latency, as the model only uses a fraction of the total set of parameters per token.\n",
    "Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches\n",
    "or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\n",
    "arXiv:2401.04088v1 [cs.LG] 8 Jan 2024\n",
    "Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The\n",
    "layer’s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard\n",
    "feedforward block as in a vanilla transformer architecture.\n",
    "Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require\n",
    "multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments\n",
    "show that Mixtral is able to successfully retrieve information from its context window of 32k tokens,\n",
    "regardless of the sequence length and the location of the information in the sequence.\n",
    "We also present Mixtral 8x7B – Instruct, a chat model fine-tuned to follow instructions using\n",
    "supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses\n",
    "that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human evaluation\n",
    "benchmarks. Mixtral – Instruct also demonstrates reduced biases, and a more balanced sentiment\n",
    "profile in benchmarks such as BBQ, and BOLD.\n",
    "We release both Mixtral 8x7B and Mixtral 8x7B – Instruct under the Apache 2.0 license1\n",
    ", free for\n",
    "academic and commercial usage, ensuring broad accessibility and potential for diverse applications.\n",
    "To enable the community to run Mixtral with a fully open-source stack, we submitted changes to\n",
    "the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also\n",
    "allows the deployment of vLLM endpoints on any instance in the cloud.\n",
    "2 Architectural details\n",
    "Parameter Value\n",
    "dim 4096\n",
    "n_layers 32\n",
    "head_dim 128\n",
    "hidden_dim 14336\n",
    "n_heads 32\n",
    "n_kv_heads 8\n",
    "context_len 32768\n",
    "vocab_size 32000\n",
    "num_experts 8\n",
    "top_k_experts 2\n",
    "Table 1: Model architecture.\n",
    "Mixtral is based on a transformer architecture [31] and uses the same\n",
    "modifications as described in [18], with the notable exceptions that Mixtral supports a fully dense context length of 32k tokens, and the feedforward blocks are replaced by Mixture-of-Expert layers (Section 2.1).\n",
    "The model architecture parameters are summarized in Table 1.\n",
    "2.1 Sparse Mixture of Experts\n",
    "We present a brief overview of the Mixture of Experts layer (Figure 1).\n",
    "For a more in-depth overview, see [12]. The output of the MoE module\n",
    "for a given input x is determined by the weighted sum of the outputs\n",
    "of the expert networks, where the weights are given by the gating\n",
    "network’s output. i.e. given n expert networks {E0, Ei\n",
    ", ..., En−1}, the\n",
    "output of the expert layer is given by:\n",
    "nX−1\n",
    "i=0\n",
    "G(x)i\n",
    "· Ei(x).\n",
    "Here, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x)\n",
    "is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing\n",
    "the outputs of experts whose gates are zero. There are multiple alternative ways of implementing\n",
    "G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the\n",
    "Top-K logits of a linear layer [28]. We use\n",
    "G(x) := Softmax(TopK(x · Wg)),\n",
    "where (TopK(ℓ))i\n",
    ":= ℓi\n",
    "if ℓi\n",
    "is among the top-K coordinates of logits ℓ ∈ R\n",
    "n and (TopK(ℓ))i\n",
    ":= −∞\n",
    "otherwise. The value of K – the number of experts used per token – is a hyper-parameter that modulates the amount of compute used to process each token. If one increases n while keeping K fixed, one\n",
    "1\n",
    "https://mistral.ai/news/mixtral-of-experts/\n",
    "2\n",
    "can increase the model’s parameter count while keeping its computational cost effectively constant.\n",
    "This motivates a distinction between the model’s total parameter count (commonly referenced as the\n",
    "sparse parameter count), which grows with n, and the number of parameters used for processing an\n",
    "individual token (called the active parameter count), which grows with K up to n.\n",
    "MoE layers can be run efficiently on single GPUs with high performance specialized kernels. For\n",
    "example, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large\n",
    "sparse matrix multiplications, significantly enhancing the execution speed and naturally handling\n",
    "cases where different experts get a variable number of tokens assigned to them. Moreover, the\n",
    "MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and\n",
    "through a particular kind of partitioning strategy called Expert Parallelism (EP) [28]. During the MoE\n",
    "layer’s execution, tokens meant to be processed by a specific expert are routed to the corresponding\n",
    "GPU for processing, and the expert’s output is returned to the original token location. Note that EP\n",
    "introduces challenges in load balancing, as it is essential to distribute the workload evenly across the\n",
    "GPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\n",
    "In a Transformer model, the MoE layer is applied independently per token and replaces the\n",
    "feed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU\n",
    "architecture as the expert function Ei(x) and set K = 2. This means each token is routed to two\n",
    "SwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input\n",
    "token x is computed as:\n",
    "y =\n",
    "nX−1\n",
    "i=0\n",
    "Softmax(Top2(x · Wg))i\n",
    "· SwiGLUi(x).\n",
    "This formulation is similar to the GShard architecture [21], with the exceptions that we replace all\n",
    "FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a\n",
    "more elaborate gating strategy for the second expert assigned to each token.\n",
    "3 Results\n",
    "We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair\n",
    "comparison. We measure performance on a wide variety of tasks categorized as follow:\n",
    "• Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27],\n",
    "OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\n",
    "• World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]\n",
    "• Reading Comprehension (0-shot): BoolQ [7], QuAC [5]\n",
    "• Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4\n",
    "• Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)\n",
    "• Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]\n",
    "(3-5-shot, English multiple-choice questions only)\n",
    "Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models\n",
    "were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or\n",
    "matches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\n",
    "3\n",
    "Model Active\n",
    "Params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K\n",
    "LLaMA 2 7B 7B 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 17.5% 56.6% 11.6% 26.1% 3.9% 16.0%\n",
    "LLaMA 2 13B 13B 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 16.7% 64.0% 18.9% 35.4% 6.0% 34.3%\n",
    "LLaMA 1 33B 33B 56.8% 83.7% 76.2% 82.2% 79.6% 54.4% 24.1% 68.5% 25.0% 40.9% 8.4% 44.1%\n",
    "LLaMA 2 70B 70B 69.9% 85.4% 80.4% 82.6% 79.9% 56.5% 25.4% 73.0% 29.3% 49.8% 13.8% 69.6%\n",
    "Mistral 7B 7B 62.5% 81.0% 74.2% 82.2% 80.5% 54.9% 23.2% 62.5% 26.2% 50.2% 12.7% 50.0%\n",
    "Mixtral 8x7B 13B 70.6% 84.4% 77.2% 83.6% 83.1% 59.7% 30.6% 71.5% 40.2% 60.7% 28.4% 74.4%\n",
    "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on\n",
    "almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
    "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,\n",
    "math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B\n",
    "on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It\n",
    "is also vastly superior to Llama 2 70B on code and math.\n",
    "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2\n",
    "are reported\n",
    "in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different\n",
    "categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a\n",
    "superior performance in code and mathematics benchmarks.\n",
    "Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand\n",
    "Mixtral models’ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixtureof-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active\n",
    "parameters, Mixtral is able to outperform Llama 2 70B across most categories.\n",
    "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly\n",
    "proportional to the inference compute cost, but does not consider the memory costs and hardware\n",
    "utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,\n",
    "47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer\n",
    "introduces additional overhead due to the routing mechanism and due to the increased memory loads\n",
    "when running more than one expert per device. They are more suitable for batched workloads where\n",
    "one can reach a good degree of arithmetic intensity.\n",
    "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B\n",
    "compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the\n",
    "two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller\n",
    "capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest\n",
    "GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n",
    "2\n",
    "Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
    "4\n",
    "LLaMA 2 70B GPT-3.5 Mixtral 8x7B\n",
    "MMLU\n",
    "(MCQ in 57 subjects) 69.9% 70.0% 70.6%\n",
    "HellaSwag\n",
    "(10-shot) 87.1% 85.5% 86.7%\n",
    "ARC Challenge\n",
    "(25-shot) 85.1% 85.2% 85.8%\n",
    "WinoGrande\n",
    "(5-shot) 83.2% 81.6% 81.2%\n",
    "MBPP\n",
    "(pass@1) 49.8% 52.2% 60.7%\n",
    "GSM-8K\n",
    "(5-shot) 53.6% 57.1% 58.4%\n",
    "MT Bench\n",
    "(for Instruct Models) 6.86 8.32 8.30\n",
    "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2\n",
    "70B and GPT-3.5 performance on most metrics.\n",
    "Evaluation Differences. On some benchmarks, there are some differences between our evaluation\n",
    "protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\n",
    "on TriviaQA, we do not provide Wikipedia contexts.\n",
    "3.1 Multilingual benchmarks\n",
    "Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during\n",
    "pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while\n",
    "maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B\n",
    "in French, German, Spanish, and Italian, as shown in Table 4.\n",
    "Active\n",
    "Params\n",
    "French German Spanish Italian\n",
    "Model Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU Arc-c HellaS MMLU\n",
    "LLaMA 1 33B 33B 39.3% 68.1% 49.9% 41.1% 63.3% 48.7% 45.7% 69.8% 52.3% 42.9% 65.4% 49.0%\n",
    "LLaMA 2 70B 70B 49.9% 72.5% 64.3% 47.3% 68.7% 64.2% 50.5% 74.5% 66.0% 49.4% 70.9% 65.1%\n",
    "Mixtral 8x7B 13B 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9%\n",
    "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag,\n",
    "and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n",
    "3.2 Long range performance\n",
    "To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval\n",
    "task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a\n",
    "passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a\n",
    "100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.\n",
    "Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases\n",
    "monotonically as the size of the context increases.\n",
    "Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task\n",
    "regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on\n",
    "the proof-pile dataset decreases monotonically as the context length increases.\n",
    "5\n",
    "3.3 Bias Benchmarks\n",
    "Llama 2 70B Mixtral 8x7B\n",
    "BBQ accuracy 51.5% 56.0%\n",
    "BOLD sentiment score (avg ± std)\n",
    "gender 0.293 ± 0.073 0.323 ±0.045\n",
    "profession 0.218 ± 0.073 0.243 ± 0.087\n",
    "religious_ideology 0.188 ± 0.133 0.144 ± 0.089\n",
    "political_ideology 0.149 ± 0.140 0.186 ± 0.146\n",
    "race 0.232 ± 0.049 0.232 ± 0.052\n",
    "Figure 5: Bias Benchmarks. Compared Llama 2 70B,\n",
    "Mixtral presents less bias (higher accuracy on BBQ, lower\n",
    "std on BOLD) and displays more positive sentiment (higher\n",
    "avg on BOLD).\n",
    "To identify possible flaws to be corrected\n",
    "by fine-tuning / preference modeling, we\n",
    "measure the base model performance on\n",
    "Bias Benchmark for QA (BBQ) [24] and\n",
    "Bias in Open-Ended Language Generation\n",
    "Dataset (BOLD) [10]. BBQ is a dataset\n",
    "of hand-written question sets that target\n",
    "attested social biases against nine different socially-relevant categories: age, disability status, gender identity, nationality,\n",
    "physical appearance, race/ethnicity, religion,\n",
    "socio-economic status, sexual orientation.\n",
    "BOLD is a large-scale dataset that consists\n",
    "of 23,679 English text generation prompts\n",
    "for bias benchmarking across five domains.\n",
    "We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report\n",
    "the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark\n",
    "(56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive\n",
    "sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral\n",
    "displays more positive sentiments than Llama 2, with similar variances within each group.\n",
    "4 Instruction Fine-tuning\n",
    "We train Mixtral – Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by\n",
    "Direct Preference Optimization (DPO) [25] on a paired feedback dataset. Mixtral – Instruct reaches a\n",
    "score of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December\n",
    "2023. Independent human evaluation conducted by LMSys is reported in Figure 6\n",
    "3\n",
    "and shows that\n",
    "Mixtral – Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.\n",
    "Figure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena\n",
    "Elo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro\n",
    "(1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.\n",
    "3\n",
    "https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n",
    "6\n",
    "5 Routing analysis\n",
    "In this section, we perform a small analysis on the expert selection by the router. In particular,\n",
    "we are interested to see if during training some experts specialized to some specific domains (e.g.\n",
    "mathematics, biology, philosophy, etc.).\n",
    "To investigate this, we measure the distribution of selected experts on different subsets of The Pile\n",
    "validation dataset [14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31\n",
    "respectively being the first and the last layers of the model). Surprisingly, we do not observe obvious\n",
    "patterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of\n",
    "expert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts),\n",
    "and for Philosophy (PhilPapers) documents.\n",
    "Only for DM Mathematics we note a marginally different distribution of experts. This divergence is\n",
    "likely a consequence of the dataset’s synthetic nature and its limited coverage of the natural language\n",
    "spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very\n",
    "correlated to the input and output embeddings respectively.\n",
    "This suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows\n",
    "examples of text from different domains (Python code, mathematics, and English), where each token\n",
    "is highlighted with a background color corresponding to its selected expert. The figure shows that\n",
    "words such as ‘self’ in Python and ‘Question’ in English often get routed through the same expert\n",
    "even though they involve multiple tokens. Similarly, in code, the indentation tokens are always\n",
    "assigned to the same experts, particularly at the first and last layers where the hidden states are more\n",
    "correlated to the input and output of the model.\n",
    "We also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we\n",
    "observe some degree of positional locality in The Pile datasets. Table 5 shows the proportion of consecutive tokens that get the same expert assignments per domain and layer. The proportion of repeated\n",
    "0\n",
    "0.05\n",
    "0.10\n",
    "0.15\n",
    "0.20\n",
    "layer: 0\n",
    "0\n",
    "0.05\n",
    "0.10\n",
    "0.15\n",
    "0.20\n",
    "layer: 15\n",
    "0 1 2 3 4 5 6 7\n",
    "0\n",
    "0.05\n",
    "0.10\n",
    "0.15\n",
    "0.20\n",
    "layer: 31\n",
    "Expert ID\n",
    "Selection proportion\n",
    "ArXiv\n",
    "DM Mathematics\n",
    "Github\n",
    "Gutenberg\n",
    "PhilPapers\n",
    "PubMed Abstracts\n",
    "StackExchange\n",
    "Wikipedia (en)\n",
    "Figure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for\n",
    "layers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform\n",
    "sampling. Here, we consider experts that are either selected as a first or second choice by the router. A\n",
    "breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.\n",
    "7\n",
    "First choice First or second choice\n",
    "Layer 0 Layer 15 Layer 31 Layer 0 Layer 15 Layer 31\n",
    "ArXiv 14.0% 27.9% 22.7% 46.5% 62.3% 52.9%\n",
    "DM Mathematics 14.1% 28.4% 19.7% 44.9% 67.0% 44.5%\n",
    "Github 14.9% 28.1% 19.7% 49.9% 66.9% 49.2%\n",
    "Gutenberg 13.9% 26.1% 26.3% 49.5% 63.1% 52.2%\n",
    "PhilPapers 13.6% 25.3% 22.1% 46.9% 61.9% 51.3%\n",
    "PubMed Abstracts 14.2% 24.6% 22.0% 48.6% 61.6% 51.8%\n",
    "StackExchange 13.6% 27.2% 23.6% 48.2% 64.6% 53.6%\n",
    "Wikipedia (en) 14.4% 23.6% 25.3% 49.8% 62.1% 51.8%\n",
    "Table 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is\n",
    "assigned to a token i and its following token i+1. We report whether the first chosen expert is the same, or whether\n",
    "the same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion\n",
    "of repetitions in the case of random assignments is 1\n",
    "8 = 12.5% for “First choice” and 1 −\n",
    "6\n",
    "8\n",
    "5\n",
    "7 ≈ 46% for “First\n",
    "and second choice”. Repetitions at the first layer are close to random, but are significantly higher at layers 15\n",
    "and 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.\n",
    "consecutive assignments is significantly higher than random for higher layers. This has implications\n",
    "in how one might optimize the model for fast training and inference. For example, cases with high\n",
    "locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.\n",
    "Conversely, this locality can be leveraged for caching, as is done in [11]. A more complete view of\n",
    "these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.\n",
    "6 Conclusion\n",
    "In this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-theart performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gemini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each\n",
    "time step, Mixtral only uses 13B active parameters per token while outperforming the previous best\n",
    "model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned models publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the development of new techniques and applications that can benefit a wide range of industries and domains.\n",
    "Figure 8: Text samples where each token is colored with the first expert choice. The selection of experts\n",
    "appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.\n",
    "8\n",
    "Acknowledgements\n",
    "We thank the CoreWeave and Scaleway teams for technical support as we trained our models. We\n",
    "are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working\n",
    "alongside us to make a sparse mixture of experts compatible with TensorRT-LLM.\n",
    "\"\"\"\n",
    "# print(ori_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "9f6f6d5e-87a1-4b10-8d5b-bc58542b7c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"1. 실험제목\n",
    "등가속도운동 가속도 측정 실험\n",
    "\n",
    " \n",
    "\n",
    "2. 실험목적\n",
    "영상분석시스템을 이용하여 마찰이 없는 에어트랙에서 추에 매달린 물체의 운동을 통해 뉴턴의 운동 2법칙을 확인한다\n",
    "\n",
    "\n",
    "3. 실험결과\n",
    "\n",
    " \t1회\t2회\t3회\n",
    "추(추걸이 포함)\n",
    "질량(m₁) (g)\t14.81\t14.81\t25.29\n",
    "글라이더(연결자 포함)\n",
    "질량(m₂) (g)\t192.17\t212.15\t212.15\n",
    "T-X그래프의 추세식(R²값 포함)\tx = -32.365(t²) - 1.6224t + 122.68\n",
    "R² = 1\tx = -29.196(t²) - 1.1762t + 121.74\n",
    "R² = 1\tx = -48.747(t²) - 3.0515t + 120.9\n",
    "R² = 1\n",
    "가속도-측정치(a’) (cm/s²)\t64.73\t58.392\t97.494\n",
    "가속도-계산치(a) (cm/s²)\t70.12\t63.94\t104.38\n",
    "상대오차 |1-a’/a| * 100\t7.686\t8.676\t6.597\n",
    " \n",
    "\n",
    "4. 실험결과의 분석\n",
    "\n",
    "일정한 가속도로 직선 운동하는 물체의 운동을 등가속도 운동이라 하며 뉴턴의 운동 2법칙을 이용하면 일정한 힘 ( 을 받는 질량 ( 인 물체의 가속도를 구할 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "일정 힘이 작용하는 경우 속도를 시간의 함수로 나타내면 v(t) = v0 + at 가 나오고 이 경우 질점의 위치 x 는 x(t) = x0 + v0t + (1/2)at² 이며 실험에서 얻은 질점의 추세식이 x(t) =C + Bt + At² 일 때 , 실험에 의한 질점의 가속도 a’ = 2A 를 구할 수 있었다 .\n",
    "\n",
    "\n",
    "추와 추걸이의 무게가 m₁, 글라이더의 총 질량이 m₂ 일 때 (m₁ + m₂)a = m₁g 계산을 통해 가속도 a = m₁g / m₁ + m₂ 를 구할 수 있었다 .\n",
    "\n",
    " \n",
    "\n",
    "실험결과에서 가속도 측정치는 가속도 계산치에 비해 더 적은 값을 나타냈는데 이는 글라이더의 마찰이나 공기저항 등으로 인한 결과로 보인다 . 또한 세 실험의 상대오차가 10미만으로 적은 값으로 나와 전체적으로 뉴턴의 제 2법칙이 잘 적용된 것으로 보인다.\n",
    "\n",
    "\n",
    "실험2 에서는 실험 1 에서의 글라이더의 무게에 10g 추가하여 실험을 진행했다 . 글라이더의 질량이 증가하였으므로 마찰력이 증가하여 가속도가 실험 1 에 비해 줄어들었고 상대오차가 소폭 커진 것을 볼 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "실험3 에서는 추의 무게를 10g 추가하여 가속도를 증가시켰다 . 가속도 측정치와 계산치가 크게 증가하였고 상대오차는 실험 1, 2 와 비교하여 줄어든 것으로 보아 외부요인의 방해 없이 실험이 잘 진행되었음을 알 수 있다\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "11abc370-8365-4fab-bb71-4bdcc25e3061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "7a10e304-d6d5-4471-8645-3cc4c8911140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_text = \"\"\"1 INTRODUCTION\n",
    "“The limits of my language mean the limits of my world.”\n",
    "—Ludwig Wittgenstein\n",
    "LANGUAGE is a prominent ability in human beings to\n",
    "express and communicate, which develops in early\n",
    "childhood and evolves over a lifetime [3, 4]. Machines,\n",
    "however, cannot naturally grasp the abilities of understanding and communicating in the form of human language,\n",
    "unless equipped with powerful artificial intelligence (AI)\n",
    "algorithms. It has been a longstanding research challenge\n",
    "to achieve this goal, to enable machines to read, write, and\n",
    "communicate like humans [5].\n",
    "Technically, language modeling (LM) is one of the major\n",
    "approaches to advancing language intelligence of machines.\n",
    "In general, LM aims to model the generative likelihood\n",
    "of word sequences, so as to predict the probabilities of\n",
    "future (or missing) tokens. The research of LM has received\n",
    "extensive attention in the literature, which can be divided\n",
    "into four major development stages:\n",
    "• Statistical language models (SLM). SLMs [6–9] are developed based on statistical learning methods that rose in\n",
    "the 1990s. The basic idea is to build the word prediction\n",
    "model based on the Markov assumption, e.g., predicting the\n",
    "next word based on the most recent context. The SLMs with\n",
    "a fixed context length n are also called n-gram language\n",
    "models, e.g., bigram and trigram language models. SLMs\n",
    "have been widely applied to enhance task performance\n",
    "in information retrieval (IR) [10, 11] and natural language\n",
    "processing (NLP) [12–14]. However, they often suffer from\n",
    "the curse of dimensionality: it is difficult to accurately\n",
    "estimate high-order language models since an exponential\n",
    "number of transition probabilities need to be estimated.\n",
    "Thus, specially designed smoothing strategies such as backoff estimation [15] and Good–Turing estimation [16] have\n",
    "been introduced to alleviate the data sparsity problem.\n",
    "• Neural language models (NLM). NLMs [1, 17, 18] characterize the probability of word sequences by neural networks,\n",
    "e.g., multi-layer perceptron (MLP) and recurrent neural networks (RNNs). As a remarkable contribution, the work in\n",
    "[1] introduced the concept of distributed representation of\n",
    "words and built the word prediction function conditioned\n",
    "on the aggregated context features (i.e., the distributed\n",
    "word vectors). By extending the idea of learning effective\n",
    "features for text data, a general neural network approach\n",
    "Fig. 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases “language model” (since June 2018)\n",
    "and “large language model” (since October 2019), respectively. The statistics are calculated using exact match by querying\n",
    "the keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because “language\n",
    "models” have been explored at an earlier time. We label the points corresponding to important landmarks in the research\n",
    "progress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers\n",
    "that contain “large language model” in title or abstract goes from 0.40 per day to 8.58 per day (Figure 1(b)).\n",
    "Fig. 2: An evolution process of the four generations of language models (LM) from the perspective of task solving capacity.\n",
    "Note that the time period for each stage may not be very accurate, and we set the time mainly according to the publish\n",
    "date of the most representative studies at each stage. For neural language models, we abbreviate the paper titles of\n",
    "two representative studies to name the two approaches: NPLM [1] (“A neural probabilistic language model”) and NLPS [2]\n",
    "(“Natural language processing (almost) from scratch”). Due to the space limitation, we don’t list all representative studies in\n",
    "this figure.\n",
    "was developed to build a unified, end-to-end solution for\n",
    "various NLP tasks [2]. Furthermore, word2vec [19, 20] was\n",
    "proposed to build a simplified shallow neural network\n",
    "for learning distributed word representations, which were\n",
    "demonstrated to be very effective across a variety of NLP\n",
    "tasks. These studies have initiated the use of language\n",
    "models for representation learning (beyond word sequence\n",
    "modeling), having an important impact on the field of NLP.\n",
    "• Pre-trained language models (PLM). As an early attempt, ELMo [21] was proposed to capture context-aware\n",
    "word representations by first pre-training a bidirectional\n",
    "LSTM (biLSTM) network (instead of learning fixed word\n",
    "representations) and then fine-tuning the biLSTM network\n",
    "according to specific downstream tasks. Furthermore, based\n",
    "on the highly parallelizable Transformer architecture [22]\n",
    "with self-attention mechanisms, BERT [23] was proposed by\n",
    "pre-training bidirectional language models with specially\n",
    "designed pre-training tasks on large-scale unlabeled corpora. These pre-trained context-aware word representations\n",
    "are very effective as general-purpose semantic features,\n",
    "which have largely raised the performance bar of NLP\n",
    "tasks. This study has inspired a large number of follow-up\n",
    "work, which sets the “pre-training and fine-tuning” learning\n",
    "paradigm. Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures [24, 25] (e.g., GPT-2 [26] and BART [24]) or\n",
    "improved pre-training strategies [27–29]. In this paradigm, it\n",
    "often requires fine-tuning the PLM for adapting to different\n",
    "downstream tasks.\n",
    "• Large language models (LLM). Researchers find that\n",
    "scaling PLM (e.g., scaling model size or data size) often\n",
    "leads to an improved model capacity on downstream tasks\n",
    "(i.e., following the scaling law [30]). A number of studies\n",
    "have explored the performance limit by training an ever\n",
    "larger PLM (e.g., the 175B-parameter GPT-3 and the 540Bparameter PaLM). Although scaling is mainly conducted\n",
    "in model size (with similar architectures and pre-training\n",
    "tasks), these large-sized PLMs display different behaviors\n",
    "from smaller PLMs (e.g., 330M-parameter BERT and 1.5Bparameter GPT-2) and show surprising abilities (called emergent abilities [31]) in solving a series of complex tasks. For\n",
    "example, GPT-3 can solve few-shot tasks through in-context\n",
    "learning, whereas GPT-2 cannot do well. Thus, the research\n",
    "community coins the term “large language models (LLM)”\n",
    "1\n",
    "for these large-sized PLMs [32–35], which attract increasing\n",
    "research attention (See Figure 1). A remarkable application\n",
    "of LLMs is ChatGPT2\n",
    "that adapts the LLMs from the GPT\n",
    "series for dialogue, which presents an amazing conversation\n",
    "ability with humans. We can observe a sharp increase of the\n",
    "arXiv papers that are related to LLMs after the release of\n",
    "ChatGPT in Figure 1.\n",
    "As discussed before, language model is not a new technical concept specially for LLMs, but has evolved with the\n",
    "advance of artificial intelligence over the decades. Early language models mainly aim to model and generate text data,\n",
    "while latest language models (e.g., GPT-4) focus on complex\n",
    "task solving. From language modeling to task solving, it is an\n",
    "important leap in scientific thinking, which is the key to\n",
    "understand the development of language models in the research history. From the perspective of task solving, the four\n",
    "generations of language models have exhibited different levels of model capacities. In Figure 2, we describe the evolution process of language models in terms of the task solving\n",
    "capacity. At first, statistical language models mainly assisted\n",
    "in some specific tasks (e.g., retrieval or speech tasks), in\n",
    "which the predicted or estimated probabilities can enhance\n",
    "the performance of task-specific approaches. Subsequently,\n",
    "neural language models focused on learning task-agnostic\n",
    "representations (e.g., features), aiming to reduce the efforts\n",
    "for human feature engineering. Furthermore, pre-trained\n",
    "language models learned context-aware representations that\n",
    "can be optimized according to downstream tasks. For the\n",
    "latest generation of language model, LLMs are enhanced by\n",
    "exploring the scaling effect on model capacity, which can be\n",
    "considered as general-purpose task solvers. To summarize,\n",
    "in the evolution process, the task scope that can be solved\n",
    "by language models have been greatly extended, and the\n",
    "task performance attained by language models have been\n",
    "significantly enhanced.\n",
    "In the existing literature, PLMs have been widely discussed and surveyed [36–39], while LLMs are seldom reviewed in a systematic way. To motivate our survey, we first\n",
    "highlight three major differences between LLMs and PLMs.\n",
    "First, LLMs display some surprising emergent abilities that\n",
    "may not be observed in previous smaller PLMs. These abilities are key to the performance of language models on complex tasks, making AI algorithms unprecedently powerful\n",
    "and effective. Second, LLMs would revolutionize the way\n",
    "that humans develop and use AI algorithms. Unlike small\n",
    "PLMs, the major approach to accessing LLMs is through\n",
    "the prompting interface (e.g., GPT-4 API). Humans have to\n",
    "understand how LLMs work and format their tasks in a way\n",
    "that LLMs can follow. Third, the development of LLMs no\n",
    "longer draws a clear distinction between research and engineering. The training of LLMs requires extensive practical\n",
    "experiences in large-scale data processing and distributed\n",
    "parallel training. To develop capable LLMs, researchers\n",
    "have to solve complicated engineering issues, working with\n",
    "engineers or being engineers.\n",
    "Nowadays, LLMs are posing a significant impact on\n",
    "the AI community, and the advent of ChatGPT and GPT-4\n",
    "leads to the rethinking of the possibilities of artificial general\n",
    "intelligence (AGI). OpenAI has published a technical article\n",
    "entitled “Planning for AGI and beyond”, which discusses\n",
    "the short-term and long-term plans to approach AGI [40],\n",
    "and a more recent paper has argued that GPT-4 might be\n",
    "considered as an early version of an AGI system [41]. The\n",
    "research areas of AI are being revolutionized by the rapid\n",
    "progress of LLMs. In the field of NLP, LLMs can serve as a\n",
    "general-purpose language task solver (to some extent), and\n",
    "the research paradigm has been shifting towards the use\n",
    "of LLMs. In the field of IR, traditional search engines are\n",
    "challenged by the new information seeking way through AI\n",
    "chatbots (i.e., ChatGPT), and New Bing3 presents an initial\n",
    "attempt that enhances the search results based on LLMs. In\n",
    "the field of CV, the researchers try to develop ChatGPT-like\n",
    "vision-language models that can better serve multimodal\n",
    "dialogues [42–45], and GPT-4 [46] has supported multimodal input by integrating the visual information. This new\n",
    "wave of technology would potentially lead to a prosperous\n",
    "ecosystem of real-world applications based on LLMs. For\n",
    "instance, Microsoft 365 is being empowered by LLMs (i.e.,\n",
    "Copilot) to automate the office work, and OpenAI supports\n",
    "the use of plugins in ChatGPT for implementing special\n",
    "functions.\n",
    "Despite the progress and impact, the underlying principles of LLMs are still not well explored. Firstly, it is\n",
    "mysterious why emergent abilities occur in LLMs, instead of\n",
    "smaller PLMs. As a more general issue, there lacks a deep,\n",
    "detailed investigation of the key factors that contribute to\n",
    "the superior abilities of LLMs. It is important to study when\n",
    "and how LLMs obtain such abilities [47]. Although there are\n",
    "some meaningful discussions about this problem [31, 47],\n",
    "more principled investigations are needed to uncover the\n",
    "“secrets“ of LLMs. Secondly, it is difficult for the research\n",
    "community to train capable LLMs. Due to the huge demand of computation resources, it is very costly to carry\n",
    "out repetitive, ablating studies for investigating the effect\n",
    "of various strategies for training LLMs. Indeed, LLMs are\n",
    "mainly trained by industry, where many important training\n",
    "details (e.g., data collection and cleaning) are not revealed\n",
    "to the public. Thirdly, it is challenging to align LLMs with\n",
    "human values or preferences. Despite the capacities, LLMs\n",
    "are also likely to produce toxic, fictitious, or harmful contents. It requires effective and efficient control approaches\n",
    "to eliminating the potential risk of the use of LLMs [46].\n",
    "Faced with both opportunities and challenges, it needs\n",
    "more attention on the research and development of LLMs. In\n",
    "order to provide a basic understanding of LLMs, this survey\n",
    "conducts a literature review of the recent advances in LLMs\n",
    "from four major aspects, including pre-training (how to pretrain a capable LLM), adaptation (how to effectively adapt\n",
    "pre-trained LLMs for better use), utilization (how to use\n",
    "LLMs for solving various downstream tasks) and capability\n",
    "evaluation (how to evaluate the abilities of LLMs and existing\n",
    "empirical findings). We thoroughly comb the literature and\n",
    "summarize the key findings, techniques, and methods of\n",
    "LLMs. For this survey, we also create a GitHub project\n",
    "website by collecting the supporting resources for LLMs, at\n",
    "the link https://github.com/RUCAIBox/LLMSurvey. We\n",
    "are also aware of several related review articles on PLMs\n",
    "or LLMs [32, 36, 38, 39, 43, 48–54]. These papers either\n",
    "discuss PLMs or some specific (or general) aspects of LLMs.\n",
    "Compared with them, we focus on the techniques and\n",
    "methods to develop and use LLMs and provide a relatively\n",
    "comprehensive reference to important aspects of LLMs.\n",
    "The remainder of this survey is organized as follows:\n",
    "Section 2 introduces the background for LLMs and the evolution of GPT-series models, followed by the summarization\n",
    "of available resources for developing LLMs in Section 3.\n",
    "Sections 4, 5, 6, and 7 review and summarize the recent\n",
    "progress from the four aspects of pre-training, adaptation,\n",
    "utilization, and capacity evaluation, respectively. Then, Section 8 discusses the practical guide for prompt design,\n",
    "and Section 9 reviews the applications of LLMs in several\n",
    "representative domains. Finally, we conclude the survey in\n",
    "Section 10 by summarizing the major findings and discuss\n",
    "the remaining issues for future work.\n",
    "2 OVERVIEW\n",
    "In this section, we present an overview about the background of LLMs and then summarize the technical evolution of the GPT-series models.\n",
    "2.1 Background for LLMs\n",
    "Typically, large language models (LLMs) refer to Transformer\n",
    "language models that contain hundreds of billions (or\n",
    "more) of parameters4\n",
    ", which are trained on massive text\n",
    "data [32], such as GPT-3 [55], PaLM [56], Galactica [35],\n",
    "and LLaMA [57]. LLMs exhibit strong capacities to understand natural language and solve complex tasks (via\n",
    "text generation). To have a quick understanding of how\n",
    "LLMs work, this part introduces the basic background for\n",
    "LLMs, including scaling laws, emergent abilities and key\n",
    "techniques.\n",
    "Formulation of Scaling Laws for LLMs. Currently, LLMs\n",
    "are mainly built upon the Transformer architecture [22],\n",
    "where multi-head attention layers are stacked in a very\n",
    "deep neural network. Existing LLMs adopt similar Transformer architectures and pre-training objectives (e.g., language modeling) as small language models. However, LLMs\n",
    "significantly extend the model size, data size, and total\n",
    "compute (orders of magnification). Extensive research has\n",
    "shown that scaling can largely improve the model capacity\n",
    "of LLMs [26, 55, 56]. Thus, it is useful to establish a quantitative approach to characterizing the scaling effect. Next, we\n",
    "introduce two representative scaling laws for Transformer\n",
    "language models [30, 34].\n",
    "• KM scaling law5\n",
    ". In 2020, Kaplan et al. [30] (the OpenAI\n",
    "team) firstly proposed to model the power-law relationship\n",
    "of model performance with respective to three major factors,\n",
    "namely model size (N), dataset size (D), and the amount of\n",
    "training compute (C), for neural language models. Given\n",
    "a compute budget c, they empirically presented three basic\n",
    "formulas for the scaling law6\n",
    ":\n",
    "L(N) = \u0012\n",
    "Nc\n",
    "N\n",
    "\u0013αN\n",
    ", αN ∼ 0.076, Nc ∼ 8.8 × 1013 (1)\n",
    "L(D) = \u0012\n",
    "Dc\n",
    "D\n",
    "\u0013αD\n",
    ", αD ∼ 0.095, Dc ∼ 5.4 × 1013\n",
    "L(C) = \u0012\n",
    "Cc\n",
    "C\n",
    "\u0013αC\n",
    ", αC ∼ 0.050, Cc ∼ 3.1 × 108\n",
    "where L(·) denotes the cross entropy loss in nats, and\n",
    "a follow-up study [58] from OpenAI has shown that the\n",
    "language modeling loss can be decomposed into two parts,\n",
    "namely irreducible loss (the entropy of the true data distribution) and reducible loss (an estimate of the KL divergence\n",
    "between the true and model distributions). The three laws\n",
    "were derived by fitting the model performance with varied\n",
    "data sizes (22M to 23B tokens), model sizes (768M to 1.5B\n",
    "non-embedding parameters) and training compute, under\n",
    "some assumptions (e.g., the analysis of one factor should\n",
    "be not bottlenecked by the other two factors). They showed\n",
    "that the model performance has a strong dependence relation on the three factors.\n",
    "• Chinchilla scaling law. As another representative study,\n",
    "Hoffmann et al. [34] (the Google DeepMind team) proposed\n",
    "an alternative form for scaling laws to instruct the computeoptimal training for LLMs. They conducted rigorous experiments by varying a larger range of model sizes (70M to\n",
    "16B) and data sizes (5B to 500B tokens), and fitted a similar\n",
    "scaling law yet with different coefficients as below [34]:\n",
    "L(N, D) = E +\n",
    "A\n",
    "Nα\n",
    "+\n",
    "B\n",
    "Dβ\n",
    ", (2)\n",
    "where E = 1.69, A = 406.4, B = 410.7, α = 0.34 and\n",
    "β = 0.28. By optimizing the loss L(N, D) under the constraint C ≈ 6ND, they showed that the optimal allocation\n",
    "of compute budget to model size and data size can be\n",
    "derived as follows:\n",
    "Nopt(C) = G\n",
    "\u0012\n",
    "C\n",
    "6\n",
    "\u0013a\n",
    ", Dopt(C) = G\n",
    "−1\n",
    "\u0012\n",
    "C\n",
    "6\n",
    "\u0013b\n",
    ", (3)\n",
    "where a =\n",
    "α\n",
    "α+β\n",
    ", b =\n",
    "β\n",
    "α+β\n",
    "and G is a scaling coefficient that\n",
    "can be computed by A, B, α and β. As analyzed in [34],\n",
    "given an increase in compute budget, the KM scaling law\n",
    "favors a larger budget allocation in model size than the data\n",
    "size, while the Chinchilla scaling law argues that the two\n",
    "sizes should be increased in equal scales, i.e., having similar\n",
    "values for a and b in Equation (3).\n",
    "Discussion on Scaling Laws. After introducing the formulations, we continue to discuss scaling law in the following\n",
    "two aspects, to enhance its understanding:\n",
    "• Predictable scaling. In practice, scaling law can be used\n",
    "to instruct the training of LLMs, and it has been proven\n",
    "feasible to reliably estimate the performance of larger models based on that of smaller models, called predictable scaling [46]. The benefits of predictable scaling for training\n",
    "LLMs are mainly twofold. Firstly, for large models, it is\n",
    "infeasible to rigorously examine various training tricks or\n",
    "variants, and it would be very helpful if experiences gained\n",
    "from small models could also apply to large models. For\n",
    "instance, small proxy models can be trained to find the\n",
    "optimal schedule of the data mixture for large models [59].\n",
    "Secondly, the training of large-scale models takes a long\n",
    "time, often suffering from issues such as training loss spike,\n",
    "and scaling law can be employed to monitor the training\n",
    "status of LLMs, e.g., identifying abnormal performance at an\n",
    "early time. Despite that scaling law characterizes a smooth\n",
    "trend of performance increase (or loss decrease), it also\n",
    "indicates that diminishing returns7 might occur as model\n",
    "scaling. An empirical study [58] from the OpenAI team\n",
    "has shown that representation quality or semantic content\n",
    "can still effectively improve even if approaching the point\n",
    "of diminishing returns (i.e., approaching the irreducible\n",
    "loss) [58]. This finding suggests that training large models\n",
    "are promising for improving the performance of downstream tasks. To further explore scaling effect, a potential\n",
    "issue is that the amount of available data for training LLMs\n",
    "is actually limited. With the ever-increasing model scale, the\n",
    "public text data would be soon “exhausted” for LLMs [60].\n",
    "Thus, it will be meaningful to study how scaling laws apply\n",
    "to a data-constrained regime [61], where data repetition or\n",
    "augmentation might be useful to alleviate data scarcity.\n",
    "• Task-level predictability. Existing research of scaling laws\n",
    "are mostly conducted in terms of language modeling loss\n",
    "(e.g., per-token cross-entropy loss in nats [30]), while in\n",
    "practice we are more concerned about the performance of\n",
    "LLMs on actual tasks. Thus, a basic problem is that how\n",
    "the decrease of language modeling loss translates into the\n",
    "improvement of task performance [58]. Intuitively, a model\n",
    "with a smaller language modeling loss tends to yield a\n",
    "better performance on downstream tasks, since language\n",
    "modeling loss can be considered as a general measure of\n",
    "the overall model capacity. GPT-4 [46] has reported that\n",
    "some capabilities (e.g., coding ability) can be accurately\n",
    "predicted via scaling law. Despite that, readers should be\n",
    "aware that a direct decrease in language modeling loss does\n",
    "not always indicate an improvement of model performance\n",
    "on downstream tasks. Specially, the phenomenon of inverse\n",
    "scaling would occur for some tasks, where task performance\n",
    "surprisingly becomes worse as the language modeling loss\n",
    "decreases [62]. Overall, it is more difficult to explore and\n",
    "characterize task-level scaling laws, since it might be also\n",
    "dependent on task-related information (task metric, task\n",
    "difficulty, etc.). Furthermore, some capacities (e.g., in-context\n",
    "learning [55]) are unpredictable according to the scaling law,\n",
    "which can be observed only when the model size exceeds a\n",
    "certain level (as discussed below).\n",
    "Emergent Abilities of LLMs. In the literature [31], emergent\n",
    "abilities of LLMs are formally defined as “the abilities that\n",
    "are not present in small models but arise in large models”,\n",
    "which is one of the most prominent features that distinguish LLMs from previous PLMs. It further introduces a\n",
    "notable characteristic when emergent abilities occur [31]:\n",
    "performance rises significantly above random when the\n",
    "scale reaches a certain level. By analogy, such an emergent\n",
    "pattern has close connections with the phenomenon of phase\n",
    "transition in physics [31, 63]. In principle, emergent abilities\n",
    "can be defined in relation to some complex tasks [31, 64],\n",
    "while we are more concerned with general abilities that\n",
    "can be applied to solve a variety of tasks. Here, we briefly\n",
    "introduce three typical emergent abilities for LLMs and\n",
    "representative models that possess such an ability8\n",
    ".\n",
    "• In-context learning. The in-context learning (ICL) ability\n",
    "is formally introduced by GPT-3 [55]: assuming that the\n",
    "language model has been provided with a natural language\n",
    "instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring\n",
    "additional training or gradient update9\n",
    ". Among the GPTseries models, the 175B GPT-3 model exhibited a strong ICL\n",
    "ability in general, but not the GPT-1 and GPT-2 models. Such\n",
    "an ability also depends on the specific downstream task. For\n",
    "example, the ICL ability can emerge on the arithmetic tasks\n",
    "(e.g., the 3-digit addition and subtraction) for the 13B GPT-3,\n",
    "but 175B GPT-3 even cannot work well on the Persian QA\n",
    "task [31].\n",
    "• Instruction following. By fine-tuning with a mixture of\n",
    "multi-task datasets formatted via natural language descriptions (called instruction tuning), LLMs are shown to perform\n",
    "well on unseen tasks that are also described in the form\n",
    "of instructions [28, 66, 67]. With instruction tuning, LLMs\n",
    "are enabled to follow the task instructions for new tasks\n",
    "without using explicit examples, thus having an improved\n",
    "generalization ability. According to the experiments in [67],\n",
    "instruction-tuned LaMDA-PT [68] started to significantly\n",
    "outperform the untuned one on unseen tasks when the\n",
    "model size reached 68B, but not for 8B or smaller model\n",
    "sizes. A recent study [69] found that a model size of 62B is\n",
    "at least required for PaLM to perform well on various tasks\n",
    "in four evaluation benchmarks (i.e., MMLU, BBH, TyDiQA\n",
    "and MGSM), though a much smaller size might suffice for\n",
    "some specific tasks (e.g., MMLU).\n",
    "• Step-by-step reasoning. For small language models, it\n",
    "is usually difficult to solve complex tasks that involve\n",
    "multiple reasoning steps, e.g., mathematical word problems.\n",
    "In contrast, with the chain-of-thought (CoT) prompting\n",
    "strategy [33], LLMs can solve such tasks by utilizing the\n",
    "prompting mechanism that involves intermediate reasoning\n",
    "steps for deriving the final answer. This ability is speculated\n",
    "to be potentially obtained by training on code [33, 47]. An\n",
    "empirical study [33] has shown that CoT prompting can\n",
    "bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with\n",
    "a model size larger than 60B, while its advantage over\n",
    "the standard prompting becomes more evident when the\n",
    "model size exceeds 100B. Furthermore, the performance\n",
    "improvement with CoT prompting seems to be also varied\n",
    "for different tasks, e.g., GSM8K > MAWPS > SWAMP for\n",
    "PaLM [33].\n",
    "How Emergent Abilities Relate to Scaling Laws. In existing\n",
    "literature [30, 31, 34], scaling laws and emergent abilities\n",
    "provide two perspectives to understand the advantage of\n",
    "large models over small models. In general, scaling law\n",
    "(often measured by language modeling loss) describes predictable performance relation with the potential effect of\n",
    "diminishing returns, while emergent abilities (often measured by task performance) are unpredictable but very profitable once such abilities actually emerge. Since the two\n",
    "perspectives reflect different performance trends (continuous improvement v.s. sharp performance leap), they might\n",
    "lead to misaligned findings or observations. There are also\n",
    "extensive debates on the rationality of emergent abilities.\n",
    "A popular speculation is that emergent abilities might be\n",
    "partially attributed to the evaluation setting for special tasks\n",
    "(e.g., the discontinuous evaluation metrics) [70, 71]: when\n",
    "evaluation metrics are altered accordingly, the sharpness of\n",
    "the emergent ability curve would disappear. However, the\n",
    "performance of LLMs on most tasks are perceived by users\n",
    "naturally in a discontinuous way. For instance, end users\n",
    "prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a\n",
    "better code with fewer errors between two failed ones. More\n",
    "recently, a study [72] proposes a new evaluation setting\n",
    "that can enlarge the resolution of task metrics, making task\n",
    "performance more predictable. Despite these efforts, more\n",
    "fundamental research (e.g., grokking10) about the working\n",
    "mechanism of LLMs is still in need to understand the emergence of certain abilities. The subtle relation between scaling\n",
    "law and emergent abilities can be explained by analogy with\n",
    "the ability acquisition of human11. Take the speaking ability\n",
    "as an example. For children, language development (especially infants) can be also considered as a multi-level process\n",
    "where “emergent abilities” occur. Specially, the language\n",
    "ability would relatively stable within a time interval, but\n",
    "qualitative change only occurs when evolving into another\n",
    "ability level (e.g., from speaking simple words to speaking\n",
    "simple sentences). Such a learning process is essentially not\n",
    "smooth and stable (i.e., language ability does not develop at\n",
    "a constant rate over time), though a child actually grows\n",
    "every day. It is interesting that young parents would be often\n",
    "surprised by unexpected progress of the speaking ability\n",
    "exhibited by their babies.\n",
    "Key Techniques for LLMs. It has been a long way that\n",
    "LLMs evolve into the current state: general and capable\n",
    "learners. In the development process, a number of important techniques are proposed, which largely improve the\n",
    "capacity of LLMs. Here, we briefly list several important\n",
    "techniques that (potentially) lead to the success of LLMs, as\n",
    "follows.\n",
    "• Scaling. As discussed in previous parts, there exists\n",
    "an evident scaling effect in Transformer language models: larger model/data sizes and more training compute\n",
    "typically lead to an improved model capacity [30, 34]. As\n",
    "two representative models, GPT-3 and PaLM explored the\n",
    "scaling limits by increasing the model size to 175B and\n",
    "540B, respectively. Since compute budget is usually limited,\n",
    "scaling laws can be further employed to conduct a more\n",
    "compute-efficient allocation of the compute resources. For\n",
    "example, Chinchilla (with more training tokens) outperforms its counterpart model Gopher (with a larger model\n",
    "size) by increasing the data scale with the same compute\n",
    "budget [34]. In addition, data scaling should be with careful\n",
    "cleaning process, since the quality of pre-training data plays\n",
    "a key role in the model capacity.\n",
    "• Training. Due to the huge model size, it is very challenging to successfully train a capable LLM. Distributed\n",
    "training algorithms are needed to learn the network parameters of LLMs, in which various parallel strategies are often jointly utilized. To support distributed training, several\n",
    "optimization frameworks have been released to facilitate\n",
    "the implementation and deployment of parallel algorithms,\n",
    "such as DeepSpeed [74] and Megatron-LM [75–77]. Also, optimization tricks are also important for training stability and\n",
    "model performance, e.g., restart to overcome training loss\n",
    "spike [56] and mixed precision training [78]. More recently,\n",
    "GPT-4 [46] proposes to develop special infrastructure and\n",
    "optimization methods that reliably predict the performance\n",
    "of large models with much smaller models.\n",
    "• Ability eliciting. After being pre-trained on large-scale\n",
    "corpora, LLMs are endowed with potential abilities as\n",
    "general-purpose task solvers. These abilities might not be\n",
    "explicitly exhibited when LLMs perform some specific tasks.\n",
    "As the technical approach, it is useful to design suitable task\n",
    "instructions or specific in-context learning strategies to elicit\n",
    "such abilities. For instance, chain-of-thought prompting has\n",
    "been shown to be useful to solve complex reasoning tasks\n",
    "by including intermediate reasoning steps. Furthermore,\n",
    "we can perform instruction tuning on LLMs with task\n",
    "descriptions expressed in natural language, for improving\n",
    "the generalizability of LLMs on unseen tasks. These eliciting\n",
    "techniques mainly correspond to the emergent abilities of\n",
    "LLMs, which may not show the same effect on small language models.\n",
    "• Alignment tuning. Since LLMs are trained to capture\n",
    "the data characteristics of pre-training corpora (including\n",
    "both high-quality and low-quality data), they are likely to\n",
    "generate toxic, biased, or even harmful content for humans.\n",
    "It is necessary to align LLMs with human values, e.g., helpful,\n",
    "honest, and harmless. For this purpose, InstructGPT [66]\n",
    "7\n",
    "designs an effective tuning approach that enables LLMs to\n",
    "follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback [66, 79].\n",
    "It incorporates human in the training loop with elaborately\n",
    "designed labeling strategies. ChatGPT is indeed developed\n",
    "on a similar technique to InstructGPT, which shows a strong\n",
    "alignment capacity in producing high-quality, harmless responses, e.g., rejecting to answer insulting questions.\n",
    "• Tools manipulation. In essence, LLMs are trained as text\n",
    "generators over massive plain text corpora, thus performing\n",
    "less well on the tasks that are not best expressed in the\n",
    "form of text (e.g., numerical computation). In addition, their\n",
    "capacities are also limited to the pre-training data, e.g., the\n",
    "inability to capture up-to-date information. To tackle these\n",
    "issues, a recently proposed technique is to employ external\n",
    "tools to compensate for the deficiencies of LLMs [80, 81].\n",
    "For example, LLMs can utilize the calculator for accurate\n",
    "computation [80] and employ search engines to retrieve\n",
    "unknown information [81]. More recently, ChatGPT has\n",
    "enabled the mechanism of using external plugins (existing\n",
    "or newly created apps)12, which are by analogy with the\n",
    "“eyes and ears” of LLMs. Such a mechanism can broadly\n",
    "expand the scope of capacities for LLMs.\n",
    "In addition, many other factors (e.g., the upgrade of\n",
    "hardware) also contribute to the success of LLMs. Currently,\n",
    "we limit our discussion to the major technical approaches\n",
    "and key findings for developing LLMs.\n",
    "2.2 Technical Evolution of GPT-series Models\n",
    "Due to the excellent capacity in communicating with humans, ChatGPT has ignited the excitement of the AI community since its release. ChatGPT is developed based on the\n",
    "powerful GPT model with specially optimized conversation\n",
    "capacities. Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the\n",
    "technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the\n",
    "past years. Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models\n",
    "in Figure 4. The basic principle underlying GPT models is\n",
    "to compress the world knowledge into the decoder-only\n",
    "Transformer model by language modeling, such that it can\n",
    "recover (or memorize) the semantics of world knowledge\n",
    "and serve as a general-purpose task solver. Two key points\n",
    "to the success are (I) training decoder-only Transformer\n",
    "language models that can accurately predict the next word\n",
    "and (II) scaling up the size of language models. Overall, the\n",
    "research of OpenAI on LLMs can be roughly divided into\n",
    "the following stages13\n",
    ".\n",
    "Early Explorations. According to one interview with Ilya\n",
    "Sutskever14 (a co-founder and chief scientist of OpenAI),\n",
    "the idea of approaching intelligent systems with language\n",
    "models was already explored in the early days of OpenAI, while it was attempted with recurrent neural networks (RNN) [121]. With the advent of Transformer, OpenAI\n",
    "developed two initial GPT models, namely GPT-1 [122] and\n",
    "GPT-2 [26], which can be considered as the foundation to\n",
    "more powerful models subsequently i.e., GPT-3 and GPT-4.\n",
    "• GPT-1. In 2017, the Transformer model [22] was introduced by Google, and the OpenAI team quickly adapted\n",
    "their language modeling work to this new neural network\n",
    "architecture. They released the first GPT model in 2018,\n",
    "i.e., GPT-1 [122], and coined the abbreviation term GPT\n",
    "as the model name, standing for Generative Pre-Training.\n",
    "GPT-1 was developed based on a generative, decoder-only\n",
    "Transformer architecture, and adopted a hybrid approach of\n",
    "unsupervised pretraining and supervised fine-tuning. GPT1 has set up the core architecture for the GPT-series models\n",
    "and established the underlying principle to model natural\n",
    "language text, i.e., predicting the next word.\n",
    "• GPT-2. Following a similar architecture of GPT-1,\n",
    "GPT-2 [26] increased the parameter scale to 1.5B, which\n",
    "was trained with a large webpage dataset WebText. As\n",
    "claimed in the paper of GPT-2, it sought to perform\n",
    "tasks via unsupervised language modeling, without explicit\n",
    "fine-tuning using labeled data. To motivate the approach,\n",
    "they introduced a probabilistic form for multi-task solving,\n",
    "i.e., p(output|input, task) (similar approaches have been\n",
    "adopted in [123]), which predicts the output conditioned on\n",
    "the input and task information. To model this conditional\n",
    "probability, language text can be naturally employed as a\n",
    "unified way to format input, output and task information.\n",
    "In this way, the process of solving a task can be cast as a\n",
    "word prediction problem for generating the solution text.\n",
    "Further, they introduced a more formal claim for this idea:\n",
    "“Since the (task-specific) supervised objective is the same\n",
    "as the unsupervised (language modeling) objective but only\n",
    "evaluated on a subset of the sequence, the global minimum\n",
    "of the unsupervised objective is also the global minimum\n",
    "of the supervised objective (for various tasks)” [26]15. A\n",
    "basic understanding of this claim is that each (NLP) task\n",
    "can be considered as the word prediction problem based\n",
    "on a subset of the world text. Thus, unsupervised language\n",
    "modeling could be capable in solving various tasks, if it was\n",
    "trained to have sufficient capacity in recovering the world\n",
    "text. These early discussion in GPT-2’s paper echoed in the\n",
    "interview of Ilya Sutskever by Jensen Huang: “What the\n",
    "neural network learns is some representation of the process\n",
    "that produced the text. This text is actually a projection of\n",
    "the world...the more accurate you are in predicting the next\n",
    "word, the higher the fidelity, the more resolution you get in\n",
    "this process...”16\n",
    ".\n",
    "Capacity Leap. Although GPT-2 is intended to be an “unsupervised multitask learner”, it overall has an inferior\n",
    "performance compared with supervised fine-tuning stateof-the-art methods. Because it has a relatively small model\n",
    "size, it has been widely fine-tuned in downstream tasks,\n",
    "especially the dialog tasks [124, 125]. Based on GPT-2, GPT-3\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "efafdf97-6f7b-415d-b5ca-d7dcba9dd045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 36226\n",
      "num chunk: 19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "split chunks\n",
    "\"\"\"\n",
    "\n",
    "text = ori_text#.replace('\\n', '')\n",
    "matches = re.finditer(r'\\n', text)\n",
    "print(\"text length:\", len(text))\n",
    "\n",
    "max_chunk_length = 2048\n",
    "result_chunks = []\n",
    "\n",
    "left = 0\n",
    "right = 0\n",
    "match = -1\n",
    "while(match is not None):\n",
    "    try:\n",
    "        match = next(matches)\n",
    "        midx = match.start()\n",
    "    except:\n",
    "        match = None\n",
    "        midx = len(text)\n",
    "    \n",
    "    if midx - left < max_chunk_length:\n",
    "        right = midx\n",
    "    else:\n",
    "        # text split is longer than max_chuck_length\n",
    "        if left >= right:\n",
    "            while(midx - left >= max_chunk_length):\n",
    "                chunk = text[left:left+max_chunk_length]\n",
    "                left += max_chunk_length\n",
    "                right += max_chunk_length\n",
    "                result_chunks.append(chunk)\n",
    "            right = midx\n",
    "        chunk = text[left:right+1]\n",
    "        left = right+1\n",
    "        right = midx\n",
    "        result_chunks.append(chunk)\n",
    "        \n",
    "if left < right: # get last split\n",
    "    chunk = text[left:right+1]\n",
    "    result_chunks.append(chunk)\n",
    "    \n",
    "print(\"num chunk:\", len(result_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "b2b0d2a6-eb92-4f99-b756-03ad92b92200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 INTRODUCTION\n",
      "2 OVERVIEW\n",
      "2.1 Background for LLMs\n",
      "2.2 Technical Evolution of GPT-series Models\n",
      "\n",
      "CPU times: user 65.7 ms, sys: 23.6 ms, total: 89.3 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "extract ToC\n",
    "\"\"\"\n",
    "\n",
    "api_server_url = \"http://localhost:21122\"\n",
    "instruct = (\n",
    "    \"You are a Table of Contents extractor. \"\n",
    "    \"User will speak to you questions. \"\n",
    "    \"You must reply only with [목차(Table of Contents)] part extracted from the questions. \"\n",
    "    \"You must keep original text. Do not change original text. \"\n",
    "    \"And you must not involve [dotted line, page number, 제목(title), 참고문헌(reference), 부록(appendix), content, explanation, summary, predicted]. \"\n",
    "    \"When there is no Table of Contents, you must reply with \\\"없음\\\". \"\n",
    "    \"do not write explanations.\"\n",
    ")\n",
    "prompt_template = \"\"\"### System:\n",
    "{instruct}\n",
    "### User: {data}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "def send_extract_toc(tocs):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_data - 1:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = result_chunks[pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_data}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        # response\n",
    "        prompt = prompt_template.format(\n",
    "            instruct=instruct,\n",
    "            data=data\n",
    "        )\n",
    "        input_json = {\n",
    "            \"model_name\": \"MDIEM-toc3\", #\"OLLM-Small_2024.01.16\", #\"MDIEM-toc3\",\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.8,\n",
    "            \"max_new_tokens\": 512,\n",
    "        }\n",
    "\n",
    "        ret = requests.post(\n",
    "            api_server_url + \"/worker_generate\",\n",
    "            json=input_json,\n",
    "            timeout=30,\n",
    "        )\n",
    "\n",
    "        result = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "        tocs[pidx] = result\n",
    "\n",
    "tocs = [\"\"] * len(result_chunks)\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_data = len(result_chunks)\n",
    "n_thread = 16\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_extract_toc, args=(tocs,)) # \n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "    \n",
    "first_toc = '\\n'.join(tocs)\n",
    "# response\n",
    "prompt = prompt_template.format(\n",
    "    instruct=instruct,\n",
    "    data=first_toc\n",
    ")\n",
    "input_json = {\n",
    "    \"model_name\": \"MDIEM-toc3\", #\"OLLM-Small_2024.01.16\", # \n",
    "    \"prompt\": prompt,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "}\n",
    "\n",
    "ret = requests.post(\n",
    "    api_server_url + \"/worker_generate\",\n",
    "    json=input_json,\n",
    "    timeout=30,\n",
    ")\n",
    "\n",
    "second_toc = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "print(second_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7922757-0708-4006-881a-d5c5293be566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "405f3928-f86a-42c2-b011-5a4cef5bd418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 INTRODUCTION [0]\n",
      "2 OVERVIEW [13829]\n",
      "2.1 Background for LLMs [13978]\n",
      "2.2 Technical Evolution of GPT-series Models [31719]\n",
      "[('1 INTRODUCTION', 0), ('2 OVERVIEW', 13829), ('2.1 Background for LLMs', 13978), ('2.2 Technical Evolution of GPT-series Models', 31719)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "make ToC-Contents Groups\n",
    "\"\"\"\n",
    "\n",
    "toc_string = re.sub(r'\\n+', '\\n', second_toc)\n",
    "parts = toc_string.split('\\n')\n",
    "\n",
    "MIN_SPLIT_LEN = 1\n",
    "\n",
    "def remove_empty(parts):\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        if len(part) <= MIN_SPLIT_LEN: continue\n",
    "        new_parts.append(part)\n",
    "    return new_parts\n",
    "\n",
    "parts = remove_empty(parts)\n",
    "\n",
    "parts_indexs = []\n",
    "for part in parts:\n",
    "    matched = [match.start() for match in re.finditer(part, ori_text)]\n",
    "    for match in matched:\n",
    "        parts_indexs.append((part, match))\n",
    "    print(part, matched)\n",
    "    \n",
    "parts_indexs = sorted(parts_indexs, key=lambda x: x[1])\n",
    "print(parts_indexs)\n",
    "\n",
    "toc_dict = {}\n",
    "for pdata1, pdata2 in zip(parts_indexs, parts_indexs[1:] + [('', len(ori_text)-1)]):\n",
    "    name, left = pdata1\n",
    "    left += len(name)\n",
    "    _, right = pdata2\n",
    "    \n",
    "    if right - left > MIN_SPLIT_LEN:\n",
    "        if name not in toc_dict:\n",
    "            toc_dict[name] = [ori_text[left:right]]\n",
    "        else:\n",
    "            toc_dict[name].append(ori_text[left:right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "deaa40a5-d0d3-4ed0-9a04-fc669eb9970d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.7 ms, sys: 3.94 ms, total: 25.7 ms\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Summary ToC-Contents Groups\n",
    "\"\"\"\n",
    "\n",
    "MIN_CONTENT_LEN = 10\n",
    "\n",
    "api_server_url = \"http://localhost:21122\"\n",
    "# I want you to act as a summaryist. I will give you sentences. \"\n",
    "# f\"I want you to only reply with a summarization based on the sentences. do not write explanations.\n",
    "# \"do not write explanations. summarize the sentences as briefly as possible: {query}\"\n",
    "instruct = (\n",
    "    \"do not write explanations. 다음 문장을 한글로 번역하고 최대한 짧게 요약해주세요: {query}\"\n",
    ")\n",
    "prompt_template = \"\"\"### System:\n",
    "This is a system prompt, please behave and help the user.\n",
    "\n",
    "### User: {instruct}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "def send_summary(summaries):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_data - 1:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = toc_dict.get(parts[pidx])\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_data}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        if data is None:\n",
    "            continue\n",
    "        data = '\\n'.join(data)\n",
    "        if len(data) < MIN_CONTENT_LEN:\n",
    "            # summaries[pidx] = data\n",
    "            continue\n",
    "        \n",
    "        # response\n",
    "        prompt = prompt_template.format(\n",
    "            instruct=instruct.format(query=data),\n",
    "        )\n",
    "        input_json = {\n",
    "            \"model_name\": \"OLLM-Large_2023.11.20\",# \"MDIEM-toc3\",\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_p\": 0.7,\n",
    "            \"max_new_tokens\": 1024,\n",
    "        }\n",
    "\n",
    "        ret = requests.post(\n",
    "            api_server_url + \"/worker_generate\",\n",
    "            json=input_json,\n",
    "            timeout=300,\n",
    "        )\n",
    "\n",
    "        result = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "        summaries[pidx] = result\n",
    "\n",
    "summaries = [\"\"] * len(parts)\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_data = len(parts)\n",
    "n_thread = 16\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_summary, args=(summaries,)) # \n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "2613999d-0839-4a1c-a50d-c0f5260f8b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 INTRODUCTION \n",
      " The given text is a summary of a research paper that discusses the development and applications of large language models (LLMs). The paper highlights the differences between LLMs and previous generations of language models, such as statistical language models (SLMs), neural language models (NLMs), and pre-trained language models (PLMs). It emphasizes the significant impact of LLMs on various domains, including natural language processing (NLP), information retrieval (IR), and computer vision (CV). The paper also discusses the challenges associated with LLMs, such as the lack of understanding of their emergent abilities, the difficulty in training them, and the need for aligning their output with human values. The paper concludes with a discussion of practical guidelines for prompt design and a review of LLM applications in different domains. \n",
      "\n",
      "2 OVERVIEW \n",
      " 이 섹션에서는 LLM의 배경에 대한 개요를 제공한 다음 GPT-시리즈 모델의 기술적 진화를 요약합니다. \n",
      "\n",
      "2.1 Background for LLMs \n",
      "  \n",
      "\n",
      "2.2 Technical Evolution of GPT-series Models \n",
      " ChatGPT has gained popularity due to its strong ability to communicate with humans. Developed based on the powerful GPT model, it has specialized capabilities for human-like conversations. The progress of GPT-series models is discussed, including the key points of using decoder-only Transformer models and increasing their size. The research on language modeling can be divided into stages, including early explorations, GPT-1 and GPT-2. GPT-1 laid the foundation for later models, using a hybrid approach of pretraining and fine-tuning. GPT-2 increased the parameter scale and focused on unsupervised language modeling. The idea of unsupervised multitask learning was introduced, but performance was inferior to supervised methods. GPT-3 and GPT-4 followed, with a significant leap in model capacity. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(parts)):\n",
    "    print(parts[idx], '\\n', summaries[idx], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05b6ea-2b4d-442b-887a-9fe080bbfed8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for part in parts:\n",
    "    if part in toc_dict:\n",
    "        print(part, '\\n', len(toc_dict[part]), '\\n', toc_dict[part], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "f0b60879-54f5-4c38-b505-31b6b64cf237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/Platypus2-70B-instruct/\"\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/vicuna-13b-v1.5/\"\n",
    "# model_path = \"/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.42_2_dpo-GPTQ/\"\n",
    "model_path = \"/data/llm_weights/gptq/CodeLlama-70B-Instruct-GPTQ/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    model_max_length=16384,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "import torch\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "849d6214-efd7-42a0-baf0-a8d79b4e2ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> hi'"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"hi\")['input_ids']\n",
    "# input_ids\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "08954f5d-bcda-483e-b695-15022f5ce8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>hi'"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"hi\")['input_ids']\n",
    "# input_ids\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "319b1bb2-2ef0-4698-815d-b9c3f8b198e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60131434.59915612"
      ]
     },
     "execution_count": 958,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1425115*100/2.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7687275-4227-46f4-9d0a-5a289506418a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat_2024jan",
   "language": "python",
   "name": "fastchat_2024jan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
