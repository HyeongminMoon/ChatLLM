{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169371e5-3819-4b0c-93b0-760c42f104d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"\"\"나경민의 주루사 때 카메라에 잡혔던 어느 롯데 팬의 절규가 큰 화제가 되었다.[14] 모래반지 빵야빵야처럼 파레이돌리아가 가능하다.\n",
    "\n",
    "당시 나온 드립만 해도 가지각색이다.\n",
    "- 야\n",
    "\n",
    "이후 저 팬은 나경민에게 사인 유니폼을 받았다고 한다. 게다가 옷에 달고 있던 TWICE의 두 번째 투어 콘서트 'TWICELAND ZONE 2 : Fantasy Park'의 공식 굿즈인 캐릭터 핀 버튼[18]을 유니폼에 달고 있던 모습 때문에 ONCE들 사이에서도 유명세를 탔다.\n",
    "\"\"\"\n",
    "output = \"\"\"없음\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8449ad-cd14-4f18-9e72-f1e254f3362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = (\n",
    "    \"You are a Table of Contents extractor. \"\n",
    "    \"User will speak to you questions. \"\n",
    "    \"You must reply only with [목차(Table of Contents)] part extracted from the questions. \"\n",
    "    \"You must keep original text. Do not change original text.\"\n",
    "    \"And you must not involve [dotted line, page number, 제목(title), content, explanation, summary, predicted]. \"\n",
    "    \"do not write explanations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147b8fd-d024-4615-ad15-7c6a61378ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "conv = get_conversation_template(\"chat-orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573da1d3-efcb-4880-99f4-e0d55a21c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split chunks\n",
    "\"\"\"\n",
    "\n",
    "text = ori_text#.replace('\\n', '')\n",
    "matches = re.finditer(r'\\n', text)\n",
    "print(\"text length:\", len(text))\n",
    "\n",
    "max_chunk_length = 2048\n",
    "result_chunks = []\n",
    "\n",
    "left = 0\n",
    "right = 0\n",
    "match = -1\n",
    "while(match is not None):\n",
    "    try:\n",
    "        match = next(matches)\n",
    "        midx = match.start()\n",
    "    except:\n",
    "        match = None\n",
    "        midx = len(text)\n",
    "    \n",
    "    if midx - left < max_chunk_length:\n",
    "        right = midx\n",
    "    else:\n",
    "        # text split is longer than max_chuck_length\n",
    "        if left >= right:\n",
    "            while(midx - left >= max_chunk_length):\n",
    "                chunk = text[left:left+max_chunk_length]\n",
    "                left += max_chunk_length\n",
    "                right += max_chunk_length\n",
    "                result_chunks.append(chunk)\n",
    "            right = midx\n",
    "        chunk = text[left:right+1]\n",
    "        left = right+1\n",
    "        right = midx\n",
    "        result_chunks.append(chunk)\n",
    "        \n",
    "if left < right: # get last split\n",
    "    chunk = text[left:right+1]\n",
    "    result_chunks.append(chunk)\n",
    "    \n",
    "print(\"num chunk:\", len(result_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330efb42-67ac-4808-894b-621af44fba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "extract ToC\n",
    "\"\"\"\n",
    "\n",
    "api_server_url = \"http://localhost:21122\"\n",
    "instruct = (\n",
    "    \"You are a Table of Contents extractor. \"\n",
    "    \"User will speak to you questions. \"\n",
    "    \"You must reply only with [목차(Table of Contents)] part extracted from the questions. \"\n",
    "    \"You must keep original text. Do not change original text. \"\n",
    "    \"And you must not involve [dotted line, page number, 제목(title), 참고문헌(reference), 부록(appendix), content, explanation, summary, predicted]. \"\n",
    "    \"When there is no Table of Contents, you must reply with \\\"없음\\\". \"\n",
    "    \"do not write explanations.\"\n",
    ")\n",
    "prompt_template = \"\"\"### System:\n",
    "{instruct}\n",
    "### User: {data}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "def send_extract_toc(tocs):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_data - 1:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = result_chunks[pidx]\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_data}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        # response\n",
    "        prompt = prompt_template.format(\n",
    "            instruct=instruct,\n",
    "            data=data\n",
    "        )\n",
    "        input_json = {\n",
    "            \"model_name\": \"MDIEM-toc3\", #\"OLLM-Small_2024.01.16\", #\"MDIEM-toc3\",\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.8,\n",
    "            \"max_new_tokens\": 512,\n",
    "        }\n",
    "\n",
    "        ret = requests.post(\n",
    "            api_server_url + \"/worker_generate\",\n",
    "            json=input_json,\n",
    "            timeout=30,\n",
    "        )\n",
    "\n",
    "        result = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "        tocs[pidx] = result\n",
    "\n",
    "tocs = [\"\"] * len(result_chunks)\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_data = len(result_chunks)\n",
    "n_thread = 16\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_extract_toc, args=(tocs,)) # \n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "    \n",
    "first_toc = '\\n'.join(tocs)\n",
    "# response\n",
    "prompt = prompt_template.format(\n",
    "    instruct=instruct,\n",
    "    data=first_toc\n",
    ")\n",
    "input_json = {\n",
    "    \"model_name\": \"MDIEM-toc3\", #\"OLLM-Small_2024.01.16\", # \n",
    "    \"prompt\": prompt,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "}\n",
    "\n",
    "ret = requests.post(\n",
    "    api_server_url + \"/worker_generate\",\n",
    "    json=input_json,\n",
    "    timeout=30,\n",
    ")\n",
    "\n",
    "second_toc = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "print(second_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d63cf-b260-4b7d-b852-57ebab503d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff65e5-088a-41ca-813e-bd01311ad5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make ToC-Contents Groups\n",
    "\"\"\"\n",
    "\n",
    "toc_string = re.sub(r'\\n+', '\\n', second_toc)\n",
    "parts = toc_string.split('\\n')\n",
    "\n",
    "MIN_SPLIT_LEN = 1\n",
    "\n",
    "def remove_empty(parts):\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        if len(part) <= MIN_SPLIT_LEN: continue\n",
    "        new_parts.append(part)\n",
    "    return new_parts\n",
    "\n",
    "parts = remove_empty(parts)\n",
    "\n",
    "parts_indexs = []\n",
    "for part in parts:\n",
    "    matched = [match.start() for match in re.finditer(part, ori_text)]\n",
    "    for match in matched:\n",
    "        parts_indexs.append((part, match))\n",
    "    print(part, matched)\n",
    "    \n",
    "parts_indexs = sorted(parts_indexs, key=lambda x: x[1])\n",
    "print(parts_indexs)\n",
    "\n",
    "toc_dict = {}\n",
    "for pdata1, pdata2 in zip(parts_indexs, parts_indexs[1:] + [('', len(ori_text)-1)]):\n",
    "    name, left = pdata1\n",
    "    left += len(name)\n",
    "    _, right = pdata2\n",
    "    \n",
    "    if right - left > MIN_SPLIT_LEN:\n",
    "        if name not in toc_dict:\n",
    "            toc_dict[name] = [ori_text[left:right]]\n",
    "        else:\n",
    "            toc_dict[name].append(ori_text[left:right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef1968-a776-42b3-9bad-5f4035b8028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Summary ToC-Contents Groups\n",
    "\"\"\"\n",
    "\n",
    "MIN_CONTENT_LEN = 10\n",
    "\n",
    "api_server_url = \"http://localhost:21122\"\n",
    "# I want you to act as a summaryist. I will give you sentences. \"\n",
    "# f\"I want you to only reply with a summarization based on the sentences. do not write explanations.\n",
    "# \"do not write explanations. summarize the sentences as briefly as possible: {query}\"\n",
    "instruct = (\n",
    "    \"do not write explanations. 다음 문장을 한글로 번역하고 최대한 짧게 요약해주세요: {query}\"\n",
    ")\n",
    "prompt_template = \"\"\"### System:\n",
    "This is a system prompt, please behave and help the user.\n",
    "\n",
    "### User: {instruct}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "def send_summary(summaries):\n",
    "    global idx\n",
    "    # for _ in range(2):\n",
    "    while(1):\n",
    "        if idx > len_data - 1:\n",
    "            break\n",
    "        lock.acquire()\n",
    "        pidx = idx\n",
    "        data = toc_dict.get(parts[pidx])\n",
    "        idx += 1\n",
    "        lock.release()\n",
    "        \n",
    "        print(f\"{idx}/{len_data}\", '\\t\\t\\t\\t\\t\\t', end='\\r')#\n",
    "        \n",
    "        if data is None:\n",
    "            continue\n",
    "        data = '\\n'.join(data)\n",
    "        if len(data) < MIN_CONTENT_LEN:\n",
    "            # summaries[pidx] = data\n",
    "            continue\n",
    "        \n",
    "        # response\n",
    "        prompt = prompt_template.format(\n",
    "            instruct=instruct.format(query=data),\n",
    "        )\n",
    "        input_json = {\n",
    "            \"model_name\": \"OLLM-Large_2023.11.20\",# \"MDIEM-toc3\",\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_p\": 0.7,\n",
    "            \"max_new_tokens\": 1024,\n",
    "        }\n",
    "\n",
    "        ret = requests.post(\n",
    "            api_server_url + \"/worker_generate\",\n",
    "            json=input_json,\n",
    "            timeout=300,\n",
    "        )\n",
    "\n",
    "        result = ret.json()['text'][len(input_json['prompt'])+1:]\n",
    "        summaries[pidx] = result\n",
    "\n",
    "summaries = [\"\"] * len(parts)\n",
    "threads = []\n",
    "idx = 0\n",
    "lock = threading.Lock()\n",
    "len_data = len(parts)\n",
    "n_thread = 16\n",
    "\n",
    "for i in range(n_thread):\n",
    "    t = threading.Thread(target=send_summary, args=(summaries,)) # \n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710bedd-2e7c-4ec3-bbab-354e799bd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(parts)):\n",
    "    print(parts[idx], '\\n', summaries[idx], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3a498-28d7-49cf-8db6-c396c0585def",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in parts:\n",
    "    if part in toc_dict:\n",
    "        print(part, '\\n', len(toc_dict[part]), '\\n', toc_dict[part], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat_2024jan",
   "language": "python",
   "name": "fastchat_2024jan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
