{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4112b421-896b-44e5-bf94-5c4d3ed768b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ados/anaconda3/envs/fastchat_2023dec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model /data/llm_weights/custom_trained/DIE-70B_sftv5_task-96000/ into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:13<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded base_model\n",
      "loaded lora model\n",
      "merged model\n",
      "trained dummy\n",
      "Successfully saved the model /workspaces/temp_test_models/DIE-70B_sftv5_daily\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    "    LlamaForCausalLM,\n",
    "    GPTNeoXForCausalLM,\n",
    ")\n",
    "\n",
    "# gpt_evol koalpaca lima alpaca-gpt4 korquad summary_tech sharegpt_V3_selected sharegpt_gpt4 koen enko\n",
    "\n",
    "# numbers = [18744]\n",
    "# for number in numbers:\n",
    "model_path = \"/data/llm_weights/custom_trained/DIE-70B_sftv5_task-96000/\"\n",
    "# model_path = \"/workspaces/disk0/data/llm_weights/kiqu-70b/\"\n",
    "# adapters_path = f\"../../yarn/output/MingAI-70B-chat-orca_v0_42_2_dpo-yarn-32k/\"\n",
    "adapters_path = f\"../runs/DIE-70B_sftv5_daily/\"\n",
    "# save_path = f\"/workspaces/data/llm_weights/custom_trained/DIE-70B_sftv5_task-{number}\"\n",
    "save_path = f\"/workspaces/temp_test_models/DIE-70B_sftv5_daily\"\n",
    "\n",
    "print(f\"Starting to load the model {model_path} into memory\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map={\"\": \"cpu\"},\n",
    "    # max_memory = {i: '81920MB' for i in range(torch.cuda.device_count())},\n",
    "    # local_files_only=True\n",
    ")\n",
    "print(f\"loaded base_model\")\n",
    "first_weight = base_model.model.layers[0].self_attn.q_proj.weight\n",
    "first_weight_old = first_weight.clone()\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapters_path,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(f\"loaded lora model\")\n",
    "\n",
    "lora_weight = lora_model.base_model.model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "assert torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "# merge weights - new merging method from peft\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "print(f\"merged model\")\n",
    "\n",
    "lora_model.train(False)\n",
    "print(f\"trained dummy\")\n",
    "\n",
    "# did we do anything?\n",
    "assert not torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "lora_model_sd = lora_model.state_dict()\n",
    "deloreanized_sd = {\n",
    "    k.replace(\"base_model.model.\", \"\"): v\n",
    "    for k, v in lora_model_sd.items()\n",
    "    if \"lora\" not in k\n",
    "}\n",
    "\n",
    "LlamaForCausalLM.save_pretrained(\n",
    "    base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    ")\n",
    "\n",
    "# GPTNeoXForCausalLM.save_pretrained(\n",
    "#     base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    "# )\n",
    "\n",
    "print(f\"Successfully saved the model {save_path}\")\n",
    "\n",
    "# tokenizer 옮기는 코드 추가하기\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "    \"vocab.json\",\n",
    "    \"merges.txt\",\n",
    "]:\n",
    "    src = os.path.join(model_path, f_path)\n",
    "    dst = os.path.join(save_path, f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea4cf47-b6a3-4907-bd92-e8c8e30872a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model /data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_ep3 into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model /workspaces/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_dpo_ep3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    "    LlamaForCausalLM,\n",
    "    GPTNeoXForCausalLM,\n",
    ")\n",
    "\n",
    "# gpt_evol koalpaca lima alpaca-gpt4 korquad summary_tech sharegpt_V3_selected sharegpt_gpt4 koen enko\n",
    "\n",
    "model_path = \"/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_ep3\"\n",
    "adapters_path = f\"../runs/M-DIE-M-10.7B_gpt4_dpo\"\n",
    "save_path = f\"/workspaces/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_dpo_ep3\"\n",
    "\n",
    "print(f\"Starting to load the model {model_path} into memory\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    # max_memory = {i: '81920MB' for i in range(torch.cuda.device_count())},\n",
    "    # local_files_only=True\n",
    ")\n",
    "\n",
    "first_weight = base_model.model.layers[0].self_attn.q_proj.weight\n",
    "first_weight_old = first_weight.clone()\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapters_path,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "lora_weight = lora_model.base_model.model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "assert torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "# merge weights - new merging method from peft\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "\n",
    "lora_model.train(False)\n",
    "\n",
    "# did we do anything?\n",
    "assert not torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "lora_model_sd = lora_model.state_dict()\n",
    "deloreanized_sd = {\n",
    "    k.replace(\"base_model.model.\", \"\"): v\n",
    "    for k, v in lora_model_sd.items()\n",
    "    if \"lora\" not in k\n",
    "}\n",
    "\n",
    "LlamaForCausalLM.save_pretrained(\n",
    "    base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    ")\n",
    "\n",
    "# GPTNeoXForCausalLM.save_pretrained(\n",
    "#     base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    "# )\n",
    "\n",
    "print(f\"Successfully saved the model {save_path}\")\n",
    "\n",
    "# tokenizer 옮기는 코드 추가하기\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_path, f_path)\n",
    "    dst = os.path.join(save_path, f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929595d2-b11f-45e4-8aa1-e46bf7d1aa6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model /data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_ep3 into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model /workspaces/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_dpo_ep4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    "    LlamaForCausalLM,\n",
    "    GPTNeoXForCausalLM,\n",
    ")\n",
    "\n",
    "# gpt_evol koalpaca lima alpaca-gpt4 korquad summary_tech sharegpt_V3_selected sharegpt_gpt4 koen enko\n",
    "\n",
    "model_path = \"/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_ep3\"\n",
    "adapters_path = f\"../runs/M-DIE-M-10.7B_gpt4_dpo/checkpoint-14668/\"\n",
    "save_path = f\"/workspaces/data/llm_weights/custom_trained/M-DIE-M-10.7B_gpt4_dpo_ep4\"\n",
    "\n",
    "print(f\"Starting to load the model {model_path} into memory\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    # max_memory = {i: '81920MB' for i in range(torch.cuda.device_count())},\n",
    "    # local_files_only=True\n",
    ")\n",
    "\n",
    "first_weight = base_model.model.layers[0].self_attn.q_proj.weight\n",
    "first_weight_old = first_weight.clone()\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapters_path,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "lora_weight = lora_model.base_model.model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "assert torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "# merge weights - new merging method from peft\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "\n",
    "lora_model.train(False)\n",
    "\n",
    "# did we do anything?\n",
    "assert not torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "lora_model_sd = lora_model.state_dict()\n",
    "deloreanized_sd = {\n",
    "    k.replace(\"base_model.model.\", \"\"): v\n",
    "    for k, v in lora_model_sd.items()\n",
    "    if \"lora\" not in k\n",
    "}\n",
    "\n",
    "LlamaForCausalLM.save_pretrained(\n",
    "    base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    ")\n",
    "\n",
    "# GPTNeoXForCausalLM.save_pretrained(\n",
    "#     base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    "# )\n",
    "\n",
    "print(f\"Successfully saved the model {save_path}\")\n",
    "\n",
    "# tokenizer 옮기는 코드 추가하기\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_path, f_path)\n",
    "    dst = os.path.join(save_path, f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b658a-2741-48e3-a497-09e13388611d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    \n",
    "print(\"model loading...\")\n",
    " \n",
    "# Model & Tokenizer loading\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/llm_weights/merged/M-DIE-M-10.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data/llm_weights/merged/M-DIE-M-10.7B\")\n",
    " \n",
    "# Repository 생성 & model upload\n",
    "REPO_NAME = \"M-DIE-M-10.7B\" # ex) 'my-bert-fine-tuned'\n",
    "AUTH_TOKEN = \"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\" # <https://huggingface.co/settings/token>\n",
    " \n",
    "## Upload to Huggingface Hub\n",
    "model.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c139e9b-4b14-44ee-986b-80495b57aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '/disk1/data/llm_weights/gptq/MingAI-70B-chat-orca_v0.5a-GPTQ/'\n",
    "model_path = \"runs/MingAI-70B-chat-orca_v0.5a-retrained/checkpoint-10/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cef0cdb6-c562-4cdd-bcfb-13ddab54a085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1c8bb71-c966-41a1-9f88-d61b5e5ea9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9313d75-0f2d-44d3-b408-b52e102d76c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.4_v0.2_retrained_checkpoint-2//', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040964a-cafb-455f-a866-02c1048b5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model /disk1/data/llm_weights/llama-2-ko-70b/ into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 49/49 [00:19<00:00,  2.57it/s]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /disk1/data/llm_weights/llama-2-ko-70b/ and are newly initialized: ['model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.41.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.59.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.58.self_attn.rotary_emb.inv_freq', 'model.layers.77.self_attn.rotary_emb.inv_freq', 'model.layers.74.self_attn.rotary_emb.inv_freq', 'model.layers.69.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.44.self_attn.rotary_emb.inv_freq', 'model.layers.71.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.48.self_attn.rotary_emb.inv_freq', 'model.layers.66.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.46.self_attn.rotary_emb.inv_freq', 'model.layers.60.self_attn.rotary_emb.inv_freq', 'model.layers.47.self_attn.rotary_emb.inv_freq', 'model.layers.65.self_attn.rotary_emb.inv_freq', 'model.layers.56.self_attn.rotary_emb.inv_freq', 'model.layers.53.self_attn.rotary_emb.inv_freq', 'model.layers.61.self_attn.rotary_emb.inv_freq', 'model.layers.45.self_attn.rotary_emb.inv_freq', 'model.layers.52.self_attn.rotary_emb.inv_freq', 'model.layers.78.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.63.self_attn.rotary_emb.inv_freq', 'model.layers.73.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.55.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.51.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.62.self_attn.rotary_emb.inv_freq', 'model.layers.54.self_attn.rotary_emb.inv_freq', 'model.layers.72.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.67.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.75.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.68.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.43.self_attn.rotary_emb.inv_freq', 'model.layers.40.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.42.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.79.self_attn.rotary_emb.inv_freq', 'model.layers.57.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.49.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.76.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.50.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.70.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.64.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model /disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.5a-checkpoint-44362.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/disk1/data/llm_weights/llama-2-ko-70b/\"\n",
    "adapters_path = \"/home/ados/workspaces/FastChat_train/runs/MingAI-70B-chat-orca_v0.5a/checkpoint-44362\"\n",
    "save_path = (\n",
    "    \"/disk1/data/llm_weights/custom_trained/MingAI-70B-chat-orca_v0.5a-checkpoint-44362\"\n",
    ")\n",
    "\n",
    "print(f\"Starting to load the model {model_path} into memory\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    # max_memory = {i: '81920MB' for i in range(torch.cuda.device_count())},\n",
    "    # local_files_only=True\n",
    ")\n",
    "\n",
    "first_weight = base_model.model.layers[0].self_attn.q_proj.weight\n",
    "first_weight_old = first_weight.clone()\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapters_path,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "lora_weight = lora_model.base_model.model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "assert torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "# merge weights - new merging method from peft\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "\n",
    "lora_model.train(False)\n",
    "\n",
    "# did we do anything?\n",
    "assert not torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "lora_model_sd = lora_model.state_dict()\n",
    "deloreanized_sd = {\n",
    "    k.replace(\"base_model.model.\", \"\"): v\n",
    "    for k, v in lora_model_sd.items()\n",
    "    if \"lora\" not in k\n",
    "}\n",
    "\n",
    "LlamaForCausalLM.save_pretrained(\n",
    "    base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    ")\n",
    "\n",
    "# GPTNeoXForCausalLM.save_pretrained(\n",
    "#     base_model, save_path, state_dict=deloreanized_sd, max_shard_size=\"10GB\"\n",
    "# )\n",
    "\n",
    "print(f\"Successfully saved the model {save_path}.\")\n",
    "\n",
    "# tokenizer 옮기는 코드 추가하기\n",
    "import shutil\n",
    "\n",
    "for f_path in [\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "]:\n",
    "    src = os.path.join(model_path, f_path)\n",
    "    dst = os.path.join(save_path, f_path)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fd6d4c7-d3bf-4887-92fc-a4ac596ff4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model, tokenizer, x):\n",
    "    q = f\"### System:\\nThis is a system prompt, please behave and help the user.\\n\\n### User: {x}\\n\\n### Assistant:\"\n",
    "    # print(q)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(q, return_tensors=\"pt\", return_token_type_ids=False),  # .to('cuda')\n",
    "        max_new_tokens=50,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(tokenizer.decode(gened[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2380ba-d667-42f7-b3cb-0b40c0a25743",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(lora_model, tokenizer, \"건강하게 살기 위한 세 가지 방법은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3db83702-40f0-4b8d-a167-9d00c33c86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"/disk1/data/llm_weights/polyglot-ko-12.8b-pad/\"\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={\"\": 'cpu'},\n",
    "# )\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\": \"cpu\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5c22556-61cf-4950-a934-82ed57c51f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/disk1/data/llm_weights/polyglot-ko-12.8b-pad/', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'unk_token': '<|unused1|>', 'pad_token': '<|unused1|>', 'additional_special_tokens': ['<|endoftext|>', '<|sep|>', '<|acc|>', '<|tel|>', '<|rrn|>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f44b89c-7f84-423e-a8e5-1430c32c41ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unused1|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584eedcd-bac5-494a-bd6c-9c85fa60591b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unused1|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fde06114-630e-4c24-a0eb-f8dfd7473fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/disk1/data/llm_weights/Llama-2-13b-hf/\"\n",
    "\n",
    "# llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={\"\": 'cpu'},\n",
    "# )\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\": \"cpu\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00642e4-8268-4f89-a873-bfc6e3770cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='/disk1/data/llm_weights/Llama-2-13b-hf/', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f484ab8-3507-4f0a-a939-cfb87d3ceb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "llama_tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0967c3c7-1a2d-4b1c-8c48-219bd0056711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da7b30-7b27-4bc3-a33c-afb15d32cd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcaht_2023dec",
   "language": "python",
   "name": "fastcaht_2023dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
