1 INTRODUCTION 
 대규모 언어 모델(LLM)은 창의적 작업, 전문 업무, 대화 등 다양한 작업에 혁신을 일으키며 많은 데이터로 인간과 비슷하게 동작합니다. QWEN 시리즈는 기본 모델, 튜닝된 채팅 모델, 전문 모델 등 여러 모델을 포함하며, 이 논문은 훈련 방법, 평가 결과, 특수 모델의 성능에 대해 설명합니다. 목표는 포괄적이고 강력한 LLM을 개발하는 것입니다. 

2 PRETRAINING 
 사전 훈련 단계에서는 많은 데이터로 세계와 복잡성을 이해하며, 기본 언어 능력과 고급 기술을 포함한 다양한 기술을 학습합니다. 이 섹션에서는 데이터, 모델 설계, 확장, 벤치마크 결과를 간략하게 소개합니다. 

2.1 DATA 
 데이터 크기는 대규모 언어 모델 개발에 중요하다. 효과적인 사전 학습 데이터셋을 만들기 위해 다양한 유형, 도메인, 작업과 광범위한 데이터가 필요하다. 공공 웹 문서, 백과사전, 책, 코드 등 다양한 언어로 구성된 멀티링거얼 데이터셋을 설계했다. 데이터 품질을 보장하기 위해 철저한 데이터 사전 처리 절차를 개발했다. 고품질 지시 데이터를 사전 학습 프로세스에 통합하여 언어 모델의 성능을 향상시키고, 모델 성능을 보장하기 위해 겹침이 있는 지시 데이터를 제거했다. 3조 토큰 이상의 대규모 데이터셋을 구축하여 다양한 언어에서 높은 압축률을 달성했다. 

2.2 TOKENIZATION 
 이 연구에서는 BPE를 사용하여 번역 모델의 성능을 향상시키고 다국어 작업에서의 성능을 높였다. 100,000개 기반 어큐레이션과 일반적인 문자, 단어를 추가했으며, QWEN 토큰화 알고리즘의 압축률 측면에서 대부분 언어에서 높은 효율을 보였다. 또한 큰 어큐레이션 크기가 다운스트림 성능에 부정적 영향을 미치지 않음을 확인했다. 

2.3 ARCHITECTURE 
 QWEN은 LLaMA와 같은 최신 오픈소스 언어 모델을 기반으로 Transformer 아키텍처를 수정했습니다. 이 수정은 임베딩, 투영, 위치 인코딩, 바이어스, 사전 정상화, 활성화 함수 등 여러 측면에 이루어졌습니다. 일부 변경사항은 메모리 비용을 늘린 대신 성능을 향상시키고, 다른 일부는 안정성을 높이기 위해 최신 연구 방법을 적용했습니다. 또한, 효율성을 위해 임베딩 매트릭스에 FP32 정밀도를 사용하고, QKV 계층에 바이어스를 추가하여 추출 능력을 향상시켰습니다. 

2.4 TRAINING 
 QWEN 모델을 훈련하기 위해 번역 기법과 표준 방법을 사용하여 이전 토큰의 컨텍스트를 고려하여 다음 토큰을 예측합니다. 2048 컨텍스트 길이로 모델을 훈련시키고, Flash Attention 기버와 AdamW 최적화를 사용합니다. 하이퍼파라미터와 학습률 일정은 주어진 값을 따릅니다. 모델은 BFloat16 혼합 정밀도로 안정성을 위해 훈련됩니다. 

2.5 CONTEXT LENGTH EXTENSION 
 Transformer models have a limitation in context length, which leads to computational and memory costs. This work implements training-free techniques to extend context length. One technique used is NTK-aware interpolation. The largest QWEN model with 14 billion parameters performs well on various benchmarks and incorporates LogN-Scaling and window attention for stability and limited context window. Different window sizes are assigned to each layer based on their sensitivity to context length extension. 

2.6 EXPERIMENTAL RESULTS 
 이 연구에서는 최소 샷 및 소수 샷 학습 능력을 평가하기 위해 다양한 데이터셋을 사용하여 철저한 평가를 진행하고, 7가지 인기 있는 벤치마크를 사용하여 평가를 수행합니다. 결과에 따르면 QWEN 모델은 모든 하위 작업에서 뛰어난 성능을 보여줍니다. QWEN-14B는 3가지 작업에서 LLaMA2-70B를 능가하고 QWEN-7B는 LLaMA2-13B와 동등한 결과를 냅니다. QWEN-1.8B는 특정 작업에서 많은 파라미터를 가진 더 큰 모델을 앞섭니다. 결과는 QWEN-14B의 우수성을 강조하며 작은 모델인 QWEN-1.8B가 특정 애플리케이션에서 강력한 성능을 보일 수 있음을 시사합니다. 또한, 컨텍스트 길이 연장의 효과를 평가하기 위해 새로운 기법을 결합하여 컨텍스트 길이가 증가함에 따라 일관되게 낮은 농분도를 유지할 수 있음을 나타내는 결과가 제시됩니다. 

3 ALIGNMENT 
 최근 연구에 따르면, 슈퍼바이즈드 파인튜닝과 인간 피드백을 통한 강화학습이 적용된 언어 모델은 자연스러운 대화에 참여하는 능력이 크게 향상된다. 이 섹션에서는 QWEN 모델이 이러한 기법을 사용하여 훈련되었으며, 챗봇 비서 컨텍스트에서의 성능을 평가한다. 

3.1 SUPERVISED FINETUNING 
 사회적 행동 이해를 위해 SFT를 사용하여 챗스타일 데이터에 대해 사전 학습된 LLM을 미세 조정해야 함을 알려주며, 쿼리와 응답 모두 포함된다고 말했다. 이 방법에 대해 앞으로 섹션에서 자세히 알아볼 것이며, 데이터셋은 학술 논문이 https://arxiv.org에서 제공한다고 설명했다. 

3.1.1 DATA 
 우리는 여러 스타일로 된 대화를 배치해 자연어 생성을 위한 지도 학습 데이터셋을 향상시키고자 했습니다. Ouyang 등의 연구를 벤치마킹하여 모델의 유용성을 높이려고 했습니다. 잠재적 제한을 방지하기 위해 프롬프트 템플릿 데이터를 제외하고 폭력, 편향, 포르노그래피와 관련된 데이터를 분류하여 안전을 우선시했습니다. 또한 ChatML 스타일 형식을 사용하여 데이터 처리와 분석을 개선했습니다. 

3.1.2 TRAINING 
 SFT에서는 사전 훈련과 같은 다음 토큰 예측을 목표로 학습하고, 손실 마스크를 적용하여 모델을 훈련시킵니다. AdamW 최적화 알고리즘을 사용하며, 시퀀스 길이는 2048이고 배치 크기는 128입니다. 학습률은 4000 단계 동안 점진적으로 증가하며 과적합을 방지하기 위해 가중치 감소, 드롭아웃, 그래디언트 클립핑이 적용됩니다. 

3.2 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK 
 SFT는 제한과 과적합 문제가 있어 RLHF를 사용하여 인간 선호도에 더 부합하도록 모델을 교육했다. 이 방법은 Ouyang et al.와 Christiano et al.의 접근법을 따르며, 보상 모델 학습과 PPO를 사용한 정책 훈련이 포함된다. 

3.2.1 REWARD MODEL 
 훈련 및 미세 조정 단계에 필요한 대용량 데이터셋은 사전 학습 및 선호도 모델이 성공적인 보상 모델 생성에 중요하다. 사전 학습에는 손톱 쌍으로 구성된 비교 데이터가 필요하고, 미세 조정에는 인간 피드백과 설명적 점수를 활용한 더 높은 품질의 데이터가 사용된다. 응답 다양성을 높이기 위해 QWEN 모델이 크기와 샘플링 전략을 달리하여 사용되었다. 이 모델은 사전 학습된 QWEN 모델과 풀링 레이어를 활용하여 특정 엔드토큰에 대한 보상을 추출한다. 정확도는 보상 모델 평가의 중요 메트릭이지만 독립적이지는 않다. 

3.2.2 REINFORCEMENT LEARNING 
 PPO 알고리즘은 4가지 모델(정책, 가치, 참조, 보상)을 사용하여 가치 모델 업데이트에 집중한 후 샘플링과 KL 분포 분산을 사용하여 PPO 프로세스를 진행합니다. 학습률은 각각 1 × 10−6 및 5 × 10−6이며, 안정성을 위해 가치 손실 클립핑과 Top-p를 사용합니다. 예비 학습 그라디언트는 적절한 데이터를 사용합니다. 

3.3 AUTOMATIC AND HUMAN EVALUATION OF ALIGNED MODELS 
  

4 CODE-QWEN: SPECIALIZED MODEL FOR CODING 
 도메인 특정 데이터 학습은 코드 사전 학습과 미세 조정에 매우 효과적이며, 코딩, 디버깅, 해석 등 다양한 작업에 유용한 언어 모델이 될 수 있다. 이 작업에서는 사전 학습과 정렬 기법을 사용해 일반 모델 시리즈를 개발하고, 이를 바탕으로 코딩을 위한 도메인 특정 모델인 QWEN 기반 언어 모델(14억 및 7억 파라미터 버전)을 만들었다. 

4.1 CODE PRETRAINING 
 이 방법은 이전 접근법과 다르게 텍스트 데이터와 함께 코드 데이터로 훈련된 기본 모델을 사용하여 여러 도우미 기능을 개선합니다. 추가 훈련은 900억 토큰 이상의 코드 데이터와 긴 컨텍스트 시나리오를 사용하여 이루어지며, Flash Attention과 AdamW 최적화를 사용합니다. 

4.2 CODE SUPERVISED FINE-TUNING 
 멀티스테이지 SFT 전략이 슈퍼바이즈 파인튜닝 단계에서 초기화된 CODE-QWEN-CHAT 모델이 다른 방법보다 최고의 성능을 보였다. 이 모델은 AdamW 최적화기를 사용하여 학습률을 코사인 학습률 스케줄에 따라 조정하여 효과적으로 학습되었다. 

4.3 EVALUATION 
 CODE-QWEN 모델은 평가 세트에서 이전 모델들에 비해 우수한 성능을 보였지만, 대규모 모델이나 일부 클로즈드소스 모델에는 뒤쳐져 있다. 현재 상태의 기법에 비해 아직 뒤쳐져 있지만, 규모와 데이터 규모의 확장으로 차이를 좁힐 수 있다고 믿는다. 이 평가는 모델의 한계를 완전히 이해하기에는 부족하며, GPT-4와 비교할 때 더 엄격한 테스트가 필요하다. 

5 MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING 
 계산과 수학에 특화된 언어 모델 시리즈 MATH-QWEN-CHAT을 개발했습니다. 이 시리즈는 QWEN 사전 훈련 언어 모델을 기반으로 하며, 계산과 수학에서 뛰어나고 인간의 행동과 일치하도록 설계되었습니다. 시리즈에는 14억 개 매개변수를 가진 MATH-QWEN-14B-CHAT과 7억 개 매개변수를 가진 MATH-QWEN-7B-CHAT이 포함되어 있습니다. 

5.1 TRAINING 
 다양한 언어 모델의 성능과 다양한 벤치마크, 평가, 프로그래밍 언어에 대한 간단한 요약이 제공됩니다. 또한 수학적 이유력 데이터셋을 사용하여 수학적 이유력 모델 개발과 채팅 모델인 MATH-QWEN-CHAT에 대해 언급됩니다. 모델 교육은 짧은 시퀀스 길이를 사용하고, 수학적 이유력 결과는 착란 디코딩을 사용하여 보고됩니다. 

5.2 EVALUATION 
 다섯 개의 테스트 데이터셋에서 MATH-QWEN-CHAT 모델을 평가한 결과, 치트베타, 미네르바, RFT, 위저드매스, GAIRMath-Abel과 비교하여 더 나은 수학 추론 능력과 산수 능력을 보였다. 또한, MATH-QWEN-7B-CHAT는 MATH에서 미네르바-8B를 이겼고, MATH-QWEN-14B-CHAT는 Minerva-62B와 GPT-3.5를 앞섰으며 산수 능력과 중국어 수학 문제에서 더 우수한 성력을 보였다. 

6 RELATED WORK 
 2020년대 초반에 등장한 대형 언어 모델(LLM)은 Transformer 아키텍처를 사용하여 사전 훈련된 대규모 데이터셋으로 학습되었습니다. 이러한 모델은 2017년부터 2023년까지 계속 성장하고 발전해 왔으며, GPT-3(2020)과 ChatGPT, GPT-4(2022)와 같은 인간과 의사소통을 할 수 있는 모델이 등장했습니다. 이 기사는 또한 오픈소스 대형 언어 모델, 협업, 도구 사용, 에이전트, 코딩 및 수학 문제 해결 등에 대해 언급합니다. 이러한 발전은 연구자들이 성능을 향상시키기 위해 더 많은 연구와 개발을 수행하도록 격려했습니다. 

7 CONCLUSION 
 이 보고서에서는 14B, 7B, 1.8B 파라미터를 가진 QWEN 시리즈 대규모 언어 모델을 소개하며, 이 모델들은 최신 자연어 처리 발전을 보여줍니다. 이 모델들은 수조의 토큰과 대량의 데이터로 사전 훈련되었으며, 최첨단 기술을 사용하여 미세 조정되었습니다. QWEN 시리즈에는 코딩과 수학에 특화된 모델들이 포함되어 있으며, 각각의 분야에서 뛰어난 성능을 보이도록 도메인 별 데이터로 훈련되었습니다. 결과는 QWEN 시리즈가 기존 오픈 소스 모델과 경쟁력을 가지며, 일부 비공개 모델의 성능과 동일하다는 것을 보여줍니다. 이 보고서는 QWEN을 공개함으로써 협력과 혁신을 촉진하고, 분야의 발전을 위한 새로운 응용 프로그램 개발을 지원하고자 합니다. QWEN의 공개는 현실적인 환경에서 도입된 변수와 기술에 대한 이해를 높이는 데 도움이 될 것이며, 대규모 언어 모델 개발에서 큰 이정표를 나타내며 앞으로 발전과 혁신을 주도할 것이라고 생각합니다.