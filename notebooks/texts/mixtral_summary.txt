1 Introduction 
 이 논문에서는 Mixtral 8x7B라는 오픈 가중치가 있는 스페어스 믹스추러 모델(SMoE)을 소개합니다. 이 모델은 Apache 2.0 라이선스로 라디오 2 70B와 GPT-3.5를 대부분의 벤치마크에서 능가합니다. 토큰당 하위 집합을 사용하여 추론 속도를 낮은 배치 크기에서 빠르게 하고 대용량 배치 크기에서 처리량을 높입니다. Mixtral은 또한 32K 토큰의 맥락 크기를 사용하여 다국어 데이터로 사전 훈련되었습니다. 이 모델은 수학, 코드 생성 및 다국어 이해가 필요한 작업에서 Llama 2 70B를 크게 앞섭니다. 또한 Mixtral 8x7B-Instruct라는 명령어를 따르는 대화형 모델도 소개되며, 이 모델은 인간 평가 벤치마크에서 GPT-3.5 Turbo, Claude-2.1, Gemini Pro, Llama 2 70B-chat 모델을 크게 앞섭니다. Mixtral 8x7B와 Mixtral 8x7B-Instruct는 모두 Apache 2.0 라이선스로 학술 및 상업 목적으로 무료로 공개되어 다양한 용도로 사용할 수 있습니다. 

2 Architectural details 
 
번역결과번역결과 
매개변수 값
출력 Unit 4096
층 수 32
헤드 출력 크기 128
은닉 출력 크기 14336
헤드 수 32
키-관련 헤드 수 8
컨텍스트 길이 32768
사전 크기 32000
전문가 수 8
상위 k 전문가 2
표 1: 모델 아키텍처.
Mixtral은 변환기 아키텍처[31]을 기반으로하고 동일한 수정 사항을 사용합니다[18], 완전한 밀접한 컨텍스트 길이는 32k 토큰으로 지원되며, 피드 포워드 블록이 전문가 층(섹션 2.1)으로 대체됩니다.
표 1에서 모델 아키텍처 매개 변수를 요약합니다.
설명이 없습니다. 

2.1 Sparse Mixture of Experts 
 다음 문장은 Mixture of Experts 레이어(Figure 1)에 대한 간략한 개요를 제공합니다. 자세한 내용은 [12]를 참조하세요. 이 모듈은 게이트 네트워크의 출력에 의해 가중치가 부여된 전문가 네트워크의 출력의 가중 합으로 입력 x에 대한 출력을 결정합니다. 게이팅 벡터가 스팀스하다면 게이트가 0인 전문가의 출력을 계산할 필요가 없습니다. G(x)는 선형 층의 출력을 소프트맥스로 취하는 것으로 구현됩니다. 여러 가지 대안적인 방법이 있지만, 간단하고 효율적인 방법은 소프트맥스(Top-K 로깅 로지트)를 사용하는 것입니다. 여기서 K는 토큰당 사용되는 전문가 수의 하이퍼파라미터입니다. 이 값을 고정하고 n을 증가시키면 토큰당 사용되는 파라미터 수를 증가시킬 수 있습니다. MoE 층은 GPU에서 효율적으로 실행될 수 있으며, 토큰을 특정 전문가에게 할당할 수 있는 전문가 병렬화 기법이 있습니다. 이 층은 독립적으로 적용되며, 각 토큰에 대해 실행됩니다. 출력 y는 입력 x에 대해 계산됩니다. 

3 Results 
 번역결과  
다음 문장을 한글로 번역하고 최대한 짧게 요약합니다.  
Figure 2: Mixtral과 다른 Llama 모델의 성능을 광범위한 벤치마크에서 비교합니다. 모든 모델은 정확한 비교를 위해 평가 파이프라인을 다시 평가합니다. Mixtral은 Llama 2 70B를 포함한 대부분의 벤치마크에서 우수한 성능을 보입니다. 특히 수학과 코드 생성 분야에서 매우 우수한 성능을 보입니다. Table 2: Mixtral과 Llama 간의 비교를 나타냅니다. Mixtral은 Llama 2 70B의 성능을 대부분의 인기있는 벤치마크에서 맞먹거나 능가하며 추가 파라미터 수가 5배 적습니다. Figure 3: MMLU, 커먼센스 이해, 세계 지식, 읽기 이해, 수학 및 코드 생성 범주에서 Mistral(7B/8x7B), Llama 2(7B/13B/70B) 및 Mixtral(8x7B)의 결과를 보여줍니다. Mixtral은 대부분의 벤치마크에서 Llama 2 70B의 성능을 크게 능가하며 코드 및 수학 분야에서 큰 우세를 나타냅니다. 추가 파라미터 수가 5배 적습니다. 테이블 2의 세부 결과는 Mixtral, Mistral 7B 및 Llama 2 7B/13B/70B 및 Llama 1 34B2에 대한 자세한 결과를 보고합니다. 그림 2에 따르면 Mixtral은 대부분의 카테고리에서 Llama 2 70B의 성능을 능가하거나 맞먹습니다. 특히 수학 및 코드 벤치마크에서 우수한 성능을 보입니다. 크기와 효율성. 비용-성능 스펙트럼에서 Mixtral 모델의 효율성을 이해하기 위해 Llama 2 가족을 비교합니다. 스파크 믹스 오브 엑스퍼트 모델로 작동하는 Mixtral은 활성 파라미터 수가 5배 적습니다. 5배  

3.1 Multilingual benchmarks 
 비교적 미스트랄 7B보다 많은 양의 다국어 데이터를 사전 훈련 과정에 사용했다. 이로 인해 미크스트랄은 영어 정확도를 유지하면서 프랑스어, 독일어, 스페인어, 이탈리아어와 같은 다국어 벤치마크에서 우수한 성능을 보인다. 특히, 테이블 4에 따르면 미크스트랄은 라마 2 70B를 능가하며 프랑스어, 독일어, 스페인어, 이탈리아어에서 더 높은 성능을 보인다. 

3.2 Long range performance 
 다음 문장을 한글로 번역하고 요약하면 다음과 같습니다: 
Mixtral의 장시간 상태 대응 능력을 평가하기 위해, 우리는 임의로 삽입된 암호키를 검색하는 작업을 평가합니다. 이 작업은 [23]에 소개되었으며, 모델의 암호키 검색 능력을 측정하기 위해 합성된 작업입니다. 그림 4(왼쪽)에 따르면, Mixtral은 컨텍스트 길이나 암호키 삽입 위치에 관계없이 100% 재현 정확도를 달성합니다. 그림 4(오른쪽)에 따르면, proof-pile 데이터셋의 하위집합에서 Mixtral의 농담 값은 컨텍스트 길이가 증가함에 따라 계속 감소합니다. 
번역결과  
Mixtral의 장시간 상태 대응 능력을 평가하기 위해, 우리는 임의로 삽입된 암호키를 검색하는 작업을 평가합니다. 이 작업은 [23]에 소개되었으며, 모델의 암호키 검색 능력을 측정하기 위해 합성된 작업입니다. 그림 4(왼쪽)에 따르면, Mixtral은 컨텍스트 길이나 암호키 삽입 위치에 관계없이 100% 재현 정확도를 달성합니다. 그림 4(오른쪽)에 따르면, proof-pile 데이터셋의 하위집합에서 Mixtral의 농담 값은 컨텍스트 길이가 증가함에 따라 계속 감소합니다. 

3.3 Bias Benchmarks 
 이 문장을 한글로 번역하고 최대한 짧게 요압해주세요: 
Llama 2 70B Mixtral 8x7B
BBQ 정확도 51.5% 56.0%
BOLD 감성점수 (평균 ± 표준편차)
성별 0.293 ± 0.073 0.323 ±0.045
직업 0.218 ± 0.073 0.243 ± 0.087
종교적 이념 0.188 ± 0.133 0.144 ± 0.089
정치적 이념 0.149 ± 0.140 0.186 ± 0.146
인종 0.232 ± 0.049 0.232 ± 0.052
Figure 5: 편향 평가. 비교 Llama 2와 Mixtral 2, Mixtral은 더 낮은 정확도(BBQ 평가에서 56.0% 대 51.5%), 더 높은 평균 감성 점수(BOLD에서 더 낮은 표준편차)를 보여줍니다. 이는 더 작은 편향을 나타내고 있습니다.
또한, Mixtral은 비교적 Llama 2보다 더 긍정적인 감성을 보여줍니다, 각 그룹의 더불어 같은 분산은 각 그룹의 변동성을 나타냅니다. 
4. 미세 조정 및 선호도 모델링
Mixtral-Instruct는 감독 학습(SFT)을 사용하여 지시사항 데이터셋에 대해 미세 조정되었고, Direct Preference Optimization(DPO)을 사용하여 쌍 피드백 피드백 데이터셋에 대해 미세 조정되었습니다. Mixtral-Instruct는 MT-Bench에서 8.30의 점수를 받았으며, 2023년 12월 기준으로 최고의 오픈소스 모델입니다. 또한, LMSys Leaderboard에 따르면 Mixtral-Instruct는 GPT-3.5-Turbo, Gemini Pro, Claude-2.1, Llama-2-70b-chat을 능가합니다. 
5. 경로 분석
이 섹션에서는 라우터가 특정 도메인에서 특정 전문가를 선호하는 패턴을 보이는지 작은 규모의 분석을 수행합니다. 다양한 도메인(예: 수학, 생물학, 철학 등 