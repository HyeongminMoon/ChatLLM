1 INTRODUCTION 
 다양한 작업에 대한 강력한 기반이 되는 대규모 언어 모델(LLM)은 창의적 작업, 전문 지식 업무, 자연어 대화 등 다양한 작업을 수행하는 데 혁명을 일으키고 있습니다. 이러한 모델은 많은 양의 데이터로 훈련되어 인간과 유사한 방식으로 작동하며, 다양한 응용 프로그램에 사용될 수 있습니다. 이 논문에서는 QWEN 시리즈를 소개하며, 이 시리즈는 기본 훈련된 언어 모델, 인간 선호도에 맞춰 튜닝된 채팅 모델, 코딩 및 수학을 전문으로 하는 특수 모델 등 다양한 모델을 포함합니다. 이 논문은 훈련 방법, 평가 결과 및 코딩, 수학, 이미지 처리와 같은 다양한 작업에 대한 특수 모델의 성능에 대해 설명합니다. 이 논문의 목적은 개발자 또는 응용 프로그램 친화적인 규모의 보다 포괄적이고 강력한 LLM을 제공하는 것입니다. 

2 PRETRAINING 
 사전 훈련 단계에서는 많은 양의 데이터를 학습하여 세계와 복잡성을 포괄적으로 이해합니다. 기본 언어 능력뿐만 아니라 수학, 코딩, 논리적 추론 등 고급 기술도 포함됩니다. 이 섹션에서는 데이터, 모델 설계 및 확장, 벤치마크 데이터셋에서의 포괄적인 평가 결과를 소개합니다. 

2.1 DATA 
 데이터 크기가 대규모 언어 모델 개발에 중요한 요소임을 이전 연구들이 보여주었다. 효과적인 사전 학습 데이터셋을 만들기 위해 다양한 유형, 도메인, 작업을 포함하며 다양화되고 광범위한 범위의 데이터가 필요하다. 공공 웹 문서, 백과사전, 책, 코드 등 다양한 언어와 영어, 중국어를 포함한 다양한 언어로 구성된 데이터를 포함하는 멀티링거얼 데이터셋을 설계했다. 데이터 품질을 보장하기 위해 철저한 데이터 사전 처리 절차를 개발했다. 다중 작업 지시사항을 사용하여 언어 모델의 제로 샷 및 소수점 샷 성능을 향상시키기 위해 고품질 지시 데이터를 사전 학습 프로세스에 통합했다. 평가 과정에서 모델 성능을 보장하기 위해 벤치마크 세트에 포함된 모든 입력에 대해 약간의 겹침이 있는 지시 데이터를 제거했다. 다양한 언어에서 높은 압축률을 달성하여 훈련 및 추론 효율성을 향상시키기 위해 3조 토큰 이상의 대규모 데이터셋을 구축했다. 

2.2 TOKENIZATION 
 문장을 한글로 최대한 짧게 요약합니다: 
이 연구에서는 BPE를 토큰화 방법으로 사용하여 번역 모델의 성능을 향상시키고 다국어 작업에서의 성능을 향상시킵니다. 기존 모델과 공개 패턴을 따르면서 100,000개 기반 어큐레이션을 사용하고 일부 언어에서 성능을 향상시키기 위해 일반적인 문자와 단어를 추가했습니다. 압축률 측면에서 QWEN 토큰화 알고리즘의 성능을 확인하고, 다른 토큰화 방법과 비교하여 대부분의 언어에서 높은 압축 효율을 보였습니다. 또한 큰 어큐레이션 크기가 사전 훈련 모델의 다운스트림 성능에 부정적인 영향을 미치지 않음을 초기 실험을 통해 확인했습니다. 

2.3 ARCHITECTURE 
 QWEN은 언어 모델 학습의 최근 오픈소스 접근법인 LLaMA를 기반으로 하여 Transformer 아키텍처를 수정하여 설계되었습니다. 이러한 수정사항은 임베딩 및 출력 투영, 위치 인코딩, 바이어스, 사전 정상화 및 활성화 함수 등과 같은 측면에서 이루어졌습니다. 이 중 일부 변경사항은 성능 향상을 위해 기존 방법보다 메모리 비용이 더 많이 들지만, 다른 일부는 훈련 안정성 향상을 위해 최근 연구에서 제안한 대안 방법을 사용하였습니다. 또한, 효율성을 위해 임베딩 매트릭스에 FP32 정밀도를 사용하고, 효과적인 추출 능력을 향상시키기 위해 QKV 계층에 바이어스를 추가하였습니다. 

2.4 TRAINING 
 다음 문장을 한글로 최대한 짧게 요약하면: 
QWEN 모델을 훈련하기 위해 번역 기법 및 논문에 설명된 표준 방법을 따른다. 이는 이전 토큰에 의해 제공된 컨텍스트를 고려하여 다음 토큰을 예측하는 것을 포함한다. 2048 컨텍스트 길이로 모델을 훈련시킨다. 효율성과 메모리 사용량을 줄이기 위해 Flash Attention 기버를 사용한다. AdamW 최적화를 사용한다. 하이퍼파라미터는 β1 = 0.9, β2 = 0.95, ϵ = 10−8로 설정된다. 학습률 일정은 피크 학습률에 도달한 후 10%로 감소하는 사인형 곡선을 따른다. 모든 모델은 BFloat16 혼합 정밀도를 사용하여 안정성을 위해 훈련된다. 

2.5 CONTEXT LENGTH EXTENSION 
 Transformer models have a limitation in context length for their attention mechanism, leading to computational and memory costs. This work implements training-free techniques to extend context length. One technique used is NTK-aware interpolation. The largest QWEN model with 14 billion parameters outperforms previous 13B SoTA models on various benchmarks. The model incorporates LogN-Scaling and window attention to ensure stability and restrict attention to a limited context window. Different window sizes are assigned to each layer based on their sensitivity to context length extension. 

2.6 EXPERIMENTAL RESULTS 
 이 연구에서는 최소 샷 및 소수 샷 학습 능력을 평가하기 위해 다양한 데이터셋을 사용한 철저한 평가를 진행합니다. 우리의 모델은 LLaMA, LLAMA 2, MPT, Falcon, Baichuan2, ChatGLM2, InternLM, XVERSE, StableBeluga2 등 최신 오픈소스 기반 모델과 비교됩니다. 7가지 인기 있는 벤치마크를 사용하여 평가를 수행합니다. 결과에 따르면 QWEN 모델은 모든 하위 작업에서 뛰어난 성능을 보여줍니다. QWEN-14B는 3가지 작업에서 LLaMA2-70B를 능가하고 QWEN-7B는 LLaMA2-13B와 동등한 결과를 냅니다. QWEN-1.8B는 특정 작업에서 많은 파라미터를 가진 더 큰 모델을 앞섭니다. 결과는 QWEN-14B의 우수성을 강조하며 작은 모델인 QWEN-1.8B가 특정 애플리케이션에서 강력한 성능을 보일 수 있음을 시사합니다. 긴 컨텍스트 길이 연장의 효과를 평가하기 위해, 새로운 기법을 결합함으로써 컨텍스트 길이가 증가함에 따라 일관되게 낮은 농분도를 유지할 수 있음을 나타내는 결과가 제시됩니다. 

3 ALIGNMENT 
 사전 훈련된 큰 언어 모델은 인간 행동과 일치하지 않아 AI 비서로 사용하기에 적합하지 않다. 최근 연구에 따르면 슈퍼바이즈드 파인튜닝(SFT)과 인간 피드백을 통한 강화학습(RLHF)과 같은 일치 기법을 사용하면 언어 모델이 자연스러운 대화에 참여하는 능력이 크게 향상된다. 이 섹션에서는 QWEN 모델이 SFT와 RLHF를 사용하여 훈련되었으며 챗 기반 비서 컨텍스트에서의 성능을 평가한다. 

3.1 SUPERVISED FINETUNING 
 사회적 행동을 이해하기 위해서는 먼저 SFT를 수행하여 챗스타일 데이터에 대해 사전 학습된 LLM을 미세 조정하는 것이 필요합니다. 이 방법은 쿼리와 응답을 모두 포함합니다. 앞으로 몇 가지 섹션에서는 데이터 구성과 학습 방법에 대해 자세히 알아볼 것입니다. 이 데이터셋은 https://arxiv.org에서 제공하는 학술 논문을 포함합니다. 

3.1.1 DATA 
 To improve our supervised finetuning datasets, we have annotated conversations in multiple styles, focusing on natural language generation for diverse tasks. Inspired by Ouyang et al. (2022), we aim to enhance the model's helpfulness. We excluded prompt template data to prevent potential limitations, prioritizing safety by annotating data related to violence, bias, and pornography. Additionally, we utilized the ChatML-style format for improved data processing and analysis. 

3.1.2 TRAINING 
 다음 문장을 한글로 최대한 짧게 요약하면: 
- 사용자 피드포워드 학습(SFT)에서는 사전 훈련과 동일하게 다음 토큰 예측을 학습 과제로 사용합니다.
- 손실 마스크를 시스템 및 사용자 입력에 적용합니다. (자세한 내용은 섹션 A.1.1에서 확인)
- 모델의 훈련 과정에서는 AdamW 최적화 알고리즘을 사용하며, β1은 0.9, β2는 0.95, ε는 10^-8로 설정됩니다.
- 시퀀스 길이는 2048로 제한되고, 배치 크기는 128입니다.
- 모델은 총 4000 단계를 거쳐 학습률이 점진적으로 증가하며 1430단계 째에서 2 × 10^-6으로 최대점에 도달합니다.
- 과적합을 방지하기 위해 가중치 감소(0.1)가 적용되고, 드롭아웃(0.1)이 설정되며, 그래디언트 클립핑(1.0)이 강제됩니다. 

3.2 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK 
 SFT는 효과적이나 일반화, 창의성에 제한이 있고 과적합에 취약하다. 이 문제를 해결하기 위해 RLHF를 사용해 인간 선호도에 더 부합하도록 모델을 교육했다. 이 방법은 Ouyang et al.와 Christiano et al.의 접근법을 따른다. 과정에는 보상 모델 학습과 PPO를 사용한 정책 훈련이 포함된다. 

3.2.1 REWARD MODEL 
 훈련 및 미세 조정 단계에 대한 대규모 데이터셋이 필요한 사전 학습 및 선호도 모델 사전 학습(PMP)이 성공적인 보상 모델을 만드는 데 중요하다. 사전 학습 단계에는 대량의 비교 데이터가 필요하며, 이 데이터는 질 표시를 갖춘 손톱 쌍으로 구성된다. 반면, 미세 조정 단계에서는 더 높은 품질의 데이터를 사용하여 반응을 업데이트한다. 다양한 프롬프트를 사용하여 인간 피드백을 통해 보상 모델을 조정하고, 설명적인 점수에 따라 쌍으로 평가된다. 응답의 다양성을 높이기 위해 크기와 샘플링 전략이 다른 QWEN 모델이 사용되었다. 이 모델은 사전 학습된 QWEN 모델을 사용하여 시작되며, 특정 엔드토큰에 대한 보상을 추출하기 위해 풀링 레이어가 추가되었다. 테스트 데이터셋에서의 정확도는 보상 모델을 평가하는 중요한 메트릭이지만 독립적인 것은 아닌 것으로 보인다. 

3.2.2 REINFORCEMENT LEARNING 
 다음은 요약된 문장입니다:
PPO 알고리즘은 정책, 가치, 참조, 보상 모델 4가지로 구성되며, 가치 모델 업데이트에 집중한 후 PPO 프로세스를 진행합니다. 샘플링 두 개의 응답을 동시에 사용하고 KL 분포 분산 계수를 0.04로 설정하며, 보상을 실행 평균에 대한 보상으로 정규화합니다. 정책 및 가치 모델의 학습률은 각각 1 × 10−6 및 5 × 10−6입니다. 안정성을 높이기 위해 가치 손실 클립핑(0.15)을 사용하고, 추론 시 Top-p를 0.9로 설정합니다. 예비 학습 그라디언트는 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞은 양의 데이터를 사용하여 알맞 

3.3 AUTOMATIC AND HUMAN EVALUATION OF ALIGNED MODELS 
  

4 CODE-QWEN: SPECIALIZED MODEL FOR CODING 
 도메인 특정 데이터 학습은 특히 코드 사전 학습 및 미세 조정에 매우 효과적이라는 것이 입증되었습니다. 이러한 언어 모델은 코딩, 디버깅, 해석 등 다양한 작업에 귀중한 도구가 될 수 있습니다. 본 작업에서는 사전 학습 및 정렬 기법을 사용하여 일반 모델 시리즈를 개발했습니다. 이 기반 위에 코딩을 위한 도메인 특정 모델인 QWEN 기반 언어 모델(14억 및 7억 파라미터 버전)을 만들었습니다. 

4.1 CODE PRETRAINING 
 이 문장은 이전 접근법과 달리 코드 데이터와 함께 텍스트 데이터로 훈련된 기본 모델을 사용하여 다양한 도우미 기능을 향상시키는 방법을 설명합니다. 900억 토큰 이상의 코드 데이터로 모델을 추가 훈련시키고, 코드 처리와 같은 작업에 대한 긴 컨텍스트 시나리오를 처리하기 위해 최대 8192자 길이의 컨텍스트를 사용합니다. Flash Attention과 표준 최적화 알고리즘인 AdamW를 사용하여 학습률을 설정합니다. 

4.2 CODE SUPERVISED FINE-TUNING 
 다양한 실험 결과, 멀티스테이지 SFT 전략이 다른 방법보다 가장 뛰어난 성능을 보였다. 슈퍼바이즈 파인튜닝 단계에서는 CODE-QWEN-CHAT 모델이 CODE-QWEN 코드 기반 모델로부터 초기화되어 AdamW 최적화器(β1 = 0.9, β2 = 0.95, ϵ = 10−8)와 함께 학습률 2.0 × 10−6(14B 모델), 1.0 × 10−5(7B 모델)로 최적화되었다. 학습률은 코사인 학습률 스케줄(3% 웜업 단계)에 따라 피크 값까지 증가한 후 일정하게 유지되었다. 

4.3 EVALUATION 
 요약: 
CODE-QWEN 모델은 테이블 10과 11에 나타나 있는 Humaneval, MBPP, 및 HUMANEVALPACK 벤치마크 테스트 세트에서 평가되었습니다. 이 모델은 이전 베이스라인 모델(OCTOGEEX, InstructCodeT5+, CodeGeeX2)과 비교하여 파라미터 수가 유사하더라도 크게 우수한 성능을 보였습니다. 그러나 대규모 모델(Starcoder)과 비교하면 뒤쳐지며, 일부 클로즈드소스 모델에도 패스@1 성능에서 우위를 점했습니다. 그러나 현재 상태의 기법(GPT-4)에 비해 아직 뒤쳐져 있지만, 모델 규모와 데이터 규모의 계속된 확장으로 차이를 좁힐 수 있다고 믿습니다. 이 평가는 모델의 강점과 약점을 완전히 이해하기에 충분하지 않으며, GPT-4와 비교할 때 더 엄격한 테스트가 필요하다고 강조합니다. 

5 MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING 
 우리는 계산과 수학에 특화된 언어 모델 시리즈인 MATH-QWEN-CHAT을 개발했습니다. 이 시리즈는 QWEN 사전 훈련 언어 모델을 기반으로 하며, 계산과 수학에서 뛰어나고 인간의 행동과 일치하도록 설계되었습니다. 이 시리즈에는 각각 14억 개와 7억 개의 매개변수를 가진 MATH-QWEN-14B-CHAT과 MATH-QWEN-7B-CHAT이 포함되어 있습니다. 

5.1 TRAINING 
 The provided text discusses various language models and their performances on different benchmarks, including human evaluation and MBPP, as well as programming languages. It also mentions the use of a math SFT dataset for mathematical reasoning and the development of a chat model called MATH-QWEN-CHAT. The training process involves using shorter sequence lengths, and the model's performance on mathematical reasoning is reported using greedy decoding. 

5.2 EVALUATION 
 다섯 가지 테스트 데이터셋(GSM8K, MATH, Math401, Math23K)에서 모델을 평가합니다. MATH-QWEN-CHAT를 치트베타, 미네르바, RFT, 위저드매스, GAIRMath-Abel과 비교합니다. MATH-QWEN-CHAT 모델은 공개 모델과 비교하여 더 나은 수학 추론 능력과 산수 능력을 보입니다. 미네르바-8B를 MATH에서 이긴 MATH-QWEN-7B-CHAT가 있고, MATH-QWEN-14B-CHAT는 Minerva-62B와 GPT-3.5를 따라잡고 있으며 산수 능력과 중국어 수학 문제에서 더 나은 성능을 보입니다. 

6 RELATED WORK 
 다음 문장은 2020년대 초반에 등장한 대형 언어 모델(LLM)에 대한 간단한 요약입니다. 이 모델들은 변형 학습(Transformer) 아키텍처를 사용하여 대규모 데이터셋에 사전 훈련된 후 적용되었습니다. 이러한 모델들은 2017년부터 2023년까지 계속 성장하고 발전해 왔으며, 이러한 발전은 큰 관심과 연구를 불러일으켰습니다. 2020년대 초반에는 GPT-3과 같은 대형 언어 모델이 출시되었고, 이후 2022년에는 ChatGPT와 GPT-4와 같은 인간과 의사소통을 할 수 있는 모델이 등장했습니다. 이러한 모델들은 점차적으로 인간 가치와 일반 인공지능(AGI)에 가까운 능력을 갖추게 되었습니다. 이 기사는 또한 오픈소스 대형 언어 모델의 등장, 언어 모델 협업, 도구 사용 및 에이전트, 코딩 및 수학 문제 해결에 대한 언어 모델의 사용에 대해 언급합니다. 이러한 발전은 연구자들이 대형 언어 모델의 성능을 향상시키기 위해 더 많은 연구와 개발을 수행하도록 격려했습니다. 

7 CONCLUSION 
 이 보고서에서는 14B, 7B, 1.8B 파라미터를 가진 QWEN 시리즈 대규모 언어 모델을 제시합니다. 이 모델들은 최신 자연어 처리 발전을 보여줍니다. 이 모델들은 수조의 토큰과 함께 대량의 데이터로 사전 훈련되었으며, SFT, RLHF와 같은 최첨단 기술을 사용하여 미세 조정되었습니다. QWEN 시리즈에는 코딩과 수학에 특화된 모델인 CODE-QWEN, CODE-QWEN-CHAT, MATH-QWENCHAT이 포함되어 있으며, 각각의 분야에서 뛰어난 성능을 보이도록 도메인 별 데이터로 훈련되었습니다. 결과는 QWEN 시리즈가 기존 오픈 소스 모델과 경쟁력을 가지며, 일부 비공개 모델의 성능과 동일하다는 것을 보여줍니다. 커뮤니티에 QWEN을 공개함으로써 협력과 혁신을 촉진하고, 연구원과 개발자들이 작업을 확장하고 새로운 응용 프로그램을 개발하여 분야를 발전시킬 수 있도록 하고자 합니다. QWEN의 공개는 현실적인 환경에서 도입된 변수와 기술에 대한 이해를 높이는 데 도움이 될 것입니다. QWEN 시리즈는 대형 언어 모델 개발에서 큰 이정표를 나타내며, 앞으로 발전과 혁신을 주도할 것이라고 생각합니다.