"""
The gradio demo server for chatting with a single model.
"""

import argparse
from collections import defaultdict
import datetime
import json
import os
import random
import time
import uuid

import gradio as gr
import requests

from fastchat.conversation import (
    SeparatorStyle,
    list_conv_templates,
)
from fastchat.constants import (
    LOGDIR,
    WORKER_API_TIMEOUT,
    ErrorCode,
    MODERATION_MSG,
    CONVERSATION_LIMIT_MSG,
    SERVER_ERROR_MSG,
    INACTIVE_MSG,
    INPUT_CHAR_LEN_LIMIT,
    CONVERSATION_TURN_LIMIT,
    SESSION_EXPIRATION_TIME,
)
from fastchat.model.model_adapter import (
    get_conversation_template,
    ANTHROPIC_MODEL_LIST,
)
from fastchat.model.model_registry import get_model_info, model_info
from fastchat.serve.api_provider import (
    anthropic_api_stream_iter,
    openai_api_stream_iter,
    palm_api_stream_iter,
    init_palm_chat,
)
from fastchat.utils import (
    build_logger,
    moderation_filter,
    get_window_url_params_js,
    parse_gradio_auth_creds,
)

from fastchat.modules.translator import translate
from fastchat.modules.download_url import (
    get_urls_from_search_engine,
    get_contents_from_search_engine,
)
from fastchat.modules.vector_store import (
    ChromaCollector,
    feed_data_into_collector,
    feed_file_into_collector,
    feed_urls_into_collector,
    feed_webquery_into_collector,
    get_websearch_result,
)
from fastchat.modules.keyword_generator import generate_keyword
import copy

logger = build_logger("gradio_web_server", "gradio_web_server.log")

headers = {"User-Agent": "FastChat Client"}

no_change_btn = gr.Button.update()
enable_btn = gr.Button.update(interactive=True)
disable_btn = gr.Button.update(interactive=False)

controller_url = None
enable_moderation = False

learn_more_md = """
"""
# ### License
# The service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.

ip_expiration_dict = defaultdict(lambda: 0)
# Information about custom OpenAI compatible API models.
# JSON file format:
# {
#     "vicuna-7b": {
#         "model_name": "vicuna-7b-v1.5",
#         "api_base": "http://8.8.8.55:5555/v1",
#         "api_key": "password"
#     },
# }
openai_compatible_models_info = {}


class State:
    def __init__(self, model_name, prompt_template=None):
        if prompt_template:
            self.conv = get_conversation_template(prompt_template)
        else:
            self.conv = get_conversation_template(model_name)
        self.conv_id = uuid.uuid4().hex
        self.skip_next = False
        self.model_name = model_name
        self.prompt_template = prompt_template

        ## ChatMemory
        self.chat_memory = None

        ## translator plugin
        self.translator = "None"
        self.saved_conv = None
        self.ts_lang = "ko"

        ## retrivalQA plugin
        self.use_retrievalqa = False
        self.collector = ChromaCollector()

        ## WebSearch plugin. default=True
        self.use_websearch = True

        if model_name == "palm-2":
            # According to release note, "chat-bison@001" is PaLM 2 for chat.
            # https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023
            self.palm_chat = init_palm_chat("chat-bison@001")

    def to_gradio_chatbot(self):
        return self.conv.to_gradio_chatbot()

    def dict(self):
        base = self.conv.dict()
        base.update(
            {
                "conv_id": self.conv_id,
                "model_name": self.model_name,
            }
        )
        return base


def set_global_vars(controller_url_, enable_moderation_):
    global controller_url, enable_moderation
    controller_url = controller_url_
    enable_moderation = enable_moderation_


def get_conv_log_filename():
    t = datetime.datetime.now()
    name = os.path.join(LOGDIR, f"{t.year}-{t.month:02d}-{t.day:02d}-conv.json")
    return name


def get_model_list(
    controller_url, register_openai_compatible_models, add_chatgpt, add_claude, add_palm
):
    if controller_url:
        ret = requests.post(controller_url + "/refresh_all_workers")
        assert ret.status_code == 200
        ret = requests.post(controller_url + "/list_models")
        models = ret.json()["models"]
    else:
        models = []

    # Add API providers
    if register_openai_compatible_models:
        global openai_compatible_models_info
        openai_compatible_models_info = json.load(
            open(register_openai_compatible_models)
        )
        models += list(openai_compatible_models_info.keys())

    if add_chatgpt:
        models += ["gpt-3.5-turbo", "gpt-3.5-turbo-1106", "gpt-4"]
    if add_claude:
        models += ["claude-2.0", "claude-2.1", "claude-instant-1"]
    if add_palm:
        models += ["palm-2"]
    models = list(set(models))

    if "deluxe-chat-v1" in models:
        del models[models.index("deluxe-chat-v1")]
    if "deluxe-chat-v1.1" in models:
        del models[models.index("deluxe-chat-v1.1")]

    priority = {k: f"___{i:02d}" for i, k in enumerate(model_info)}
    models.sort(key=lambda x: priority.get(x, x))
    logger.info(f"Models: {models}")
    return models


def load_demo_single(models, url_params):
    selected_model = models[0] if len(models) > 0 else ""
    if "model" in url_params:
        model = url_params["model"]
        if model in models:
            selected_model = model

    dropdown_update = gr.Dropdown.update(
        choices=models, value=selected_model, visible=True
    )

    state = None
    return (
        state,
        dropdown_update,
        gr.Chatbot.update(visible=True),
        gr.Textbox.update(visible=True),
        gr.Button.update(visible=True),
        gr.Row.update(visible=True),
        gr.Accordion.update(visible=True),
        gr.Accordion.update(visible=True),
        gr.Accordion.update(visible=True),
        gr.Accordion.update(visible=True),
    )


def load_demo(url_params, request: gr.Request):
    global models

    ip = request.client.host
    logger.info(f"load_demo. ip: {ip}. params: {url_params}")
    ip_expiration_dict[ip] = time.time() + SESSION_EXPIRATION_TIME

    if args.model_list_mode == "reload":
        models = get_model_list(
            controller_url,
            args.register_openai_compatible_models,
            args.add_chatgpt,
            args.add_claude,
            args.add_palm,
        )

    return load_demo_single(models, url_params)


def vote_last_response(state, vote_type, model_selector, request: gr.Request):
    with open(get_conv_log_filename(), "a") as fout:
        data = {
            "tstamp": round(time.time(), 4),
            "type": vote_type,
            "model": model_selector,
            "state": state.dict(),
            "ip": request.client.host,
        }
        fout.write(json.dumps(data) + "\n")


def upvote_last_response(state, model_selector, request: gr.Request):
    logger.info(f"upvote. ip: {request.client.host}")
    vote_last_response(state, "upvote", model_selector, request)
    return ("",) + (disable_btn,) * 3


def downvote_last_response(state, model_selector, request: gr.Request):
    logger.info(f"downvote. ip: {request.client.host}")
    vote_last_response(state, "downvote", model_selector, request)
    return ("",) + (disable_btn,) * 3


def flag_last_response(state, model_selector, request: gr.Request):
    logger.info(f"flag. ip: {request.client.host}")
    vote_last_response(state, "flag", model_selector, request)
    return ("",) + (disable_btn,) * 3


def stop_response(state, model_selector, request: gr.Request):
    logger.info(f"stop. ip: {request.client.host}, session_id: {state.conv_id}")
    _ = requests.post(
        state.worker_addr + "/worker_stop_stream",
        timeout=5,
        json={"session_id": state.conv_id},
    )
    return ("",) + (disable_btn,)


def regenerate(state, request: gr.Request):
    logger.info(f"regenerate. ip: {request.client.host}")
    state.conv.update_last_message(None)
    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 6


def clear_history(state, request: gr.Request):
    logger.info(f"clear_history. ip: {request.client.host}")
    if state is not None:
        model_name = state.model_name
        prompt_template = state.prompt_template
        translator = state.translator
        ts_lang = state.ts_lang
        use_retrievalqa = state.use_retrievalqa
        collector = state.collector
        use_websearch = state.use_websearch

        state = State(model_name, prompt_template)
        state.translator = translator
        state.ts_lang = ts_lang
        state.use_retrievalqa = use_retrievalqa
        state.collector = collector
        state.use_websearch = use_websearch

    return (state, [], "") + (disable_btn,) * 6


def clear_all(state, request: gr.Request):
    logger.info(f"clear_history. ip: {request.client.host}")
    state = None
    return (state, [], "") + (disable_btn,) * 6

def get_ip(request: gr.Request):
    return request.client.host

def translate_set(state, model_selector, prompt_template, ts_box, request: gr.Request):
    logger.info(f"set translator to {ts_box}")
    if state is None:
        state = State(model_selector, prompt_template)
    state.translator = ts_box
    return state


def ts_lang_set(state, model_selector, prompt_template, ts_lang, request: gr.Request):
    logger.info(f"set ts_lang to {ts_lang}")
    if state is None:
        state = State(model_selector, prompt_template)
    state.ts_lang = ts_lang
    return state


def retrieval_set(state, model_selector, prompt_template, retrieval_checkbox, request: gr.Request):
    logger.info(f"set retrieval_checkbox to {retrieval_checkbox}")
    if state is None:
        state = State(model_selector, prompt_template)
    # if state.use_websearch:
    #     log_msg = "이 플러그인은 WebSearch 플러그인과 함께 사용할 수 없습니다. 해당 플러그인을 해제 후 다시 시도하시기 바랍니다."
    #     return state, log_msg, False

    state.collector = ChromaCollector()
    if retrieval_checkbox:
        state.use_retrievalqa = True
    else:
        state.use_retrievalqa = False

    log_msg = "플러그인 로드 완료" if retrieval_checkbox else "플러그인 해제 완료"
    return state, log_msg, retrieval_checkbox


def retrieval_text_upload(
    state, retrieval_checkbox, retrieval_text, request: gr.Request
):
    logger.info(f"retrieval text upload.")
    if not retrieval_checkbox:
        return state, "플러그인을 적용한 뒤 다시 시도하시기 바랍니다."

    state.collector.clear()
    feed_data_into_collector(state.collector, retrieval_text)
    return state, "업로드 완료"


def retrieval_file_upload(
    state, retrieval_checkbox, retrieval_files, request: gr.Request
):
    logger.info(f"retrieval file upload.")
    if not retrieval_checkbox:
        yield state, "플러그인을 적용한 뒤 다시 시도하시기 바랍니다."
        return

    state.collector.clear()
    for idx, file in enumerate(retrieval_files):
        yield state, f"{os.path.basename(file.name)} 업로드 중..."
        feed_file_into_collector(state.collector, file.name)

    yield state, "파일 업로드 완료"
    return


def retrieval_urls_upload(
    state, retrieval_checkbox, retrieval_urls, request: gr.Request
):
    logger.info(f"retrieval urls upload.")
    if not retrieval_checkbox:
        yield state, "플러그인을 적용한 뒤 다시 시도하시기 바랍니다."
        return

    yield state, f"URL을 읽어들이는 중..."
    state.collector.clear()
    feed_urls_into_collector(state.collector, retrieval_urls.split("\n"))

    for url in retrieval_urls.split("\n"):
        if url.endswith(".pdf"):
            yield state, "업로드 완료 <주의> pdf url은 인식되지 않습니다. 다운로드하여 파일탭을 사용해주세요."
            return

    yield state, "업로드 완료"
    return


def websearch_set(state, model_selector, prompt_template, websearch_checkbox, request: gr.Request):
    logger.info(f"set websearch_checkbox to {websearch_checkbox}")
    if state is None:
        state = State(model_selector, prompt_template)

    # if state.use_retrievalqa:
    #     log_msg = "이 플러그인은 retrievalQA 플러그인과 함께 사용할 수 없습니다. 해당 플러그인을 해제 후 다시 시도하시기 바랍니다."
    #     return state, log_msg, False

    state.collector = ChromaCollector()
    if websearch_checkbox:
        state.use_websearch = True
    else:
        state.use_websearch = False

    log_msg = "플러그인 로드 완료" if websearch_checkbox else "플러그인 해제 완료"
    return state, log_msg, websearch_checkbox


def add_text(state, model_selector, prompt_template, text, request: gr.Request):
    ip = request.client.host
    logger.info(f"add_text. ip: {ip}. len: {len(text)}")

    if state is None:
        state = State(model_selector, prompt_template)

    if len(text) <= 0:
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), "") + (no_change_btn,) * 6

    if ip_expiration_dict[ip] < time.time():
        logger.info(f"inactive. ip: {request.client.host}. text: {text}")
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), INACTIVE_MSG) + (no_change_btn,) * 6

    if enable_moderation:
        flagged = violates_moderation(text)
        if flagged:
            logger.info(f"violate moderation. ip: {request.client.host}. text: {text}")
            state.skip_next = True
            return (state, state.to_gradio_chatbot(), MODERATION_MSG) + (
                no_change_btn,
            ) * 6

    conv = state.conv
    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:
        logger.info(f"conversation turn limit. ip: {request.client.host}. text: {text}")
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), CONVERSATION_LIMIT_MSG) + (
            no_change_btn,
        ) * 6

    # text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off
    conv.append_message(conv.roles[0], text)
    conv.append_message(conv.roles[1], None)
    if state.saved_conv is not None:
        state.saved_conv.append_message(state.saved_conv.roles[0], text)
        state.saved_conv.append_message(state.saved_conv.roles[1], None)
    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 6


def post_process_code(code):
    sep = "\n```"
    if sep in code:
        blocks = code.split(sep)
        if len(blocks) % 2 == 1:
            for i in range(1, len(blocks), 2):
                blocks[i] = blocks[i].replace("\\_", "_")
        code = sep.join(blocks)
    return code


def model_worker_stream_iter(
    conv,
    model_name,
    worker_addr,
    prompt,
    temperature,
    repetition_penalty,
    top_p,
    max_new_tokens,
    session_id,
):
    # Make requests
    gen_params = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "repetition_penalty": repetition_penalty,
        "top_p": top_p,
        "max_new_tokens": max_new_tokens,
        "stop": conv.stop_str,
        "stop_token_ids": conv.stop_token_ids,
        "echo": False,
        "session_id": session_id,
    }
    logger.info(f"==== request ====\n{gen_params}")

    # Stream output
    response = requests.post(
        worker_addr + "/worker_generate_stream",
        headers=headers,
        json=gen_params,
        stream=True,
        timeout=WORKER_API_TIMEOUT,
    )
    for chunk in response.iter_lines(decode_unicode=False, delimiter=b"\0"):
        if chunk:
            data = json.loads(chunk.decode())
            yield data


def bot_response(
    state, temperature, top_p, repetition_penalty, max_new_tokens, request: gr.Request
):
    logger.info(f"bot_response. ip: {request.client.host}")
    start_tstamp = time.time()
    temperature = float(temperature)
    top_p = float(top_p)
    max_new_tokens = int(max_new_tokens)

    if state.skip_next:
        # This generate call is skipped due to invalid inputs
        state.skip_next = False
        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 6
        return

    conv, model_name = state.conv, state.model_name
    if model_name in openai_compatible_models_info:
        model_info = openai_compatible_models_info[model_name]
        prompt = conv.to_openai_api_messages()
        stream_iter = openai_api_stream_iter(
            model_info["model_name"],
            prompt,
            temperature,
            top_p,
            max_new_tokens,
            api_base=model_info["api_base"],
            api_key=model_info["api_key"],
        )
    elif model_name in ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo-1106"]:
        # avoid conflict with Azure OpenAI
        assert model_name not in openai_compatible_models_info
        prompt = conv.to_openai_api_messages()
        stream_iter = openai_api_stream_iter(
            model_name, prompt, temperature, top_p, max_new_tokens
        )
    elif model_name in ANTHROPIC_MODEL_LIST:
        prompt = conv.get_prompt()
        stream_iter = anthropic_api_stream_iter(
            model_name, prompt, temperature, top_p, max_new_tokens
        )
    elif model_name == "palm-2":
        stream_iter = palm_api_stream_iter(
            state.palm_chat, conv.messages[-2][1], temperature, top_p, max_new_tokens
        )
    else:
        # Query worker address
        ret = requests.post(
            controller_url + "/get_worker_address", json={"model": model_name}
        )
        worker_addr = ret.json()["address"]
        state.worker_addr = worker_addr
        logger.info(f"model_name: {model_name}, worker_addr: {worker_addr}")

        # No available worker
        if worker_addr == "":
            conv.update_last_message(SERVER_ERROR_MSG)
            yield (
                state,
                state.to_gradio_chatbot(),
                disable_btn,
                disable_btn,
                disable_btn,
                disable_btn,
                enable_btn,
                enable_btn,
            )
            return

        # Construct prompt.
        # We need to call it here, so it will not be affected by "▌".
        system_type = "default"
        retrieval_chunks = []
        current_user_message = state.conv.messages[-2][1]
        ### retrievalqa
        if state.use_retrievalqa:
            system_type = "retrieval"
            retrieval_chunks = state.collector.get_sorted(
                current_user_message, n_results=5
            )

        ### websearch
        search_flag = False
        if state.use_websearch:
            if current_user_message.startswith("#"):
                current_user_message = current_user_message[1:].lstrip()
                if len(state.conv.messages) > 2:
                    state.conv.messages[-2][1] = (
                        "*정확한 검색을 위해 과거 데이터를 삭제했습니다.*\n" + current_user_message
                    )
                    state.conv.messages = [
                        state.conv.messages[-2],
                        state.conv.messages[-1],
                    ]
                    state.saved_conv = None
                else:
                    state.conv.messages[-2][1] = current_user_message

                search_flag = True
            # elif len(state.conv.messages) <= 2:
            #     search_flag = True

            if search_flag:
                conv.update_last_message(f"*Searching...*")
                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 3 + (
                    enable_btn,
                ) + (disable_btn,) * 2
                state.collector.clear()
                search_keywords = generate_keyword(
                    model_name, current_user_message, controller_url
                )

                conv.update_last_message(f"*Searching {search_keywords[0]}...*")
                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 3 + (
                    enable_btn,
                ) + (disable_btn,) * 2

                print("search", search_keywords[0])
                # search_contents, urls = get_contents_from_search_engine(search_keywords[0])
                for search_contents, urls, is_yield in get_websearch_result(
                    state.collector, search_keywords[0]
                ):
                    if is_yield:
                        conv.update_last_message(search_contents)
                        yield (state, state.to_gradio_chatbot()) + (
                            disable_btn,
                        ) * 3 + (enable_btn,) + (disable_btn,) * 2

                system_type = "retrieval"
                retrieval_chunks = [search_contents]
                state.conv.messages[-2][1] = current_user_message
                conv.update_last_message(f"")
            # try:
            #     retrieval_chunks = state.collector.get_sorted(' '.join(current_user_message.split(' ')[:]), n_results=5)
            # except:
            #     retrieval_chunks = []

        if state.saved_conv is not None:
            prompt = state.saved_conv.get_prompt(task=system_type, context='\n'.join(retrieval_chunks))
            # prompt = state.saved_conv.get_prompt()
        else:
            prompt = state.conv.get_prompt(task=system_type, context='\n'.join(retrieval_chunks))
            # prompt = state.conv.get_prompt()

        # Set repetition_penalty
        if "t5" in model_name:
            repetition_penalty = 1.2
        else:
            repetition_penalty = 1.0

        # Set custom configuration if it exists
        custom_config = state.dict().get("config")
        if custom_config is not None:
            if "temperature" in custom_config:
                temperature = custom_config["temperature"]
            if "repetition_penalty" in custom_config:
                repetition_penalty = custom_config["repetition_penalty"]
            if "top_p" in custom_config:
                top_p = custom_config["top_p"]
            if "max_new_tokens" in custom_config:
                max_new_tokens = custom_config["max_new_tokens"]

        stream_iter = model_worker_stream_iter(
            conv,
            model_name,
            worker_addr,
            prompt,
            temperature,
            repetition_penalty,
            top_p,
            max_new_tokens,
            state.conv_id,
        )

    conv.update_last_message("*Typing...*")
    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 3 + (enable_btn,) + (
        disable_btn,
    ) * 2

    try:
        for data in stream_iter:
            if data["error_code"] == 0:
                output = data["text"].strip()
                if "vicuna" in model_name:
                    output = post_process_code(output)
                conv.update_last_message(output + "▌")
                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 3 + (
                    enable_btn,
                ) + (disable_btn,) * 2
            else:
                output = data["text"] + f"\n\n(error_code: {data['error_code']})"
                conv.update_last_message(output)
                yield (state, state.to_gradio_chatbot()) + (
                    disable_btn,
                    disable_btn,
                    disable_btn,
                    disable_btn,
                    enable_btn,
                    enable_btn,
                )
                return
            time.sleep(0.015)
    except requests.exceptions.RequestException as e:
        conv.update_last_message(
            f"{SERVER_ERROR_MSG}\n\n"
            f"(error_code: {ErrorCode.GRADIO_REQUEST_ERROR}, {e})"
        )
        yield (state, state.to_gradio_chatbot()) + (
            disable_btn,
            disable_btn,
            disable_btn,
            disable_btn,
            enable_btn,
            enable_btn,
        )
        return
    except Exception as e:
        conv.update_last_message(
            f"{SERVER_ERROR_MSG}\n\n"
            f"(error_code: {ErrorCode.GRADIO_STREAM_UNKNOWN_ERROR}, {e})"
        )
        yield (state, state.to_gradio_chatbot()) + (
            disable_btn,
            disable_btn,
            disable_btn,
            disable_btn,
            enable_btn,
            enable_btn,
        )
        return

    # Delete "▌"
    last_message = conv.messages[-1][-1][:-1]
    conv.update_last_message(last_message)
    state.worker_addr = None
    yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 3 + (disable_btn,) + (
        enable_btn,
    ) * 2

    finish_tstamp = time.time()
    logger.info(f"{output}")

    with open(get_conv_log_filename(), "a") as fout:
        data = {
            "tstamp": round(finish_tstamp, 4),
            "type": "chat",
            "model": model_name,
            "gen_params": {
                "temperature": temperature,
                "top_p": top_p,
                "max_new_tokens": max_new_tokens,
            },
            "start": round(start_tstamp, 4),
            "finish": round(finish_tstamp, 4),
            "state": state.dict(),
            "ip": request.client.host,
        }
        fout.write(json.dumps(data) + "\n")

    if state.translator != "None":
        if state.saved_conv is None:
            state.saved_conv = copy.deepcopy(conv)
        else:
            state.saved_conv.update_last_message(conv.messages[-1][-1])
        ori_output = copy.deepcopy(output)
    if state.translator in ["Google", "Papago"]:
        output = translate(output, state.translator.lower(), state.ts_lang)
        last_message = ori_output + f"\n\n번역({state.translator}):\n\n" + output
        conv.update_last_message(last_message)

        yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 3 + (
            disable_btn,
        ) + (enable_btn,) * 2
    if search_flag:
        if state.saved_conv is None:
            state.saved_conv = copy.deepcopy(conv)
        else:
            state.saved_conv.update_last_message(conv.messages[-1][-1])
        last_message += (
            "\n\n※개발 중인 기능으로 답변이 정확하지 않을 수 있습니다. 정확한 정보는 아래 링크에서 확인해주세요.\nReferences:\n"
            + "\n".join(urls[:3])
        )
        conv.update_last_message(last_message)

        yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 3 + (
            disable_btn,
        ) + (enable_btn,) * 2


block_css = """
#notice_markdown {
    font-size: 104%
}
#notice_markdown th {
    display: none;
}
#notice_markdown td {
    padding-top: 6px;
    padding-bottom: 6px;
}
#model_description_markdown {
    font-size: 110%
}
#leaderboard_markdown {
    font-size: 110%
}
#leaderboard_markdown td {
    padding-top: 6px;
    padding-bottom: 6px;
}
#leaderboard_dataframe td {
    line-height: 0.1em;
}
#about_markdown {
    font-size: 110%
}
#ack_markdown {
    font-size: 110%
}
#input_box textarea {
}
#chatbot_multi {
    font-size: 80%
}
footer {
display:none !important
}
.image-container {
    display: flex;
    align-items: center;
    padding: 1px;
}
.image-container img {
    margin: 0 30px;
    height: 30px;
    max-height: 100%;
    width: auto;
    max-width: 30%;
}
.image-about img {
    margin: 0 30px;
    margin-top:  30px;
    height: 60px;
    max-height: 100%;
    width: auto;
    float: left;
    visibility: hidden;
}
"""


def get_model_description_md(models):
    model_description_md = """
| | | |
| ---- | ---- | ---- |
"""
    ct = 0
    visited = set()
    for i, name in enumerate(models):
        if name in model_info:
            minfo = model_info[name]
            if minfo.simple_name in visited:
                continue
            visited.add(minfo.simple_name)
            one_model_md = f"[{minfo.simple_name}]({minfo.link}): {minfo.description}"
        else:
            visited.add(name)
            one_model_md = (
                f"[{name}](): Add the description at fastchat/model/model_registry.py"
            )

        if ct % 3 == 0:
            model_description_md += "|"
        model_description_md += f" {one_model_md} |"
        if ct % 3 == 2:
            model_description_md += "\n"
        ct += 1
    return model_description_md


def build_single_model_ui(models, add_promotion_links=False):
    promotion = (
        """
        
"""
        if add_promotion_links
        else ""
    )
    #         - Introducing Llama 2: The Next Generation Open Source Large Language Model. [[Website]](https://ai.meta.com/llama/)
    # - Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. [[Blog]](https://lmsys.org/blog/2023-03-30-vicuna/)
    # - | [GitHub](https://github.com/lm-sys/FastChat) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |

    notice_markdown = f"""
# Ados Large Language Models Test Page
{promotion}
## 아래 드롭박스를 통해 대화할 모델을 선택하세요.
"""
    # ### Terms of use
    # By using this service, users are required to agree to the following terms: The service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. **The service collects user dialogue data and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) license.**

    state = gr.State()
    model_description_md = get_model_description_md(models)
    # gr.Markdown(notice_markdown + model_description_md, elem_id="notice_markdown")
    gr.Markdown(notice_markdown, elem_id="notice_markdown")

    with gr.Row(elem_id="model_selector_row"):
        model_selector = gr.Dropdown(
            choices=models,
            value=models[0] if len(models) > 0 else "",
            interactive=True,
            label="Select Model",
            # show_label=False,
            # container=False,
        )
        prompt_template = gr.Dropdown(
            choices=list_conv_templates(),
            value=None,
            # interactive=True,
            label="Template(option)",
            # show_label=False,
            # container=False,
        )

    chatbot = gr.Chatbot(
        elem_id="chatbot",
        label="Scroll down and start chatting",
        visible=False,
        height=550,
    )

    with gr.Row():
        with gr.Column(scale=20):
            textbox = gr.Textbox(
                show_label=False,
                placeholder="인공지능에게 질문을 해보세요. 맨 앞에 #을 붙여 검색기능을 사용할 수 있어요.",
                visible=False,
                container=False,
            )
        with gr.Column(scale=1, min_width=50):
            send_btn = gr.Button(value="Send", visible=False)

    with gr.Row(visible=False) as button_row:
        upvote_btn = gr.Button(value="👍  대화가 마음에 들어요", interactive=False, visible=True)
        downvote_btn = gr.Button(value="👎  대화가 별로에요", interactive=False, visible=True)
        flag_btn = gr.Button(value="⚠️  Flag", interactive=False, visible=False)
        stop_btn = gr.Button(value="⏹️  Stop Generation", interactive=False)
        regenerate_btn = gr.Button(value="🔄  Regenerate", interactive=False)
        clear_btn = gr.Button(value="🧹  Clear history", interactive=False)

    gr.Markdown("Plugins")
    with gr.Accordion("번역기", open=False, visible=False) as translator_row:
        with gr.Row():
            ts_box = gr.Radio(
                ["None", "Google", "Papago"],
                value="None",
                label="번역기",
                info="챗봇이 응답을 종료한 후에 적용됩니다.",
                interactive=True,
            )
            ts_lang = gr.Radio(
                ["ko", "ja", "cn", "ru", "fr", "de", "es", "tl"],
                value="ko",
                label="번역 언어",
                info="번역기 종류에 따라 일부 언어는 지원되지 않습니다.",
                interactive=True,
            )
    with gr.Accordion(
        "RetrievalQA(문서기반질문)", open=False, visible=False
    ) as retrievalqa_row:
        retrieval_checkbox = gr.Checkbox(
            label="적용",
            info="텍스트/파일/URL 을 업로드하면 챗봇이 문서를 기반으로 대답합니다.",
            container=False,
            interactive=True,
        )
        retrieval_log = gr.Markdown()
        with gr.Tabs() as retrieval_tabs:
            with gr.Tab("텍스트"):
                retrieval_text = gr.Textbox(
                    show_label=False,
                    container=False,
                    placeholder="이곳에 텍스트를 입력하세요.",
                    lines=13,
                    # max_lines=100,
                )
                retrieval_text_upload_btn = gr.Button(value="Upload")

            with gr.Tab("파일"):
                gr.Markdown("지원 형식: [text, .pdf, .docx]")
                retrieval_files = gr.File(
                    show_label=False,
                    container=False,
                    file_count="multiple",
                    file_types=["text", ".pdf", ".docx"],
                )
                retrieval_file_upload_btn = gr.Button(value="Upload")

            with gr.Tab("URL"):
                retrieval_urls = gr.Textbox(
                    show_label=False,
                    container=False,
                    placeholder="URL을 입력하세요. 여러 개일경우 줄을 구분하여 입력하세요.",
                    lines=13,
                    # max_lines=100,
                )
                retrieval_url_upload_btn = gr.Button(value="Upload", visible=True)

    with gr.Accordion("WebSearch", open=False, visible=False) as websearch_row:
        websearch_checkbox = gr.Checkbox(
            label="적용",
            info="챗봇이 웹 검색 결과를 기반으로 대답합니다. 맨 앞에 #을 붙여 검색기능을 사용할 수 있습니다.",
            container=False,
            interactive=True,
            value=True,
        )
        websearch_log = gr.Markdown()

    with gr.Accordion("Parameters", open=False, visible=False) as parameter_row:
        temperature = gr.Slider(
            minimum=0.0,
            maximum=1.0,
            value=0.7,
            step=0.1,
            interactive=True,
            label="Temperature",
            info="토큰 선택시 무작위성을 조절합니다. 낮을수록 사실적인 대답을, 높을수록 창조적인 대답을 합니다.",
        )
        top_p = gr.Slider(
            minimum=0.0,
            maximum=1.0,
            value=0.8,
            step=0.1,
            interactive=True,
            label="Top P",
            info="샘플링 결정성을 조절합니다. 낮을수록 사실적인 대답을, 높을수록 창조적인 대답을 합니다.",
        )
        repetition_penalty = gr.Slider(
            minimum=0.0,
            maximum=2.0,
            value=1.0,
            step=0.1,
            interactive=True,
            label="Repetition Penalty",
            info="반복 패널티를 조절합니다. 높을수록 단어가 반복되는 현상이 줄어듭니다.",
        )
        max_output_tokens = gr.Slider(
            minimum=16,
            maximum=32768,
            value=2048,
            step=64,
            interactive=True,
            label="Max output tokens",
            info="생성하는 최대 토큰 개수를 조절합니다.",
        )

    gr.Markdown(learn_more_md)

    # Register listeners
    btn_list = [upvote_btn, downvote_btn, flag_btn, stop_btn, regenerate_btn, clear_btn]
    upvote_btn.click(
        upvote_last_response,
        [state, model_selector],
        [textbox, upvote_btn, downvote_btn, flag_btn],
    )
    downvote_btn.click(
        downvote_last_response,
        [state, model_selector],
        [textbox, upvote_btn, downvote_btn, flag_btn],
    )
    flag_btn.click(
        flag_last_response,
        [state, model_selector],
        [textbox, upvote_btn, downvote_btn, flag_btn],
    )
    stop_btn.click(stop_response, [state, model_selector], [textbox, stop_btn])
    regenerate_btn.click(regenerate, state, [state, chatbot, textbox] + btn_list).then(
        bot_response,
        [state, temperature, top_p, repetition_penalty, max_output_tokens],
        [state, chatbot] + btn_list,
    )
    clear_btn.click(clear_history, state, [state, chatbot, textbox] + btn_list)

    model_selector.change(clear_all, state, [state, chatbot, textbox] + btn_list)
    prompt_template.change(clear_all, state, [state, chatbot, textbox] + btn_list)

    textbox.submit(
        add_text, [state, model_selector, prompt_template, textbox], [state, chatbot, textbox] + btn_list
    ).then(
        bot_response,
        [state, temperature, top_p, repetition_penalty, max_output_tokens],
        [state, chatbot] + btn_list,
    )
    send_btn.click(
        add_text, [state, model_selector, prompt_template, textbox], [state, chatbot, textbox] + btn_list
    ).then(
        bot_response,
        [state, temperature, top_p, repetition_penalty, max_output_tokens],
        [state, chatbot] + btn_list,
    )

    ts_box.change(translate_set, [state, model_selector, prompt_template, ts_box], state)
    ts_lang.change(ts_lang_set, [state, model_selector, prompt_template, ts_lang], state)

    retrieval_checkbox.change(
        retrieval_set,
        [state, model_selector, prompt_template, retrieval_checkbox],
        [state, retrieval_log, retrieval_checkbox],
    )
    retrieval_text_upload_btn.click(
        retrieval_text_upload,
        [state, retrieval_checkbox, retrieval_text],
        [state, retrieval_log],
    )
    retrieval_file_upload_btn.click(
        retrieval_file_upload,
        [state, retrieval_checkbox, retrieval_files],
        [state, retrieval_log],
    )
    retrieval_url_upload_btn.click(
        retrieval_urls_upload,
        [state, retrieval_checkbox, retrieval_urls],
        [state, retrieval_log],
    )

    websearch_checkbox.change(
        websearch_set,
        [state, model_selector, prompt_template, websearch_checkbox],
        [state, websearch_log, websearch_checkbox],
    )

    return (
        state,
        model_selector,
        chatbot,
        textbox,
        send_btn,
        button_row,
        translator_row,
        retrievalqa_row,
        websearch_row,
        parameter_row,
    )


def build_demo(models):
    with gr.Blocks(
        title="Ados Chatbot Demo Page",
        theme=gr.themes.Base(),
        css=block_css,
    ) as demo:
        url_params = gr.JSON(visible=False)

        (
            state,
            model_selector,
            chatbot,
            textbox,
            send_btn,
            button_row,
            translator_row,
            retrievalqa_row,
            websearch_row,
            parameter_row,
        ) = build_single_model_ui(models)

        if args.model_list_mode not in ["once", "reload"]:
            raise ValueError(f"Unknown model list mode: {args.model_list_mode}")
        demo.load(
            load_demo,
            [url_params],
            [
                state,
                model_selector,
                chatbot,
                textbox,
                send_btn,
                button_row,
                translator_row,
                retrievalqa_row,
                websearch_row,
                parameter_row,
            ],
            _js=get_window_url_params_js,
        )

    return demo


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int)
    parser.add_argument(
        "--share",
        action="store_true",
        help="Whether to generate a public, shareable link.",
    )
    parser.add_argument(
        "--controller-url",
        type=str,
        default="http://localhost:21001",
        help="The address of the controller.",
    )
    parser.add_argument(
        "--concurrency-count",
        type=int,
        default=10,
        help="The concurrency count of the gradio queue.",
    )
    parser.add_argument(
        "--model-list-mode",
        type=str,
        default="once",
        choices=["once", "reload"],
        help="Whether to load the model list once or reload the model list every time.",
    )
    parser.add_argument(
        "--moderate", action="store_true", help="Enable content moderation"
    )
    parser.add_argument(
        "--add-chatgpt",
        action="store_true",
        help="Add OpenAI's ChatGPT models (gpt-3.5-turbo, gpt-4)",
    )
    parser.add_argument(
        "--register-openai-compatible-models",
        type=str,
        help="Register custom OpenAI API compatible models by loading them from a JSON file",
    )
    parser.add_argument(
        "--add-claude",
        action="store_true",
        help="Add Anthropic's Claude models (claude-2, claude-instant-1)",
    )
    parser.add_argument(
        "--add-palm",
        action="store_true",
        help="Add Google's PaLM model (PaLM 2 for Chat: chat-bison@001)",
    )
    parser.add_argument(
        "--gradio-auth-path",
        type=str,
        help='Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: "u1:p1,u2:p2,u3:p3"',
        default=None,
    )
    args = parser.parse_args()
    logger.info(f"args: {args}")

    # Set global variables
    set_global_vars(args.controller_url, args.moderate)
    models = get_model_list(
        args.controller_url,
        args.register_openai_compatible_models,
        args.add_chatgpt,
        args.add_claude,
        args.add_palm,
    )

    # Set authorization credentials
    auth = None
    if args.gradio_auth_path is not None:
        auth = parse_gradio_auth_creds(args.gradio_auth_path)

    # Launch the demo
    demo = build_demo(models)
    demo.queue(
        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False
    ).launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        max_threads=200,
        auth=auth,
    )
